#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šç®—æ³•å¯¹æ¯”ç®¡ç†å™¨

è´Ÿè´£ç®¡ç†å¤šä¸ªç®—æ³•çš„æ•°æ®é›†ï¼Œæ”¯æŒç®—æ³•å¯¹æ¯”åˆ†æã€‚
ä½¿ç”¨é¢å‘å¯¹è±¡è®¾è®¡ï¼Œæ”¯æŒå¹¶å‘å¤„ç†ã€‚
"""

from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import asyncio
from concurrent.futures import ThreadPoolExecutor
import hashlib
import json
from utils.logger import Logger
from spmid.spmid_analyzer import SPMIDAnalyzer
from spmid.spmid_reader import Note

logger = Logger.get_logger()


class AlgorithmStatus(Enum):
    """ç®—æ³•çŠ¶æ€æšä¸¾"""
    PENDING = "pending"  # ç­‰å¾…åŠ è½½
    LOADING = "loading"  # æ­£åœ¨åŠ è½½
    READY = "ready"  # å·²å°±ç»ª
    ERROR = "error"  # åŠ è½½å¤±è´¥


@dataclass
class AlgorithmMetadata:
    """ç®—æ³•å…ƒæ•°æ®"""
    algorithm_name: str  # ç®—æ³•åç§°ï¼ˆå†…éƒ¨å”¯ä¸€æ ‡è¯†ï¼šç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
    display_name: str  # æ˜¾ç¤ºåç§°ï¼ˆç”¨æˆ·è¾“å…¥çš„åŸå§‹ç®—æ³•åç§°ï¼‰
    filename: str  # åŸå§‹æ–‡ä»¶å
    upload_time: float  # ä¸Šä¼ æ—¶é—´æˆ³
    status: AlgorithmStatus = AlgorithmStatus.PENDING
    error_message: Optional[str] = None


class AlgorithmDataset:
    """
    å•ä¸ªç®—æ³•çš„æ•°æ®é›†ç±»
    
    å°è£…å•ä¸ªç®—æ³•çš„æ‰€æœ‰æ•°æ®ã€åˆ†æç»“æœå’Œç»Ÿè®¡ä¿¡æ¯ã€‚
    æ¯ä¸ªç®—æ³•å®ä¾‹ç‹¬ç«‹ç®¡ç†è‡ªå·±çš„åˆ†æå™¨ã€‚
    """
    
    # é¢„å®šä¹‰é¢œè‰²æ–¹æ¡ˆï¼ˆç”¨äºå›¾è¡¨æ˜¾ç¤ºï¼‰
    COLOR_PALETTE = [
        '#1f77b4',  # è“è‰²
        '#ff7f0e',  # æ©™è‰²
        '#2ca02c',  # ç»¿è‰²
        '#d62728',  # çº¢è‰²
        '#9467bd',  # ç´«è‰²
        '#8c564b',  # æ£•è‰²
        '#e377c2',  # ç²‰è‰²
        '#7f7f7f',  # ç°è‰²
    ]
    
    def __init__(self, algorithm_name: str, display_name: str, filename: str, color_index: int = 0):
        """
        åˆå§‹åŒ–ç®—æ³•æ•°æ®é›†
        
        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆå†…éƒ¨å”¯ä¸€æ ‡è¯†ï¼šç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
            display_name: æ˜¾ç¤ºåç§°ï¼ˆç”¨æˆ·è¾“å…¥çš„åŸå§‹ç®—æ³•åç§°ï¼‰
            filename: åŸå§‹æ–‡ä»¶å
            color_index: é¢œè‰²ç´¢å¼•ï¼ˆç”¨äºåˆ†é…å›¾è¡¨é¢œè‰²ï¼‰
        """
        self.metadata = AlgorithmMetadata(
            algorithm_name=algorithm_name,
            display_name=display_name,
            filename=filename,
            upload_time=0.0
        )
        
        # åˆ†æå™¨å®ä¾‹
        self.analyzer: Optional[SPMIDAnalyzer] = None
        
        # æ˜¾ç¤ºæ§åˆ¶
        self.color = self.COLOR_PALETTE[color_index % len(self.COLOR_PALETTE)]
        self.is_active: bool = True  # æ˜¯å¦åœ¨å¯¹æ¯”ä¸­æ˜¾ç¤º
        
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºé‡æ–°åˆ†æï¼‰
        self.record_data: Optional[List[Note]] = None
        self.replay_data: Optional[List[Note]] = None
        
        logger.info(f"âœ… AlgorithmDatasetåˆå§‹åŒ–: {algorithm_name} (æ–‡ä»¶: {filename})")
    
    def load_data(self, record_data: List[Note], replay_data: List[Note]) -> bool:
        """
        åŠ è½½å¹¶åˆ†ææ•°æ®
        
        Args:
            record_data: å½•åˆ¶æ•°æ®
            replay_data: æ’­æ”¾æ•°æ®
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.metadata.status = AlgorithmStatus.LOADING

            # æ¸…é™¤ä¹‹å‰çš„ä¸€è‡´æ€§éªŒè¯çŠ¶æ€ï¼Œç¡®ä¿é‡æ–°éªŒè¯
            self._last_algorithm_hash = None
            self._last_overview_metrics = None

            # ä¿å­˜åŸå§‹æ•°æ®
            self.record_data = record_data
            self.replay_data = replay_data
            
            # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
            self.analyzer = SPMIDAnalyzer()
            self.analyzer.analyze(record_data, replay_data)

            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            self._verify_algorithm_consistency()

            self.metadata.status = AlgorithmStatus.READY
            logger.info(f"âœ… ç®—æ³• {self.metadata.algorithm_name} æ•°æ®åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            self.metadata.status = AlgorithmStatus.ERROR
            self.metadata.error_message = str(e)
            logger.error(f"âŒ ç®—æ³• {self.metadata.algorithm_name} æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.analyzer:
            return {}
        
        return {
            'algorithm_name': self.metadata.algorithm_name,  # å†…éƒ¨å”¯ä¸€æ ‡è¯†
            'display_name': self.metadata.display_name,  # æ˜¾ç¤ºåç§°
            'filename': self.metadata.filename,
            'offset_statistics': self.analyzer.get_offset_statistics() if self.analyzer.note_matcher else {},
            'global_average_delay': self.analyzer.get_global_average_delay() if self.analyzer.note_matcher else 0.0,
            'mean_error': self.analyzer.get_mean_error() if self.analyzer.note_matcher else 0.0,
            'matched_pairs_count': len(self.analyzer.matched_pairs) if hasattr(self.analyzer, 'matched_pairs') else 0,
        }
    
    def get_offset_alignment_data(self) -> List[Dict[str, Union[int, float]]]:
        """
        è·å–åç§»å¯¹é½æ•°æ®
        
        Returns:
            List[Dict[str, Any]]: åç§»å¯¹é½æ•°æ®åˆ—è¡¨
        """
        if not self.analyzer or not self.analyzer.note_matcher:
            return []
        
        return self.analyzer.note_matcher.get_offset_alignment_data()
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥ç®—æ³•æ˜¯å¦å·²å°±ç»ª"""
        return self.metadata.status == AlgorithmStatus.READY and self.analyzer is not None

    def _verify_algorithm_consistency(self) -> None:
        """
        éªŒè¯ç®—æ³•æ•°æ®ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆæŒ‡æ ‡çš„å…·ä½“å¯¹æ¯”

        è®¡ç®—åˆ†æç»“æœçš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹ç›¸åŒè¾“å…¥æ˜¯å¦äº§ç”Ÿç›¸åŒè¾“å‡ºã€‚
        """
        try:
            import hashlib
            import json

            # è®¡ç®—å½“å‰åˆ†æç»“æœçš„å“ˆå¸Œå€¼
            current_hash = self._calculate_algorithm_hash()
            current_metrics = self._calculate_overview_metrics()

            # è·å–ä¹‹å‰ä¿å­˜çš„å“ˆå¸Œå€¼å’ŒæŒ‡æ ‡
            previous_hash = getattr(self, '_last_algorithm_hash', None)
            previous_metrics = getattr(self, '_last_overview_metrics', None)

            if previous_hash is not None and previous_metrics is not None:
                if current_hash == previous_hash:
                    logger.info(f"âœ… ç®—æ³• {self.metadata.algorithm_name} æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
                    logger.info(f"ğŸ“Š æ•°æ®æ¦‚è§ˆæŒ‡æ ‡éªŒè¯: å‡†ç¡®ç‡={current_metrics.get('accuracy_percent', 'N/A')}%, "
                              f"ä¸¢é”¤æ•°={current_metrics.get('drop_hammers_count', 'N/A')}, "
                              f"å¤šé”¤æ•°={current_metrics.get('multi_hammers_count', 'N/A')}, "
                              f"å·²é…å¯¹æ•°={current_metrics.get('matched_pairs_count', 'N/A')}")
                else:
                    logger.warning(f"âš ï¸ ç®—æ³• {self.metadata.algorithm_name} æ•°æ®ä¸€è‡´æ€§è­¦å‘Šï¼šç›¸åŒè¾“å…¥äº§ç”Ÿäº†ä¸åŒè¾“å‡ºï¼")
                    logger.warning(f"  ä¹‹å‰çš„å“ˆå¸Œå€¼: {previous_hash}")
                    logger.warning(f"  å½“å‰çš„å“ˆå¸Œå€¼: {current_hash}")

                    # å¯¹æ¯”å…·ä½“æŒ‡æ ‡
                    self._log_metrics_comparison(previous_metrics, current_metrics)
            else:
                logger.info(f"ğŸ“ ç®—æ³• {self.metadata.algorithm_name} é¦–æ¬¡åˆ†æï¼Œè®°å½•æ•°æ®å“ˆå¸Œå€¼: {current_hash}")
                logger.info(f"ğŸ“Š è®°å½•æ•°æ®æ¦‚è§ˆæŒ‡æ ‡: å‡†ç¡®ç‡={current_metrics.get('accuracy_percent', 'N/A')}%, "
                          f"ä¸¢é”¤æ•°={current_metrics.get('drop_hammers_count', 'N/A')}, "
                          f"å¤šé”¤æ•°={current_metrics.get('multi_hammers_count', 'N/A')}, "
                          f"å·²é…å¯¹æ•°={current_metrics.get('matched_pairs_count', 'N/A')}")

                # è¾“å‡ºè¯¦ç»†çš„ä¸¢é”¤æŒ‰é”®ä¿¡æ¯
                drop_hammers_count = current_metrics.get('drop_hammers_count', 0)
                if drop_hammers_count > 0:
                    logger.info(f"ğŸ” ç®—æ³• {self.metadata.algorithm_name} ä¸¢é”¤æŒ‰é”®è¯¦æƒ…:")
                    drop_hammers = getattr(self.analyzer, 'drop_hammers', [])
                    for i, error_note in enumerate(drop_hammers):
                        if len(error_note.infos) > 0:
                            rec = error_note.infos[0]
                            logger.info(f"  ğŸª“ ä¸¢é”¤{i+1}: æŒ‰é”®ID={rec.keyId}, ç´¢å¼•={rec.index}")

                # è¾“å‡ºè¯¦ç»†çš„å¤šé”¤æŒ‰é”®ä¿¡æ¯
                multi_hammers_count = current_metrics.get('multi_hammers_count', 0)
                if multi_hammers_count > 0:
                    logger.info(f"ğŸ” ç®—æ³• {self.metadata.algorithm_name} å¤šé”¤æŒ‰é”®è¯¦æƒ…:")
                    multi_hammers = getattr(self.analyzer, 'multi_hammers', [])
                    for i, error_note in enumerate(multi_hammers):
                        if len(error_note.infos) > 0:
                            play = error_note.infos[0]
                            logger.info(f"  ğŸ”¨ å¤šé”¤{i+1}: æŒ‰é”®ID={play.keyId}, ç´¢å¼•={play.index}")

            # ä¿å­˜å½“å‰å“ˆå¸Œå€¼å’ŒæŒ‡æ ‡ä¾›ä¸‹æ¬¡æ¯”è¾ƒ
            self._last_algorithm_hash = current_hash
            self._last_overview_metrics = current_metrics

        except Exception as e:
            logger.warning(f"âš ï¸ ç®—æ³• {self.metadata.algorithm_name} ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")

    def _log_metrics_comparison(self, previous_metrics: Dict[str, Any], current_metrics: Dict[str, Any]) -> None:
        """
        è®°å½•æŒ‡æ ‡å¯¹æ¯”ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•ä¸ä¸€è‡´é—®é¢˜

        Args:
            previous_metrics: ä¹‹å‰çš„æŒ‡æ ‡æ•°æ®
            current_metrics: å½“å‰çš„æŒ‡æ ‡æ•°æ®
        """
        try:
            logger.warning("ğŸ” æ•°æ®æ¦‚è§ˆæŒ‡æ ‡å¯¹æ¯”:")

            metrics_to_compare = [
                ('accuracy_percent', 'å‡†ç¡®ç‡(%)'),
                ('drop_hammers_count', 'ä¸¢é”¤æ•°'),
                ('multi_hammers_count', 'å¤šé”¤æ•°'),
                ('matched_pairs_count', 'å·²é…å¯¹éŸ³ç¬¦æ•°'),
                ('total_valid_record', 'æœ‰æ•ˆå½•åˆ¶éŸ³ç¬¦æ•°'),
                ('total_valid_replay', 'æœ‰æ•ˆæ’­æ”¾éŸ³ç¬¦æ•°'),
                ('total_valid_combined', 'æ€»æœ‰æ•ˆéŸ³ç¬¦æ•°')
            ]

            for key, name in metrics_to_compare:
                prev_val = previous_metrics.get(key, 'N/A')
                curr_val = current_metrics.get(key, 'N/A')
                if prev_val != curr_val:
                    logger.warning(f"  âŒ {name}: {prev_val} â†’ {curr_val} (ä¸ä¸€è‡´ï¼)")
                else:
                    logger.info(f"  âœ… {name}: {curr_val} (ä¸€è‡´)")

        except Exception as e:
            logger.warning(f"è®°å½•æŒ‡æ ‡å¯¹æ¯”å¤±è´¥: {e}")

    def _calculate_algorithm_hash(self) -> str:
        """
        è®¡ç®—ç®—æ³•åˆ†æç»“æœçš„å“ˆå¸Œå€¼ï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆæŒ‡æ ‡

        Returns:
            str: åˆ†æç»“æœçš„SHA256å“ˆå¸Œå€¼
        """
        try:
            # è·å–æ•°æ®æ¦‚è§ˆæŒ‡æ ‡çš„å…·ä½“æ•°å€¼
            overview_metrics = self._calculate_overview_metrics()

            hash_data = {
                'overview_metrics': overview_metrics,
                'matched_pairs_count': len(getattr(self.analyzer, 'matched_pairs', [])),
                'valid_record_count': len(getattr(self.analyzer, 'valid_record_data', [])),
                'valid_replay_count': len(getattr(self.analyzer, 'valid_replay_data', [])),
                'multi_hammers_count': len(getattr(self.analyzer, 'multi_hammers', [])),
                'drop_hammers_count': len(getattr(self.analyzer, 'drop_hammers', [])),
                'silent_hammers_count': len(getattr(self.analyzer, 'silent_hammers', [])),
            }

            # è®°å½•ä¸¢é”¤è¯¦ç»†ä¿¡æ¯
            drop_hammers = getattr(self.analyzer, 'drop_hammers', [])
            if drop_hammers:
                drop_info = []
                for i, error_note in enumerate(drop_hammers[:10]):  # åªè®°å½•å‰10ä¸ª
                    if len(error_note.infos) > 0:
                        rec = error_note.infos[0]
                        drop_info.append({
                            'index': i+1,
                            'key_id': rec.keyId,
                            'note_index': rec.index,
                            'key_on': rec.keyOn / 10.0,
                            'key_off': rec.keyOff / 10.0
                        })
                hash_data['drop_hammers_detail'] = drop_info

            # è®°å½•å¤šé”¤è¯¦ç»†ä¿¡æ¯
            multi_hammers = getattr(self.analyzer, 'multi_hammers', [])
            if multi_hammers:
                multi_info = []
                for i, error_note in enumerate(multi_hammers[:10]):  # åªè®°å½•å‰10ä¸ª
                    if len(error_note.infos) > 0:
                        play = error_note.infos[0]
                        multi_info.append({
                            'index': i+1,
                            'key_id': play.keyId,
                            'note_index': play.index,
                            'key_on': play.keyOn / 10.0,
                            'key_off': play.keyOff / 10.0
                        })
                hash_data['multi_hammers_detail'] = multi_info

            # æ·»åŠ matched_pairsçš„è¯¦ç»†ä¿¡æ¯
            if hasattr(self.analyzer, 'matched_pairs') and self.analyzer.matched_pairs:
                pairs_info = []
                for i, (r_idx, p_idx, r_note, p_note) in enumerate(self.analyzer.matched_pairs[:5]):
                    pairs_info.append({
                        'record_index': r_idx,
                        'replay_index': p_idx,
                        'record_note_id': getattr(r_note, 'id', None),
                        'replay_note_id': getattr(p_note, 'id', None)
                    })
                hash_data['matched_pairs_sample'] = pairs_info

            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶è®¡ç®—å“ˆå¸Œ
            hash_string = json.dumps(hash_data, sort_keys=True, default=str)
            return hashlib.sha256(hash_string.encode('utf-8')).hexdigest()

        except Exception as e:
            logger.warning(f"è®¡ç®—ç®—æ³•å“ˆå¸Œå¤±è´¥: {e}")
            return "hash_calculation_failed"

    def _calculate_overview_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—æ•°æ®æ¦‚è§ˆä¸­çš„å…³é”®æŒ‡æ ‡ï¼Œç”¨äºä¸€è‡´æ€§éªŒè¯

        Returns:
            Dict[str, Any]: åŒ…å«æ•°æ®æ¦‚è§ˆæŒ‡æ ‡çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨ä¸UIç›¸åŒçš„è®¡ç®—é€»è¾‘
            initial_valid_record = getattr(self.analyzer, 'initial_valid_record_data', None)
            initial_valid_replay = getattr(self.analyzer, 'initial_valid_replay_data', None)

            total_valid_record = len(initial_valid_record) if initial_valid_record else 0
            total_valid_replay = len(initial_valid_replay) if initial_valid_replay else 0

            matched_pairs = getattr(self.analyzer, 'matched_pairs', [])
            drop_hammers = getattr(self.analyzer, 'drop_hammers', [])
            multi_hammers = getattr(self.analyzer, 'multi_hammers', [])

            matched_count = len(matched_pairs)
            total_valid = total_valid_record + total_valid_replay
            accuracy = (matched_count * 2 / total_valid * 100) if total_valid > 0 else 0.0

            return {
                'accuracy_percent': round(accuracy, 1),
                'drop_hammers_count': len(drop_hammers),
                'multi_hammers_count': len(multi_hammers),
                'matched_pairs_count': matched_count,
                'total_valid_record': total_valid_record,
                'total_valid_replay': total_valid_replay,
                'total_valid_combined': total_valid
            }

        except Exception as e:
            logger.warning(f"è®¡ç®—æ¦‚è§ˆæŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}


class MultiAlgorithmManager:
    """
    å¤šç®—æ³•å¯¹æ¯”ç®¡ç†å™¨ç±»
    
    è´Ÿè´£ç®¡ç†å¤šä¸ªç®—æ³•æ•°æ®é›†ï¼Œæ”¯æŒï¼š
    - æ·»åŠ /åˆ é™¤ç®—æ³•
    - å¹¶å‘åŠ è½½å¤šä¸ªç®—æ³•
    - ç®—æ³•çŠ¶æ€ç®¡ç†
    - ç®—æ³•æ˜¾ç¤ºæ§åˆ¶
    """
    
    def __init__(self, max_algorithms: Optional[int] = None):
        """
        åˆå§‹åŒ–å¤šç®—æ³•ç®¡ç†å™¨
        
        Args:
            max_algorithms: æœ€å¤§ç®—æ³•æ•°é‡ï¼ˆNoneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        """
        self.algorithms: Dict[str, AlgorithmDataset] = {}  # algorithm_name -> AlgorithmDataset
        self.max_algorithms = max_algorithms
        # çº¿ç¨‹æ± ç”¨äºå¹¶å‘å¤„ç†ï¼Œå¦‚æœæ— é™åˆ¶åˆ™ä½¿ç”¨é»˜è®¤å€¼10
        executor_workers = max_algorithms if max_algorithms is not None else 10
        self.executor = ThreadPoolExecutor(max_workers=executor_workers)
        
        limit_text = "æ— é™åˆ¶" if max_algorithms is None else str(max_algorithms)
        logger.info(f"âœ… MultiAlgorithmManageråˆå§‹åŒ–å®Œæˆ (æœ€å¤§ç®—æ³•æ•°: {limit_text})")
    
    def get_algorithm_count(self) -> int:
        """è·å–å½“å‰ç®—æ³•æ•°é‡"""
        return len(self.algorithms)
    
    def can_add_algorithm(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ æ–°ç®—æ³•"""
        if self.max_algorithms is None:
            return True  # æ— é™åˆ¶
        return self.get_algorithm_count() < self.max_algorithms
    
    def validate_algorithm_name(self, algorithm_name: str) -> Tuple[bool, str]:
        """
        éªŒè¯ç®—æ³•åç§°æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if not algorithm_name or not algorithm_name.strip():
            return False, "ç®—æ³•åç§°ä¸èƒ½ä¸ºç©º"
        
        algorithm_name = algorithm_name.strip()
        
        if algorithm_name in self.algorithms:
            return False, f"ç®—æ³•åç§° '{algorithm_name}' å·²å­˜åœ¨"
        
        return True, ""
    
    def _generate_unique_algorithm_name(self, algorithm_name: str, filename: str) -> str:
        """
        ç”Ÿæˆå”¯ä¸€çš„ç®—æ³•åç§°ï¼ˆç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
        
        Args:
            algorithm_name: ç”¨æˆ·è¾“å…¥çš„ç®—æ³•åç§°
            filename: æ–‡ä»¶å
            
        Returns:
            str: å”¯ä¸€çš„ç®—æ³•åç§°
        """
        import os
        # å»æ‰è·¯å¾„å’Œæ‰©å±•åï¼Œåªä¿ç•™æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰
        basename = os.path.basename(filename)
        filename_without_ext = os.path.splitext(basename)[0]
        # ç”Ÿæˆç»„åˆåç§°ï¼šç®—æ³•å_æ–‡ä»¶å
        unique_name = f"{algorithm_name}_{filename_without_ext}"
        return unique_name
    
    async def add_algorithm_async(self, algorithm_name: str, filename: str,
                                  record_data: List[Note], replay_data: List[Note]) -> Tuple[bool, str]:
        """
        å¼‚æ­¥æ·»åŠ ç®—æ³•ï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰
        
        ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘å¤„ç†ï¼Œå› ä¸ºæ•°æ®åˆ†ææ˜¯ CPU å¯†é›†å‹ä»»åŠ¡ã€‚
        è‡ªåŠ¨é€šè¿‡"ç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰"ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼ŒåŒºåˆ†åŒç§ç®—æ³•çš„ä¸åŒæ›²å­ã€‚
        
        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆç”¨æˆ·è¾“å…¥çš„åŸå§‹åç§°ï¼‰
            filename: æ–‡ä»¶å
            record_data: å½•åˆ¶æ•°æ®
            replay_data: æ’­æ”¾æ•°æ®
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        # ç”Ÿæˆå”¯ä¸€çš„ç®—æ³•åç§°ï¼ˆç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
        unique_algorithm_name = self._generate_unique_algorithm_name(algorithm_name, filename)
        
        # éªŒè¯å”¯ä¸€ç®—æ³•åç§°
        is_valid, error_msg = self.validate_algorithm_name(unique_algorithm_name)
        if not is_valid:
            return False, error_msg
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ•°é‡
        if not self.can_add_algorithm():
            limit_text = str(self.max_algorithms) if self.max_algorithms is not None else "æ— é™åˆ¶"
            return False, f"å·²è¾¾åˆ°æœ€å¤§ç®—æ³•æ•°é‡é™åˆ¶ ({limit_text})"
        
        # åˆ›å»ºç®—æ³•æ•°æ®é›†ï¼ˆä½¿ç”¨å”¯ä¸€åç§°ä½œä¸ºå†…éƒ¨æ ‡è¯†ï¼ŒåŸå§‹åç§°ä½œä¸ºæ˜¾ç¤ºåç§°ï¼‰
        color_index = len(self.algorithms)
        algorithm = AlgorithmDataset(unique_algorithm_name, algorithm_name, filename, color_index)
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ•°æ®åŠ è½½ï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ›´é«˜æ•ˆï¼‰
        loop = asyncio.get_event_loop()
        success = await loop.run_in_executor(
            self.executor,
            algorithm.load_data,
            record_data,
            replay_data
        )
        
        if success:
            self.algorithms[unique_algorithm_name] = algorithm
            logger.info(f"âœ… ç®—æ³• '{algorithm_name}' (æ–‡ä»¶: {filename}) æ·»åŠ æˆåŠŸï¼Œå†…éƒ¨æ ‡è¯†: '{unique_algorithm_name}'")
            return True, ""
        else:
            error_msg = algorithm.metadata.error_message or "æœªçŸ¥é”™è¯¯"
            logger.error(f"âŒ ç®—æ³• '{algorithm_name}' (æ–‡ä»¶: {filename}) æ·»åŠ å¤±è´¥: {error_msg}")
            return False, error_msg
    
    def remove_algorithm(self, algorithm_name: str) -> bool:
        """
        ç§»é™¤ç®—æ³•
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if algorithm_name not in self.algorithms:
            return False
        
        del self.algorithms[algorithm_name]
        logger.info(f"âœ… ç®—æ³• '{algorithm_name}' å·²ç§»é™¤")
        return True
    
    def get_algorithm(self, algorithm_name: str) -> Optional[AlgorithmDataset]:
        """è·å–æŒ‡å®šç®—æ³•"""
        return self.algorithms.get(algorithm_name)
    
    def get_all_algorithms(self) -> List[AlgorithmDataset]:
        """è·å–æ‰€æœ‰ç®—æ³•åˆ—è¡¨"""
        return list(self.algorithms.values())
    
    def get_active_algorithms(self) -> List[AlgorithmDataset]:
        """è·å–æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯”æ˜¾ç¤ºï¼‰"""
        return [alg for alg in self.algorithms.values() if alg.is_active and alg.is_ready()]
    
    def toggle_algorithm(self, algorithm_name: str) -> bool:
        """
        åˆ‡æ¢ç®—æ³•çš„æ˜¾ç¤º/éšè—çŠ¶æ€
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if algorithm_name not in self.algorithms:
            return False
        
        algorithm = self.algorithms[algorithm_name]
        algorithm.is_active = not algorithm.is_active
        logger.info(f"âœ… ç®—æ³• '{algorithm_name}' æ˜¾ç¤ºçŠ¶æ€: {'æ˜¾ç¤º' if algorithm.is_active else 'éšè—'}")
        return True
    
    def rename_algorithm(self, old_name: str, new_name: str) -> bool:
        """
        é‡å‘½åç®—æ³•
        
        Args:
            old_name: æ—§åç§°
            new_name: æ–°åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if old_name not in self.algorithms:
            return False
        
        if new_name in self.algorithms:
            return False  # æ–°åç§°å·²å­˜åœ¨
        
        algorithm = self.algorithms.pop(old_name)
        algorithm.metadata.algorithm_name = new_name
        self.algorithms[new_name] = algorithm
        
        logger.info(f"âœ… ç®—æ³•é‡å‘½å: '{old_name}' -> '{new_name}'")
        return True
    
    def clear_all(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç®—æ³•"""
        self.algorithms.clear()
        logger.info("âœ… æ‰€æœ‰ç®—æ³•å·²æ¸…ç©º")
    
    def get_comparison_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰ç®—æ³•çš„å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        """
        active_algorithms = self.get_active_algorithms()
        
        if not active_algorithms:
            return {}
        
        comparison_data = {}
        for algorithm in active_algorithms:
            comparison_data[algorithm.metadata.algorithm_name] = algorithm.get_statistics()
        
        return comparison_data

