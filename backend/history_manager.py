import sqlite3
from datetime import datetime
import threading
import os
import json
import hashlib
import traceback
from utils.logger import Logger

logger = Logger.get_logger()

class HistoryManager:
    """åŽ†å²è®°å½•ç®¡ç†å™¨ spmid_historyè¡¨"""

    def __init__(self, db_path=None, disable_database=False):
        """
        åˆå§‹åŒ–åŽ†å²è®°å½•ç®¡ç†å™¨

        Args:
            db_path: æ•°æ®åº“è·¯å¾„
            disable_database: æ˜¯å¦ç¦ç”¨æ•°æ®åº“åŠŸèƒ½ï¼ˆç”¨äºŽæµ‹è¯•æˆ–ç‰¹å®šçŽ¯å¢ƒï¼‰
        """
        self.disable_database = disable_database or os.environ.get('DISABLE_DATABASE', 'false').lower() == 'true'

        if not self.disable_database:
            # å¦‚æžœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œåˆ™åœ¨backendç›®å½•ä¸‹åˆ›å»ºæ•°æ®åº“
            if db_path is None:
                backend_dir = os.path.dirname(os.path.abspath(__file__))
                self.db_path = os.path.join(backend_dir, "history_spmid.db")
            else:
                self.db_path = db_path

            self._lock = threading.RLock()
            self.init_database()
        else:
            # ç¦ç”¨æ•°æ®åº“æ—¶ï¼Œä½¿ç”¨å†…å­˜å­˜å‚¨ä½œä¸ºæ›¿ä»£
            self.db_path = None
            self._lock = threading.RLock()
            self._memory_storage = []

        # åªåœ¨ä¸»è¿›ç¨‹ä¸­è®°å½•åˆå§‹åŒ–æ—¥å¿—ï¼ˆé¿å…Flask debugæ¨¡å¼ä¸‹çš„é‡å¤æ—¥å¿—ï¼‰
        if os.environ.get("WERKZEUG_RUN_MAIN") == "true":
            pass

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æž„ - æ ¹æ®å®žé™…è¡¨ç»“æž„"""
        if self.disable_database:
            return

        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute('''
                SELECT name FROM sqlite_master WHERE type='table' AND name='spmid_history'
            ''')
            table_exists = cursor.fetchone() is not None

            if not table_exists:
                # åˆ›å»ºæ–°è¡¨ - æ·»åŠ file_contentå­—æ®µç”¨äºŽå­˜å‚¨åŽŸå§‹æ–‡ä»¶å†…å®¹
                cursor.execute('''
                    CREATE TABLE spmid_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        file_hash TEXT NOT NULL UNIQUE,
                        file_content BLOB,
                        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        multi_hammers_count INTEGER DEFAULT 0,
                        drop_hammers_count INTEGER DEFAULT 0,
                        total_errors INTEGER DEFAULT 0,
                        file_size INTEGER DEFAULT 0
                    )
                ''')
            else:
                # è¡¨å·²å­˜åœ¨ï¼Œæ£€æŸ¥åˆ—ç»“æž„æ˜¯å¦å®Œæ•´
                cursor.execute("PRAGMA table_info(spmid_history)")
                columns = [column[1] for column in cursor.fetchall()]

                # éœ€è¦çš„åˆ—åŠå…¶å®šä¹‰ - æ·»åŠ file_contentå­—æ®µ
                required_columns = {
                    'file_hash': 'TEXT NOT NULL',
                    'file_content': 'BLOB',
                    'upload_time': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',
                    'multi_hammers_count': 'INTEGER DEFAULT 0',
                    'drop_hammers_count': 'INTEGER DEFAULT 0',
                    'total_errors': 'INTEGER DEFAULT 0',
                    'file_size': 'INTEGER DEFAULT 0'
                }

                # æ·»åŠ ç¼ºå¤±çš„åˆ—
                for column_name, column_definition in required_columns.items():
                    if column_name not in columns:
                        try:
                            cursor.execute(f'ALTER TABLE spmid_history ADD COLUMN {column_name} {column_definition}')
                            logger.info(f"âœ… æ·»åŠ åˆ—: {column_name}")
                        except sqlite3.OperationalError as e:
                            logger.info(f"âš ï¸ æ·»åŠ åˆ— {column_name} å¤±è´¥: {e}")

            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            try:
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_spmid_history_upload_time 
                    ON spmid_history (upload_time)
                ''')
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_spmid_history_filename 
                    ON spmid_history (filename)
                ''')
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_spmid_history_file_hash 
                    ON spmid_history (file_hash)
                ''')
            except sqlite3.OperationalError as e:
                logger.info(f"âš ï¸ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")

            conn.commit()
            conn.close()

    def save_analysis_result(self, filename, backend, upload_id=None, file_content=None):
        """ä¿å­˜åˆ†æžç»“æžœåˆ°æ•°æ®åº“ - ä¿®å¤æ–‡ä»¶å†…å®¹ä¿å­˜é€»è¾‘"""

        if self.disable_database:
            logger.info(f"âš ï¸ æ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡ä¿å­˜åˆ†æžç»“æžœ: {filename}")
            # è¿”å›žä¸€ä¸ªå‡çš„IDç”¨äºŽå…¼å®¹æ€§
            return 999999

        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                # å‡†å¤‡åˆ†æžæ•°æ®
                multi_hammers_count = len(backend.multi_hammers) if hasattr(backend, 'multi_hammers') else 0
                drop_hammers_count = len(backend.drop_hammers) if hasattr(backend, 'drop_hammers') else 0
                total_errors = multi_hammers_count + drop_hammers_count

                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„file_contentï¼Œç„¶åŽå°è¯•ä»ŽbackendèŽ·å–
                if not file_content and hasattr(backend, 'original_file_content'):
                    file_content = backend.original_file_content
                    logger.info("âœ… ä»ŽbackendèŽ·å–åŽŸå§‹æ–‡ä»¶å†…å®¹")

                # éªŒè¯æ–‡ä»¶å†…å®¹
                if file_content:
                    if isinstance(file_content, str):
                        # å¦‚æžœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºbytes
                        file_content = file_content.encode('utf-8')

                    file_hash = hashlib.md5(file_content).hexdigest()
                    file_size = len(file_content)
                    logger.info(f"ðŸ“Š æ–‡ä»¶ä¿¡æ¯: å¤§å° {file_size} å­—èŠ‚, å“ˆå¸Œ {file_hash[:8]}...")
                else:
                    # å¦‚æžœæ²¡æœ‰æ–‡ä»¶å†…å®¹ï¼Œç”ŸæˆåŸºäºŽæ–‡ä»¶åçš„å“ˆå¸Œï¼Œä½†ä¸åˆ é™¤çŽ°æœ‰è®°å½•
                    file_hash = hashlib.md5(f"{filename}_{datetime.now().isoformat()}".encode()).hexdigest()
                    file_size = 0
                    logger.info("âš ï¸ æ²¡æœ‰æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨æ–‡ä»¶å+æ—¶é—´æˆ³ç”Ÿæˆå“ˆå¸Œ")

                # ä¿®å¤ï¼šåªæœ‰å½“æœ‰çœŸå®žæ–‡ä»¶å†…å®¹æ—¶æ‰åˆ é™¤é‡å¤è®°å½•
                # é¿å…å› ä¸ºæ–‡ä»¶åç›¸åŒè€Œè¯¯åˆ å…¶ä»–æœ‰æ•ˆè®°å½•
                if file_content and file_size > 0:
                    cursor.execute('SELECT id FROM spmid_history WHERE file_hash = ?', (file_hash,))
                    existing_record = cursor.fetchone()
                    if existing_record:
                        logger.info(f"ðŸ”„ å‘çŽ°é‡å¤æ–‡ä»¶(å“ˆå¸Œ: {file_hash[:8]}...)ï¼Œæ›´æ–°çŽ°æœ‰è®°å½• ID: {existing_record[0]}")
                        # æ›´æ–°çŽ°æœ‰è®°å½•è€Œä¸æ˜¯åˆ é™¤é‡æ–°åˆ›å»º
                        cursor.execute('''
                            UPDATE spmid_history 
                            SET filename = ?, multi_hammers_count = ?, drop_hammers_count = ?, 
                                total_errors = ?, file_size = ?, upload_time = CURRENT_TIMESTAMP
                            WHERE file_hash = ?
                        ''', (filename, multi_hammers_count, drop_hammers_count, total_errors, file_size, file_hash))

                        history_id = existing_record[0]
                        conn.commit()
                        logger.info(f"âœ… æ›´æ–°çŽ°æœ‰è®°å½•å®Œæˆï¼Œè®°å½•ID: {history_id}")
                        return history_id

                # æ’å…¥æ–°è®°å½•åˆ°spmid_historyè¡¨
                cursor.execute('''
                    INSERT INTO spmid_history 
                    (filename, file_hash, file_content, multi_hammers_count, drop_hammers_count, total_errors, file_size)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (filename, file_hash, file_content, multi_hammers_count, drop_hammers_count, total_errors, file_size))

                history_id = cursor.lastrowid
                conn.commit()

                logger.info(f"âœ… åˆ†æžç»“æžœå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œè®°å½•ID: {history_id}")
                logger.info(f"ðŸ“Š ä¿å­˜ä¿¡æ¯: å¤šé”¤ {multi_hammers_count} ä¸ª, ä¸¢é”¤ {drop_hammers_count} ä¸ª, æ€»è®¡ {total_errors} ä¸ª")
                logger.info(f"ðŸ’¾ æ–‡ä»¶å†…å®¹ä¿å­˜çŠ¶æ€: {'å·²ä¿å­˜' if file_content else 'æœªä¿å­˜'}")
                return history_id

            except Exception as e:
                logger.info(f"âŒ ä¿å­˜åˆ†æžç»“æžœå¤±è´¥: {e}")
                traceback.print_exc()
                conn.rollback()  # æ·»åŠ å›žæ»šæ“ä½œ
                return None
            finally:
                conn.close()

    def get_record_details(self, record_id):
        """èŽ·å–ç‰¹å®šè®°å½•çš„è¯¦ç»†ä¿¡æ¯ - ä¿®å¤æ–‡ä»¶å†…å®¹èŽ·å–é€»è¾‘"""
        if self.disable_database:
            logger.info(f"âš ï¸ æ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•èŽ·å–è®°å½•è¯¦æƒ…: {record_id}")
            return None

        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                # ä»Žspmid_historyè¡¨èŽ·å–è®°å½• - åŒ…å«æ–‡ä»¶å†…å®¹
                cursor.execute('''
                    SELECT id, filename, upload_time, multi_hammers_count, drop_hammers_count, 
                           total_errors, file_hash, file_size, file_content
                    FROM spmid_history WHERE id = ?
                ''', (record_id,))

                main_record = cursor.fetchone()
                if not main_record:
                    logger.info(f"âŒ æœªæ‰¾åˆ°è®°å½• ID: {record_id}")
                    return None

                logger.info(f"ðŸ“‹ æŸ¥è¯¢åˆ°è®°å½•: {main_record[1]} (ID: {main_record[0]})")

                # ä¿®å¤ï¼šå®‰å…¨èŽ·å–æ–‡ä»¶å†…å®¹å¹¶éªŒè¯
                file_content = main_record[8] if len(main_record) > 8 else None
                file_size = main_record[7] if len(main_record) > 7 else 0

                # éªŒè¯æ–‡ä»¶å†…å®¹çš„å®Œæ•´æ€§
                if file_content:
                    actual_size = len(file_content)
                    logger.info(f"ðŸ“Š æ–‡ä»¶å†…å®¹: å£°æ˜Žå¤§å° {file_size} å­—èŠ‚, å®žé™…å¤§å° {actual_size} å­—èŠ‚")
                    if actual_size == 0:
                        logger.info("âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œå¯èƒ½å­˜åœ¨ä¿å­˜é—®é¢˜")
                        file_content = None
                else:
                    logger.info("âš ï¸ è¯¥åŽ†å²è®°å½•æ²¡æœ‰ä¿å­˜æ–‡ä»¶å†…å®¹")

                return {
                    'main_record': main_record,
                    'file_content': file_content,
                    'error_details': []
                }

            except Exception as e:
                logger.info(f"âŒ èŽ·å–è®°å½•è¯¦æƒ…å¤±è´¥: {e}")
                traceback.print_exc()
                return None
            finally:
                conn.close()

    def get_history_list(self, limit=50):
        """èŽ·å–åŽ†å²è®°å½•åˆ—è¡¨ - æ ¹æ®å®žé™…è¡¨ç»“æž„"""
        if self.disable_database:
            logger.info(f"âš ï¸ æ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›žç©ºçš„åŽ†å²è®°å½•åˆ—è¡¨")
            return []

        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                cursor.execute('''
                    SELECT id, filename, upload_time, multi_hammers_count, drop_hammers_count, 
                           total_errors, file_size
                    FROM spmid_history 
                    ORDER BY upload_time DESC 
                    LIMIT ?
                ''', (limit,))

                results = cursor.fetchall()
                logger.info(f"ðŸ“Š ä»Žspmid_historyè¡¨æŸ¥è¯¢åˆ° {len(results)} æ¡åŽ†å²è®°å½•")

                # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                history_list = []
                for row in results:
                    history_list.append({
                        'id': row[0],
                        'filename': row[1],
                        'upload_time': row[2],  # ä½¿ç”¨å®žé™…çš„upload_timeå­—æ®µ
                        'timestamp': row[2],    # å…¼å®¹æ€§å­—æ®µå
                        'multi_hammers_count': row[3],  # ä½¿ç”¨å®žé™…å­—æ®µå
                        'multi_hammers': row[3],        # å…¼å®¹æ€§å­—æ®µå
                        'drop_hammers_count': row[4],   # ä½¿ç”¨å®žé™…å­—æ®µå
                        'drop_hammers': row[4],         # å…¼å®¹æ€§å­—æ®µå
                        'total_errors': row[5],
                        'file_size': row[6] or 0,
                        'analysis_duration': 0.0  # å®žé™…è¡¨ä¸­æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                    })

                return history_list

            except Exception as e:
                logger.info(f"âŒ èŽ·å–åŽ†å²è®°å½•åˆ—è¡¨å¤±è´¥: {e}")
                traceback.print_exc()
                return []
            finally:
                conn.close()

    def add_record(self, filename, file_content, analysis_result, analysis_duration=0, notes=""):
        """æ·»åŠ SPMIDæ–‡ä»¶è®°å½• - æ ¹æ®å®žé™…è¡¨ç»“æž„"""

        file_hash = hashlib.md5(file_content).hexdigest()

        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                # å…ˆåˆ é™¤å¯èƒ½å­˜åœ¨çš„åŒæ–‡ä»¶è®°å½•ï¼Œé¿å…é‡å¤
                cursor.execute('DELETE FROM spmid_history WHERE file_hash = ?', (file_hash,))

                # æ’å…¥ä¸»è®°å½•åˆ°spmid_historyè¡¨ - åªä½¿ç”¨å®žé™…å­˜åœ¨çš„åˆ—
                cursor.execute('''
                    INSERT INTO spmid_history 
                    (filename, file_hash, multi_hammers_count, drop_hammers_count, total_errors, file_size)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    filename,
                    file_hash,
                    analysis_result.get('multi_hammers_count', 0),
                    analysis_result.get('drop_hammers_count', 0),
                    analysis_result.get('total_errors', 0),
                    len(file_content)
                ))

                history_id = cursor.lastrowid
                conn.commit()

                logger.info(f"âœ… åŽ†å²è®°å½•å·²ä¿å­˜åˆ°spmid_historyè¡¨: {filename}, ID: {history_id}")
                return history_id

            except Exception as e:
                logger.info(f"âŒ æ·»åŠ åŽ†å²è®°å½•å¤±è´¥: {e}")
                traceback.print_exc()
                return None
            finally:
                conn.close()

    def delete_record(self, record_id):
        """åˆ é™¤æŒ‡å®šçš„åŽ†å²è®°å½•"""
        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                cursor.execute('DELETE FROM spmid_history WHERE id = ?', (record_id,))
                deleted_count = cursor.rowcount
                conn.commit()

                if deleted_count > 0:
                    logger.info(f"âœ… å·²åˆ é™¤åŽ†å²è®°å½• ID: {record_id}")
                    return True
                else:
                    logger.info(f"âš ï¸ æœªæ‰¾åˆ°è¦åˆ é™¤çš„è®°å½• ID: {record_id}")
                    return False

            except Exception as e:
                logger.info(f"âŒ åˆ é™¤åŽ†å²è®°å½•å¤±è´¥: {e}")
                return False
            finally:
                conn.close()

    def clear_all_records(self):
        """æ¸…ç©ºæ‰€æœ‰åŽ†å²è®°å½•"""
        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            try:
                cursor.execute('DELETE FROM spmid_history')
                deleted_count = cursor.rowcount
                conn.commit()

                logger.info(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰åŽ†å²è®°å½•ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•")
                return deleted_count

            except Exception as e:
                logger.info(f"âŒ æ¸…ç©ºåŽ†å²è®°å½•å¤±è´¥: {e}")
                return 0
            finally:
                conn.close()
    
    # ==================== åŽ†å²è®°å½•å¤„ç†ç›¸å…³æ–¹æ³• ====================
    
    def process_history_selection(self, history_id, backend):
        """
        å¤„ç†åŽ†å²è®°å½•é€‰æ‹© - ä»Žæ•°æ®åº“åŠ è½½åŽ†å²è®°å½•å¹¶åˆå§‹åŒ–backendçŠ¶æ€

        Args:
            history_id: åŽ†å²è®°å½•ID
            backend: åŽç«¯å®žä¾‹

        Returns:
            tuple: (success, result_data, error_msg)
                   - success: æ˜¯å¦å¤„ç†æˆåŠŸ
                   - result_data: æˆåŠŸæ—¶çš„ç»“æžœæ•°æ®ï¼ˆåŒ…å«filenameã€main_recordç­‰ï¼‰
                   - error_msg: å¤±è´¥æ—¶çš„é”™è¯¯ä¿¡æ¯
        """
        if self.disable_database:
            logger.info(f"âš ï¸ æ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•å¤„ç†åŽ†å²è®°å½•é€‰æ‹©: {history_id}")
            return False, None, "æ•°æ®åº“åŠŸèƒ½å·²ç¦ç”¨"

        try:
            logger.info(f"ðŸ”„ å¤„ç†åŽ†å²è®°å½•é€‰æ‹©: {history_id}")

            # èŽ·å–åŽ†å²è®°å½•è¯¦æƒ…
            record_details = self.get_record_details(history_id)
            if not record_details:
                return False, None, "åŽ†å²è®°å½•ä¸å­˜åœ¨"
            
            # è§£æžåŽ†å²è®°å½•ä¿¡æ¯
            filename, main_record = self._parse_history_record(record_details)
            
            # åˆå§‹åŒ–åŽ†å²è®°å½•çŠ¶æ€
            self._initialize_history_state(backend, history_id, filename)
            
            # å¤„ç†åŽ†å²è®°å½•
            if 'file_content' in record_details and record_details['file_content']:
                success = self._load_history_file_content(backend, record_details['file_content'])
                if success:
                    result_data = {
                        'filename': filename,
                        'history_id': history_id,
                        'main_record': main_record,
                        'has_file_content': True,
                        'record_count': len(backend.data_manager.spmid_loader.get_record_data()),
                        'replay_count': len(backend.data_manager.spmid_loader.get_replay_data())
                    }
                    return True, result_data, None
                else:
                    return False, None, "åŽ†å²æ–‡ä»¶åˆ†æžå¤±è´¥"
            else:
                result_data = {
                    'filename': filename,
                    'history_id': history_id,
                    'main_record': main_record,
                    'has_file_content': False
                }
                return True, result_data, None
                
        except Exception as e:
            logger.error(f"âŒ åŽ†å²è®°å½•å¤„ç†é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False, None, str(e)
    
    def save_upload_result(self, filename, backend):
        """
        ä¿å­˜ä¸Šä¼ ç»“æžœåˆ°åŽ†å²è®°å½•
        
        Args:
            filename: æ–‡ä»¶å
            backend: åŽç«¯å®žä¾‹
            
        Returns:
            str: åŽ†å²è®°å½•ID
        """
        history_id = self.save_analysis_result(filename, backend)
        self._log_upload_success(filename, backend, history_id)
        return history_id
    
    # ==================== ç§æœ‰æ–¹æ³• ====================
    
    def _initialize_history_state(self, backend, history_id, filename):
        """åˆå§‹åŒ–åŽ†å²è®°å½•çŠ¶æ€"""
        # æ¸…ç†æ•°æ®çŠ¶æ€
        backend.data_manager.clear_data_state()
        
        # è®¾ç½®åŽ†å²è®°å½•æ•°æ®æºä¿¡æ¯
        backend.data_manager.set_history_data_source(history_id, filename)
        backend._data_source = 'history'
        backend._current_history_id = history_id
    
    def _load_history_file_content(self, backend, file_content):
        """åŠ è½½åŽ†å²è®°å½•æ–‡ä»¶å†…å®¹"""
        logger.info("ðŸ”„ ä»Žæ•°æ®åº“é‡æ–°åˆ†æžåŽ†å²æ–‡ä»¶...")
        
        # ç›´æŽ¥åŠ è½½æ–‡ä»¶å†…å®¹ - å·²è§£ç 
        if isinstance(file_content, str):
            decoded_bytes = file_content.encode('utf-8')
        else:
            decoded_bytes = file_content
        success = backend.data_manager.spmid_loader.load_spmid_data(decoded_bytes)
        
        if success:
            logger.info("âœ… åŽ†å²è®°å½•é‡æ–°åˆ†æžå®Œæˆ")
            logger.info(f"ðŸ“Š æ•°æ®ç»Ÿè®¡: å½•åˆ¶ {len(backend.data_manager.spmid_loader.get_record_data())} ä¸ªéŸ³ç¬¦, æ’­æ”¾ {len(backend.data_manager.spmid_loader.get_replay_data())} ä¸ªéŸ³ç¬¦")
        
        return success
    
    def _log_upload_success(self, filename, backend, history_id):
        """è®°å½•æ–‡ä»¶ä¸Šä¼ æˆåŠŸä¿¡æ¯"""
        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ å¤„ç†å®Œæˆ - {filename}")
        logger.info(f"ðŸ“Š æ•°æ®ç»Ÿè®¡: å½•åˆ¶ {len(backend.data_manager.spmid_loader.get_record_data())} ä¸ªéŸ³ç¬¦, æ’­æ”¾ {len(backend.data_manager.spmid_loader.get_replay_data())} ä¸ªéŸ³ç¬¦")
        logger.info(f"ðŸ’¾ åŽ†å²è®°å½•ID: {history_id}")
    
    def _parse_history_record(self, record_details):
        """è§£æžåŽ†å²è®°å½•ä¿¡æ¯"""
        main_record = record_details['main_record']
        # main_recordæ˜¯ä¸€ä¸ªtupleï¼Œæ ¼å¼ä¸º: (id, filename, upload_time, multi_hammers_count, drop_hammers_count, total_errors, file_hash, file_size, file_content)
        if isinstance(main_record, tuple) and len(main_record) >= 2:
            filename = main_record[1] if main_record[1] else 'æœªçŸ¥æ–‡ä»¶'
        else:
            filename = 'æœªçŸ¥æ–‡ä»¶'
        return filename, main_record
