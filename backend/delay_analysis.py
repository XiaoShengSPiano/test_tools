"""
å»¶æ—¶å…³ç³»åˆ†ææ¨¡å—
è´Ÿè´£åˆ†æå»¶æ—¶ä¸æŒ‰é”®ã€å»¶æ—¶ä¸é”¤é€Ÿä¹‹é—´çš„å…³ç³»
"""

import math
import traceback
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
from scipy import stats
from scipy.stats import f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from utils.logger import Logger

logger = Logger.get_logger()


class DelayAnalysis:
    """å»¶æ—¶å…³ç³»åˆ†æå™¨ - åˆ†æå»¶æ—¶ä¸æŒ‰é”®ã€å»¶æ—¶ä¸é”¤é€Ÿä¹‹é—´çš„å…³ç³»"""
    
    def __init__(self, analyzer=None):
        """
        åˆå§‹åŒ–å»¶æ—¶åˆ†æå™¨
        
        Args:
            analyzer: SPMIDAnalyzerå®ä¾‹
        """
        self.analyzer = analyzer
    
    
    def _calculate_descriptive_stats(self, key_groups: Dict[int, List[float]]) -> List[Dict[str, Any]]:
        """è®¡ç®—æ¯ä¸ªæŒ‰é”®çš„æè¿°æ€§ç»Ÿè®¡"""
        stats_list = []
        
        for key_id in sorted(key_groups.keys()):
            offsets = key_groups[key_id]
            if not offsets:
                continue
            
            offsets_ms = [x / 10.0 for x in offsets]  # è½¬æ¢ä¸ºms
            
            stats_list.append({
                'key_id': key_id,
                'count': len(offsets),
                'mean': np.mean(offsets_ms),
                'median': np.median(offsets_ms),
                'std': np.std(offsets_ms),
                'variance': np.var(offsets_ms),
                'min': np.min(offsets_ms),
                'max': np.max(offsets_ms),
                'q25': np.percentile(offsets_ms, 25),
                'q75': np.percentile(offsets_ms, 75)
            })
        
        return stats_list
    
    def _calculate_overall_stats(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        all_offsets = []
        for offsets in key_groups.values():
            all_offsets.extend(offsets)
        
        if not all_offsets:
            return {
                'overall_mean': 0.0,
                'overall_std': 0.0,
                'overall_variance': 0.0,
                'total_count': 0,
                'key_range': (0, 0)
            }
        
        all_offsets_ms = [x / 10.0 for x in all_offsets]
        
        key_ids = sorted(key_groups.keys())
        
        # è®¡ç®—æŒ‰é”®é—´å¹³å‡å»¶æ—¶çš„æå·®
        key_means = []
        for key_id in key_ids:
            offsets = key_groups[key_id]
            if offsets:
                key_means.append(np.mean([x / 10.0 for x in offsets]))
        
        return {
            'overall_mean': np.mean(all_offsets_ms),
            'overall_std': np.std(all_offsets_ms),
            'overall_variance': np.var(all_offsets_ms),
            'total_count': len(all_offsets),
            'key_count': len(key_groups),
            'key_range': (min(key_ids) if key_ids else 0, max(key_ids) if key_ids else 0),
            'key_mean_range': (min(key_means) if key_means else 0.0, max(key_means) if key_means else 0.0),
            'key_mean_range_diff': (max(key_means) - min(key_means)) if key_means else 0.0
        }
    
    def _perform_anova_test(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """æ‰§è¡ŒANOVAæ£€éªŒ"""
        try:
            # å‡†å¤‡æ•°æ®ï¼šæ¯ä¸ªæŒ‰é”®çš„å»¶æ—¶åˆ—è¡¨
            groups = []
            group_labels = []
            
            for key_id in sorted(key_groups.keys()):
                offsets = key_groups[key_id]
                if len(offsets) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬
                    groups.append(offsets)
                    group_labels.append(key_id)
            
            if len(groups) < 2:
                return {
                    'significant': False,
                    'f_statistic': None,
                    'p_value': None,
                    'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒANOVAæ£€éªŒï¼ˆè‡³å°‘éœ€è¦2ä¸ªæŒ‰é”®ï¼Œæ¯ä¸ªæŒ‰é”®è‡³å°‘2ä¸ªæ ·æœ¬ï¼‰'
                }
            
            # æ‰§è¡ŒANOVA
            f_statistic, p_value = f_oneway(*groups)
            
            # æ£€æŸ¥æ–¹å·®é½æ€§ï¼ˆLevene's testï¼‰
            try:
                levene_stat, levene_p = stats.levene(*groups)
                variance_homogeneous = levene_p > 0.05
            except:
                variance_homogeneous = None
                levene_stat = None
                levene_p = None
            
            significant = p_value < 0.05
            
            return {
                'significant': significant,
                'f_statistic': float(f_statistic),
                'p_value': float(p_value),
                'variance_homogeneous': variance_homogeneous,
                'levene_statistic': float(levene_stat) if levene_stat is not None else None,
                'levene_p_value': float(levene_p) if levene_p is not None else None,
                'group_count': len(groups),
                'message': f"ANOVAæ£€éªŒ: F={f_statistic:.4f}, p={p_value:.4f}, {'å­˜åœ¨æ˜¾è‘—å·®å¼‚' if significant else 'ä¸å­˜åœ¨æ˜¾è‘—å·®å¼‚'}"
            }
            
        except Exception as e:
            logger.error(f"ANOVAæ£€éªŒå¤±è´¥: {e}")
            return {
                'significant': False,
                'f_statistic': None,
                'p_value': None,
                'message': f'ANOVAæ£€éªŒå¤±è´¥: {str(e)}'
            }
    
    def _perform_posthoc_test(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """æ‰§è¡Œäº‹åæ£€éªŒï¼ˆTukey HSDï¼‰"""
        try:
            # å‡†å¤‡æ•°æ®ï¼šå°†æ‰€æœ‰å»¶æ—¶å€¼å’Œå¯¹åº”çš„æŒ‰é”®IDå±•å¹³
            data = []
            groups = []
            
            for key_id in sorted(key_groups.keys()):
                offsets = key_groups[key_id]
                if len(offsets) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬
                    data.extend(offsets)
                    groups.extend([str(key_id)] * len(offsets))
            
            if len(set(groups)) < 2:
                return {
                    'significant_pairs': [],
                    'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œäº‹åæ£€éªŒ'
                }
            
            # æ‰§è¡ŒTukey HSDæ£€éªŒ
            tukey_result = pairwise_tukeyhsd(data, groups, alpha=0.05)
            
            # æå–æ˜¾è‘—å·®å¼‚çš„æŒ‰é”®å¯¹
            significant_pairs = []
            
            # tukey_result æ˜¯ä¸€ä¸ª MultipleComparison å¯¹è±¡ï¼ŒåŒ…å« _results_table å±æ€§
            # ä»ç»“æœè¡¨ä¸­æå–æ˜¾è‘—å·®å¼‚å¯¹
            if hasattr(tukey_result, '_results_table'):
                results_table = tukey_result._results_table.data
                # results_table æ ¼å¼: [group1, group2, meandiff, lower, upper, reject, p-adj]
                for row in results_table[1:]:  # è·³è¿‡è¡¨å¤´
                    if len(row) >= 7:
                        group1 = row[0]
                        group2 = row[1]
                        meandiff = row[2]
                        p_adj = row[6]
                        reject = row[5]  # Trueè¡¨ç¤ºæ˜¾è‘—å·®å¼‚
                        
                        if reject:  # æˆ–è€…ä½¿ç”¨ p_adj < 0.05
                            significant_pairs.append({
                                'key1': int(group1),
                                'key2': int(group2),
                                'p_value': float(p_adj),
                                'meandiff': float(meandiff)
                            })
            
            return {
                'significant_pairs': significant_pairs,
                'total_pairs': len(significant_pairs),
                'message': f'äº‹åæ£€éªŒå®Œæˆï¼Œå‘ç°{len(significant_pairs)}å¯¹æŒ‰é”®å­˜åœ¨æ˜¾è‘—å·®å¼‚'
            }
            
        except Exception as e:
            logger.error(f"äº‹åæ£€éªŒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return {
                'significant_pairs': [],
                'message': f'äº‹åæ£€éªŒå¤±è´¥: {str(e)}'
            }
    
    def _identify_anomaly_keys(self, key_groups: Dict[int, List[float]], 
                               overall_stats: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å¼‚å¸¸æŒ‰é”®"""
        anomaly_keys = []
        overall_mean = overall_stats.get('overall_mean', 0.0)
        overall_std = overall_stats.get('overall_std', 0.0)
        
        threshold = 2.0  # 2å€æ ‡å‡†å·®é˜ˆå€¼
        
        for key_id in sorted(key_groups.keys()):
            offsets = key_groups[key_id]
            if not offsets:
                continue
            
            offsets_ms = [x / 10.0 for x in offsets]
            key_mean = np.mean(offsets_ms)
            key_std = np.std(offsets_ms)
            
            # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
            is_anomaly = False
            anomaly_type = None
            
            if abs(key_mean - overall_mean) > threshold * overall_std:
                is_anomaly = True
                if key_mean > overall_mean:
                    anomaly_type = 'å»¶æ—¶åé«˜'
                else:
                    anomaly_type = 'å»¶æ—¶åä½'
            
            if is_anomaly:
                anomaly_keys.append({
                    'key_id': key_id,
                    'mean_delay': key_mean,
                    'std_delay': key_std,
                    'count': len(offsets),
                    'anomaly_type': anomaly_type,
                    'deviation': key_mean - overall_mean,
                    'deviation_std': (key_mean - overall_mean) / overall_std if overall_std > 0 else 0
                })
        
        # æŒ‰åå·®å¤§å°æ’åº
        anomaly_keys.sort(key=lambda x: abs(x['deviation']), reverse=True)
        
        return anomaly_keys
    
    def _analyze_difference_pattern(self, descriptive_stats: List[Dict[str, Any]], 
                                   overall_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå·®å¼‚æ¨¡å¼"""
        if not descriptive_stats:
            return {
                'has_pattern': False,
                'pattern_type': None,
                'message': 'æ— æ•°æ®'
            }
        
        key_ids = [s['key_id'] for s in descriptive_stats]
        key_means = [s['mean'] for s in descriptive_stats]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çº¿æ€§è¶‹åŠ¿
        if len(key_ids) >= 3:
            # çº¿æ€§å›å½’
            slope, intercept, r_value, p_value, std_err = stats.linregress(key_ids, key_means)
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨è§„å¾‹
            has_linear_trend = abs(r_value) > 0.3 and p_value < 0.05
            
            # æŒ‰éŸ³åŸŸåˆ†ç»„åˆ†æï¼ˆå‡è®¾æŒ‰é”®IDèŒƒå›´å¤§è‡´å¯¹åº”éŸ³åŸŸï¼‰
            if key_ids:
                low_keys = [k for k in key_ids if k < 60]  # ä½éŸ³åŒº
                mid_keys = [k for k in key_ids if 60 <= k < 80]  # ä¸­éŸ³åŒº
                high_keys = [k for k in key_ids if k >= 80]  # é«˜éŸ³åŒº
                
                low_means = [s['mean'] for s in descriptive_stats if s['key_id'] in low_keys]
                mid_means = [s['mean'] for s in descriptive_stats if s['key_id'] in mid_keys]
                high_means = [s['mean'] for s in descriptive_stats if s['key_id'] in high_keys]
                
                region_stats = {
                    'low': {
                        'count': len(low_keys),
                        'mean': np.mean(low_means) if low_means else 0.0
                    },
                    'mid': {
                        'count': len(mid_keys),
                        'mean': np.mean(mid_means) if mid_means else 0.0
                    },
                    'high': {
                        'count': len(high_keys),
                        'mean': np.mean(high_means) if high_means else 0.0
                    }
                }
            else:
                region_stats = {}
            
            return {
                'has_pattern': has_linear_trend,
                'pattern_type': 'linear_trend' if has_linear_trend else 'no_trend',
                'linear_slope': slope,
                'linear_r_squared': r_value ** 2,
                'linear_p_value': p_value,
                'region_stats': region_stats,
                'message': f'çº¿æ€§è¶‹åŠ¿åˆ†æ: RÂ²={r_value**2:.4f}, p={p_value:.4f}'
            }
        else:
            return {
                'has_pattern': False,
                'pattern_type': None,
                'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†ææ¨¡å¼'
            }
    
    def _calculate_correlation(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """è®¡ç®—ç›¸å…³æ€§"""
        try:
            # çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆçº¿æ€§ç›¸å…³ï¼‰
            pearson_r, pearson_p = stats.pearsonr(velocities, delays)
            
            # æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°ï¼ˆéçº¿æ€§ç›¸å…³ï¼‰
            spearman_r, spearman_p = stats.spearmanr(velocities, delays)
            
            # åˆ¤æ–­ç›¸å…³å¼ºåº¦
            def interpret_correlation(r):
                abs_r = abs(r)
                if abs_r >= 0.7:
                    return 'å¼ºç›¸å…³'
                elif abs_r >= 0.3:
                    return 'ä¸­ç­‰ç›¸å…³'
                else:
                    return 'å¼±ç›¸å…³'
            
            return {
                'pearson_r': float(pearson_r),
                'pearson_p': float(pearson_p),
                'spearman_r': float(spearman_r),
                'spearman_p': float(spearman_p),
                'pearson_strength': interpret_correlation(pearson_r),
                'spearman_strength': interpret_correlation(spearman_r),
                'pearson_significant': pearson_p < 0.05,
                'spearman_significant': spearman_p < 0.05,
                'message': f'çš®å°”é€Šç›¸å…³: r={pearson_r:.4f}, p={pearson_p:.4f}; æ–¯çš®å°”æ›¼ç›¸å…³: r={spearman_r:.4f}, p={spearman_p:.4f}'
            }
            
        except Exception as e:
            logger.error(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            return {
                'pearson_r': None,
                'pearson_p': None,
                'spearman_r': None,
                'spearman_p': None,
                'message': f'ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {str(e)}'
            }
    
    def _perform_regression_analysis(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """æ‰§è¡Œå›å½’åˆ†æ"""
        try:
            # çº¿æ€§å›å½’
            slope, intercept, r_value, p_value, std_err = stats.linregress(velocities, delays)
            
            # å°è¯•å¤šé¡¹å¼å›å½’ï¼ˆ2æ¬¡ï¼‰
            try:
                poly_coeffs = np.polyfit(velocities, delays, 2)
                poly_func = np.poly1d(poly_coeffs)
                poly_r_squared = 1 - (np.sum((delays - poly_func(velocities))**2) / 
                                     np.sum((delays - np.mean(delays))**2))
            except:
                poly_coeffs = None
                poly_r_squared = None
            
            return {
                'linear': {
                    'slope': float(slope),
                    'intercept': float(intercept),
                    'r_squared': float(r_value ** 2),
                    'p_value': float(p_value),
                    'std_err': float(std_err)
                },
                'polynomial': {
                    'coefficients': [float(c) for c in poly_coeffs] if poly_coeffs is not None else None,
                    'r_squared': float(poly_r_squared) if poly_r_squared is not None else None
                },
                'best_fit': 'linear' if poly_r_squared is None or (r_value ** 2) >= poly_r_squared else 'polynomial',
                'message': f'çº¿æ€§å›å½’: RÂ²={r_value**2:.4f}, p={p_value:.4f}'
            }
            
        except Exception as e:
            logger.error(f"å›å½’åˆ†æå¤±è´¥: {e}")
            return {
                'linear': None,
                'polynomial': None,
                'message': f'å›å½’åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_by_velocity_groups(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """æŒ‰é”¤é€ŸåŒºé—´åˆ†ç»„åˆ†æ"""
        try:
            # å®šä¹‰é”¤é€ŸåŒºé—´
            velocity_ranges = [
                (0, 100, 'ä½é”¤é€Ÿ (0-100)'),
                (100, 200, 'ä¸­é”¤é€Ÿ (100-200)'),
                (200, 300, 'é«˜é”¤é€Ÿ (200-300)'),
                (300, float('inf'), 'è¶…é«˜é”¤é€Ÿ (>300)')
            ]
            
            group_stats = []
            
            for v_min, v_max, label in velocity_ranges:
                group_velocities = []
                group_delays = []
                
                for v, d in zip(velocities, delays):
                    if v_min <= v < v_max:
                        group_velocities.append(v)
                        group_delays.append(d)
                
                if group_delays:
                    group_stats.append({
                        'range_label': label,
                        'velocity_min': v_min,
                        'velocity_max': v_max,
                        'count': len(group_delays),
                        'mean_delay': np.mean(group_delays),
                        'std_delay': np.std(group_delays),
                        'mean_velocity': np.mean(group_velocities),
                        'std_velocity': np.std(group_velocities)
                    })
            
            return {
                'groups': group_stats,
                'message': f'æŒ‰é”¤é€ŸåŒºé—´åˆ†ç»„ï¼Œå…±{len(group_stats)}ä¸ªåŒºé—´'
            }
            
        except Exception as e:
            logger.error(f"åˆ†ç»„åˆ†æå¤±è´¥: {e}")
            return {
                'groups': [],
                'message': f'åˆ†ç»„åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def analyze_key_force_interaction(self) -> Dict[str, Any]:
        """
        ç”ŸæˆæŒ‰é”®ä¸åŠ›åº¦çš„äº¤äº’æ•ˆåº”å›¾æ•°æ®

        ç”ŸæˆæŒ‰é”®-åŠ›åº¦äº¤äº’æ•ˆåº”å›¾æ‰€éœ€çš„æ•°æ®ï¼Œç”¨äºå¯è§†åŒ–åˆ†ææŒ‰é”®å’ŒåŠ›åº¦å¯¹å»¶æ—¶çš„è”åˆå½±å“ã€‚

        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - interaction_plot_data: äº¤äº’æ•ˆåº”å›¾æ•°æ®
                - status: çŠ¶æ€æ ‡è¯†
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æ")
                return self._create_empty_interaction_result("åˆ†æå™¨ä¸å­˜åœ¨")

            matched_pairs = self.analyzer.note_matcher.get_matched_pairs()
            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            
            if not matched_pairs or not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return self._create_empty_interaction_result("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # æå–æ•°æ®ï¼šæŒ‰é”®IDã€é”¤é€Ÿã€å»¶æ—¶ã€ç´¢å¼•
            key_force_delay_data = self._extract_key_force_delay_data(matched_pairs, offset_data)
            
            if not key_force_delay_data:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æŒ‰é”®-åŠ›åº¦-å»¶æ—¶æ•°æ®")
                return self._create_empty_interaction_result("æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            
            # å‡†å¤‡æ•°æ®åˆ—è¡¨
            # itemæ ¼å¼: (key_id, replay_velocity, delay_ms, record_idx, replay_idx)
            key_ids_list = [item[0] for item in key_force_delay_data]
            replay_velocities_list = [item[1] for item in key_force_delay_data]
            delays_list = [item[2] for item in key_force_delay_data]
            record_indices_list = [item[3] for item in key_force_delay_data]
            replay_indices_list = [item[4] for item in key_force_delay_data]
            
            # ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ˆåŒ…å«ç´¢å¼•ä¿¡æ¯ï¼‰
            interaction_plot_data = self._generate_interaction_plot_data(
                key_ids_list, replay_velocities_list, delays_list,
                record_indices_list, replay_indices_list
            )
            
            logger.info("æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆ")
            
            return {
                'interaction_plot_data': interaction_plot_data,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self._create_empty_interaction_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def _extract_key_force_delay_data(self, matched_pairs: List[Tuple], 
                                      offset_data: List[Dict[str, Any]]) -> List[Tuple[int, float, float, int, int]]:
        """
        ä»åŒ¹é…å¯¹å’Œåç§»æ•°æ®ä¸­æå–æŒ‰é”®IDã€åŠ›åº¦ï¼ˆé”¤é€Ÿï¼‰ã€å»¶æ—¶æ•°æ®
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨
            offset_data: åç§»å¯¹é½æ•°æ®åˆ—è¡¨
            
        Returns:
            List[Tuple]: [(key_id, replay_velocity, delay_ms, record_idx, replay_idx), ...]
                - key_id: æŒ‰é”®ID
                - replay_velocity: æ’­æ”¾é”¤é€Ÿå€¼
                - delay_ms: å»¶æ—¶ï¼ˆå•ä½ï¼šmsï¼Œå¸¦ç¬¦å·ï¼‰
                - record_idx: å½•åˆ¶éŸ³ç¬¦ç´¢å¼•
                - replay_idx: å›æ”¾éŸ³ç¬¦ç´¢å¼•
        """
        # åˆ›å»ºåŒ¹é…å¯¹ç´¢å¼•åˆ°åç§»æ•°æ®çš„æ˜ å°„
        offset_map = {}
        for item in offset_data:
            record_idx = item.get('record_index')
            replay_idx = item.get('replay_index')
            if record_idx is not None and replay_idx is not None:
                offset_map[(record_idx, replay_idx)] = item
        
        result = []
        
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            # è·å–æŒ‰é”®ID
            key_id = record_note.id
            
            # æå–æ’­æ”¾éŸ³ç¬¦çš„é”¤é€Ÿï¼ˆç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
            replay_velocity = self._extract_first_hammer_velocity(replay_note)
            
            if replay_velocity is None or replay_velocity <= 0:
                continue
            
            # è·å–å»¶æ—¶
            keyon_offset = None
            if (record_idx, replay_idx) in offset_map:
                keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è®¡ç®—
                try:
                    record_keyon, _ = self.analyzer.note_matcher._calculate_note_times(record_note)
                    replay_keyon, _ = self.analyzer.note_matcher._calculate_note_times(replay_note)
                    keyon_offset = replay_keyon - record_keyon
                except:
                    continue
            
            if keyon_offset is not None:
                # è½¬æ¢ä¸ºmså•ä½ï¼ˆå¸¦ç¬¦å·ï¼‰
                delay_ms = keyon_offset / 10.0
                result.append((key_id, float(replay_velocity), delay_ms, record_idx, replay_idx))
        
        return result
    
    def _extract_first_hammer_velocity(self, note) -> Optional[float]:
        """
        æå–éŸ³ç¬¦çš„ç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼
        
        Args:
            note: éŸ³ç¬¦å¯¹è±¡
            
        Returns:
            Optional[float]: ç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        try:
            if not hasattr(note, 'hammers') or note.hammers is None:
                return None
            
            if hasattr(note.hammers, 'empty') and note.hammers.empty:
                return None
            
            if len(note.hammers) == 0:
                return None
            
            # è·å–æ—¶é—´ä¸Šæœ€æ—©çš„é”¤é€Ÿå€¼
            min_timestamp = note.hammers.index.min()
            first_velocity_raw = note.hammers.loc[min_timestamp]
            
            if isinstance(first_velocity_raw, pd.Series):
                return first_velocity_raw.iloc[0]
            else:
                return first_velocity_raw
                
        except Exception as e:
            return None
    
    def _generate_interaction_plot_data(self, key_ids: List[int], replay_velocities: List[float], 
                                       delays: List[float], record_indices: List[int] = None,
                                       replay_indices: List[int] = None) -> Dict[str, Any]:
        """
        ç”ŸæˆæŒ‰é”®-åŠ›åº¦äº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰

        æ¨ªè½´ï¼šlogâ‚â‚€(æ’­æ”¾é”¤é€Ÿ)ï¼ˆç”¨äºåˆ†æï¼‰
        çºµè½´ï¼šç›¸å¯¹å»¶æ—¶ï¼ˆå»¶æ—¶ - å¹³å‡å»¶æ—¶ï¼‰
        
        Args:
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            replay_velocities: æ’­æ”¾é”¤é€Ÿåˆ—è¡¨
            delays: å»¶æ—¶åˆ—è¡¨ï¼ˆå•ä½ï¼šmsï¼‰
            record_indices: å½•åˆ¶éŸ³ç¬¦ç´¢å¼•åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            replay_indices: å›æ”¾éŸ³ç¬¦ç´¢å¼•åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, Any]: äº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ŒåŒ…å«æ¯ä¸ªæŒ‰é”®çš„æ•°æ®
        """
        try:
            from collections import defaultdict

            # ä½¿ç”¨é¢„è®¡ç®—çš„æ•´ä½“å¹³å‡å»¶æ—¶ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
            if hasattr(self, 'analyzer') and self.analyzer and hasattr(self.analyzer, 'get_mean_error'):
                mean_delay_0_1ms = self.analyzer.get_mean_error()
                mean_delay = mean_delay_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’
            else:
                # å¤‡ç”¨è®¡ç®—ï¼ˆå¦‚æœé¢„è®¡ç®—ä¸å¯ç”¨ï¼‰
                mean_delay = np.mean(delays) if delays else 0
            logger.info(f"ğŸ“Š æ•´ä½“å¹³å‡å»¶æ—¶: {mean_delay:.2f}ms")
            
            # è®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼ˆå»¶æ—¶ - å¹³å‡å»¶æ—¶ï¼‰
            relative_delays = [delay - mean_delay for delay in delays]
            
            # æŒ‰æŒ‰é”®åˆ†ç»„
            key_groups = defaultdict(lambda: {'forces': [], 'delays': [], 'absolute_delays': [],
                                               'record_indices': [], 'replay_indices': []})
            for i, (key_id, replay_vel, rel_delay, abs_delay) in enumerate(zip(key_ids, replay_velocities, relative_delays, delays)):
                # 'forces'å­˜å‚¨æ’­æ”¾é”¤é€Ÿ
                # 'delays'å­˜å‚¨ç›¸å¯¹å»¶æ—¶
                # 'absolute_delays'å­˜å‚¨åŸå§‹å»¶æ—¶
                key_groups[key_id]['forces'].append(replay_vel)
                key_groups[key_id]['delays'].append(rel_delay)
                key_groups[key_id]['absolute_delays'].append(abs_delay)
                if record_indices and i < len(record_indices):
                    key_groups[key_id]['record_indices'].append(record_indices[i])
                if replay_indices and i < len(replay_indices):
                    key_groups[key_id]['replay_indices'].append(replay_indices[i])
            
            interaction_data = {}
            
            for key_id in sorted(key_groups.keys()):
                replay_vels_key = key_groups[key_id]['forces']
                relative_delays_key = key_groups[key_id]['delays']  # ç›¸å¯¹å»¶æ—¶
                absolute_delays_key = key_groups[key_id]['absolute_delays']  # åŸå§‹å»¶æ—¶
                record_indices_key = key_groups[key_id].get('record_indices', [])
                replay_indices_key = key_groups[key_id].get('replay_indices', [])
                
                if len(replay_vels_key) < 2:
                    continue
                
                # è¿›è¡Œçº¿æ€§å›å½’ï¼ˆæ’­æ”¾é”¤é€Ÿ vs ç›¸å¯¹å»¶æ—¶ï¼‰
                slope, intercept, r_value, p_value, std_err = stats.linregress(replay_vels_key, relative_delays_key)
                
                # ç”Ÿæˆå›å½’çº¿çš„xå’Œyå€¼ï¼ˆç”¨äºç»˜å›¾ï¼‰
                vel_min = min(replay_vels_key)
                vel_max = max(replay_vels_key)
                vel_range = vel_max - vel_min
                
                # ç”Ÿæˆ10ä¸ªç‚¹ç”¨äºç»˜åˆ¶å›å½’çº¿
                vel_line = np.linspace(vel_min - vel_range * 0.1, 
                                      vel_max + vel_range * 0.1, 10)
                delay_line = slope * vel_line + intercept
                
                interaction_data[key_id] = {
                    'forces': replay_vels_key,
                    'delays': relative_delays_key,  # ç›¸å¯¹å»¶æ—¶
                    'absolute_delays': absolute_delays_key,  # åŸå§‹å»¶æ—¶
                    'record_indices': record_indices_key,
                    'replay_indices': replay_indices_key,
                    'mean_delay': float(mean_delay),  # æ•´ä½“å¹³å‡å»¶æ—¶
                    'regression_line': {
                        'force': [float(v) for v in vel_line],
                        'delay': [float(d) for d in delay_line]
                    },
                    'slope': float(slope),
                    'intercept': float(intercept),
                    'r_squared': float(r_value ** 2),
                    'p_value': float(p_value),
                    'sample_count': len(replay_vels_key)
                }
            
            return {
                'key_data': interaction_data,
                'mean_delay': float(mean_delay),
                'message': f'ç”Ÿæˆ {len(interaction_data)} ä¸ªæŒ‰é”®çš„äº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰'
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®å¤±è´¥: {e}")
            return {
                'key_data': {},
                'message': f'ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®å¤±è´¥: {str(e)}'
            }
    
    def _create_empty_interaction_result(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„äº¤äº’æ•ˆåº”åˆ†æç»“æœ"""
        return {
            'status': 'error',
            'message': message,
            'interaction_plot_data': {}
        }
    
    def _create_empty_result(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'status': 'error',
            'message': message,
            'descriptive_stats': [],
            'overall_stats': {},
            'anova_result': {},
            'posthoc_result': None,
            'anomaly_keys': [],
            'difference_pattern': {}
        }