#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å»¶æ—¶å…³ç³»åˆ†ææ¨¡å—
è´Ÿè´£åˆ†æå»¶æ—¶ä¸æŒ‰é”®ã€å»¶æ—¶ä¸é”¤é€Ÿä¹‹é—´çš„å…³ç³»
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
from scipy import stats
from scipy.stats import f_oneway, kruskal
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from utils.logger import Logger

logger = Logger.get_logger()


class DelayAnalysis:
    """å»¶æ—¶å…³ç³»åˆ†æå™¨ - åˆ†æå»¶æ—¶ä¸æŒ‰é”®ã€å»¶æ—¶ä¸é”¤é€Ÿä¹‹é—´çš„å…³ç³»"""
    
    def __init__(self, analyzer=None):
        """
        åˆå§‹åŒ–å»¶æ—¶åˆ†æå™¨
        
        Args:
            analyzer: SPMIDAnalyzerå®ä¾‹
        """
        self.analyzer = analyzer
    
    def analyze_delay_by_key(self) -> Dict[str, Any]:
        """
        åˆ†æå»¶æ—¶ä¸æŒ‰é”®çš„å…³ç³»
        åŒ…æ‹¬ï¼šæè¿°æ€§ç»Ÿè®¡ã€ANOVAæ£€éªŒã€äº‹åæ£€éªŒã€å¼‚å¸¸æŒ‰é”®è¯†åˆ«
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - descriptive_stats: æ¯ä¸ªæŒ‰é”®çš„æè¿°æ€§ç»Ÿè®¡
                - anova_result: ANOVAæ£€éªŒç»“æœ
                - posthoc_result: äº‹åæ£€éªŒç»“æœ
                - anomaly_keys: å¼‚å¸¸æŒ‰é”®åˆ—è¡¨
                - overall_stats: æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå»¶æ—¶ä¸æŒ‰é”®åˆ†æ")
                return self._create_empty_result("åˆ†æå™¨ä¸å­˜åœ¨")
            
            # è·å–åç§»å¯¹é½æ•°æ®
            offset_data = self.analyzer.note_matcher.get_offset_alignment_data()
            
            if not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åç§»æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return self._create_empty_result("æ²¡æœ‰åç§»æ•°æ®")
            
            # æŒ‰æŒ‰é”®IDåˆ†ç»„å»¶æ—¶æ•°æ®ï¼ˆä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼Œä¿ç•™æ­£è´Ÿå€¼ï¼‰
            key_groups = defaultdict(list)
            for item in offset_data:
                key_id = item.get('key_id', 'N/A')
                keyon_offset = item.get('keyon_offset', 0)  # ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼ˆæ­£æ•°=å»¶è¿Ÿï¼Œè´Ÿæ•°=æå‰ï¼‰
                key_groups[key_id].append(keyon_offset)
            
            # 1. æè¿°æ€§ç»Ÿè®¡
            descriptive_stats = self._calculate_descriptive_stats(key_groups)
            
            # 2. æ•´ä½“ç»Ÿè®¡
            overall_stats = self._calculate_overall_stats(key_groups)
            
            # 3. ANOVAæ£€éªŒ
            anova_result = self._perform_anova_test(key_groups)
            
            # 4. äº‹åæ£€éªŒï¼ˆå¦‚æœANOVAæ˜¾è‘—ï¼‰
            posthoc_result = None
            if anova_result.get('significant', False):
                posthoc_result = self._perform_posthoc_test(key_groups)
            
            # 5. è¯†åˆ«å¼‚å¸¸æŒ‰é”®
            anomaly_keys = self._identify_anomaly_keys(key_groups, overall_stats)
            
            # 6. å·®å¼‚æ¨¡å¼åˆ†æ
            difference_pattern = self._analyze_difference_pattern(descriptive_stats, overall_stats)
            
            # å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºmså•ä½ï¼Œç”¨äºç®±çº¿å›¾
            key_groups_ms = {}
            for key_id, offsets in key_groups.items():
                key_groups_ms[key_id] = [x / 10.0 for x in offsets]  # è½¬æ¢ä¸ºms
            
            return {
                'descriptive_stats': descriptive_stats,
                'overall_stats': overall_stats,
                'anova_result': anova_result,
                'posthoc_result': posthoc_result,
                'anomaly_keys': anomaly_keys,
                'difference_pattern': difference_pattern,
                'key_groups_ms': key_groups_ms,  # æ·»åŠ åŸå§‹æ•°æ®ç”¨äºç®±çº¿å›¾
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"âŒ å»¶æ—¶ä¸æŒ‰é”®åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._create_empty_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def analyze_delay_by_velocity(self) -> Dict[str, Any]:
        """
        åˆ†æå»¶æ—¶ä¸é”¤é€Ÿçš„å…³ç³»
        åŒ…æ‹¬ï¼šç›¸å…³æ€§åˆ†æã€å›å½’åˆ†æã€åˆ†ç»„åˆ†æ
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - correlation_result: ç›¸å…³æ€§åˆ†æç»“æœ
                - regression_result: å›å½’åˆ†æç»“æœ
                - grouped_analysis: åˆ†ç»„åˆ†æç»“æœ
                - scatter_data: æ•£ç‚¹å›¾æ•°æ®
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå»¶æ—¶ä¸é”¤é€Ÿåˆ†æ")
                return self._create_empty_result("åˆ†æå™¨ä¸å­˜åœ¨")
            
            matched_pairs = self.analyzer.note_matcher.get_matched_pairs()
            offset_data = self.analyzer.note_matcher.get_offset_alignment_data()
            
            if not matched_pairs or not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return self._create_empty_result("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # æå–é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®
            velocities = []
            delays = []
            key_ids = []
            
            # åˆ›å»ºåŒ¹é…å¯¹ç´¢å¼•åˆ°åç§»æ•°æ®çš„æ˜ å°„
            offset_map = {}
            for item in offset_data:
                record_idx = item.get('record_index')
                replay_idx = item.get('replay_index')
                if record_idx is not None and replay_idx is not None:
                    offset_map[(record_idx, replay_idx)] = item
            
            # æå–æ•°æ®
            for record_idx, replay_idx, record_note, replay_note in matched_pairs:
                # è·å–ç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼ˆç”¨äºé˜ˆå€¼æ£€æŸ¥çš„é”¤é€Ÿï¼‰
                if len(replay_note.hammers) > 0:
                    min_timestamp = replay_note.hammers.index.min()
                    first_hammer_velocity_raw = replay_note.hammers.loc[min_timestamp]
                    if isinstance(first_hammer_velocity_raw, pd.Series):
                        first_hammer_velocity = first_hammer_velocity_raw.iloc[0]
                    else:
                        first_hammer_velocity = first_hammer_velocity_raw
                    
                    # è·å–å»¶æ—¶
                    keyon_offset = None
                    if (record_idx, replay_idx) in offset_map:
                        keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è®¡ç®—
                        try:
                            record_keyon, _ = self.analyzer.note_matcher._calculate_note_times(record_note)
                            replay_keyon, _ = self.analyzer.note_matcher._calculate_note_times(replay_note)
                            keyon_offset = replay_keyon - record_keyon
                        except:
                            continue
                    
                    if first_hammer_velocity > 0 and keyon_offset is not None:
                        velocities.append(first_hammer_velocity)
                        delays.append(abs(keyon_offset) / 10.0)  # è½¬æ¢ä¸ºms
                        key_ids.append(record_note.id)
            
            if not velocities or not delays:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®")
                return self._create_empty_result("æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            
            # 1. ç›¸å…³æ€§åˆ†æ
            correlation_result = self._calculate_correlation(velocities, delays)
            
            # 2. å›å½’åˆ†æ
            regression_result = self._perform_regression_analysis(velocities, delays)
            
            # 3. åˆ†ç»„åˆ†æï¼ˆæŒ‰é”¤é€ŸåŒºé—´åˆ†ç»„ï¼‰
            grouped_analysis = self._analyze_by_velocity_groups(velocities, delays)
            
            # 4. æ•£ç‚¹å›¾æ•°æ®
            scatter_data = {
                'velocities': velocities,
                'delays': delays,
                'key_ids': key_ids
            }
            
            return {
                'correlation_result': correlation_result,
                'regression_result': regression_result,
                'grouped_analysis': grouped_analysis,
                'scatter_data': scatter_data,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"âŒ å»¶æ—¶ä¸é”¤é€Ÿåˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._create_empty_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def _calculate_descriptive_stats(self, key_groups: Dict[int, List[float]]) -> List[Dict[str, Any]]:
        """è®¡ç®—æ¯ä¸ªæŒ‰é”®çš„æè¿°æ€§ç»Ÿè®¡"""
        stats_list = []
        
        for key_id in sorted(key_groups.keys()):
            offsets = key_groups[key_id]
            if not offsets:
                continue
            
            offsets_ms = [x / 10.0 for x in offsets]  # è½¬æ¢ä¸ºms
            
            stats_list.append({
                'key_id': key_id,
                'count': len(offsets),
                'mean': np.mean(offsets_ms),
                'median': np.median(offsets_ms),
                'std': np.std(offsets_ms),
                'variance': np.var(offsets_ms),
                'min': np.min(offsets_ms),
                'max': np.max(offsets_ms),
                'q25': np.percentile(offsets_ms, 25),
                'q75': np.percentile(offsets_ms, 75)
            })
        
        return stats_list
    
    def _calculate_overall_stats(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        all_offsets = []
        for offsets in key_groups.values():
            all_offsets.extend(offsets)
        
        if not all_offsets:
            return {
                'overall_mean': 0.0,
                'overall_std': 0.0,
                'overall_variance': 0.0,
                'total_count': 0,
                'key_range': (0, 0)
            }
        
        all_offsets_ms = [x / 10.0 for x in all_offsets]
        
        key_ids = sorted(key_groups.keys())
        
        # è®¡ç®—æŒ‰é”®é—´å¹³å‡å»¶æ—¶çš„æå·®
        key_means = []
        for key_id in key_ids:
            offsets = key_groups[key_id]
            if offsets:
                key_means.append(np.mean([x / 10.0 for x in offsets]))
        
        return {
            'overall_mean': np.mean(all_offsets_ms),
            'overall_std': np.std(all_offsets_ms),
            'overall_variance': np.var(all_offsets_ms),
            'total_count': len(all_offsets),
            'key_count': len(key_groups),
            'key_range': (min(key_ids) if key_ids else 0, max(key_ids) if key_ids else 0),
            'key_mean_range': (min(key_means) if key_means else 0.0, max(key_means) if key_means else 0.0),
            'key_mean_range_diff': (max(key_means) - min(key_means)) if key_means else 0.0
        }
    
    def _perform_anova_test(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """æ‰§è¡ŒANOVAæ£€éªŒ"""
        try:
            # å‡†å¤‡æ•°æ®ï¼šæ¯ä¸ªæŒ‰é”®çš„å»¶æ—¶åˆ—è¡¨
            groups = []
            group_labels = []
            
            for key_id in sorted(key_groups.keys()):
                offsets = key_groups[key_id]
                if len(offsets) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬
                    groups.append(offsets)
                    group_labels.append(key_id)
            
            if len(groups) < 2:
                return {
                    'significant': False,
                    'f_statistic': None,
                    'p_value': None,
                    'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒANOVAæ£€éªŒï¼ˆè‡³å°‘éœ€è¦2ä¸ªæŒ‰é”®ï¼Œæ¯ä¸ªæŒ‰é”®è‡³å°‘2ä¸ªæ ·æœ¬ï¼‰'
                }
            
            # æ‰§è¡ŒANOVA
            f_statistic, p_value = f_oneway(*groups)
            
            # æ£€æŸ¥æ–¹å·®é½æ€§ï¼ˆLevene's testï¼‰
            try:
                levene_stat, levene_p = stats.levene(*groups)
                variance_homogeneous = levene_p > 0.05
            except:
                variance_homogeneous = None
                levene_stat = None
                levene_p = None
            
            significant = p_value < 0.05
            
            return {
                'significant': significant,
                'f_statistic': float(f_statistic),
                'p_value': float(p_value),
                'variance_homogeneous': variance_homogeneous,
                'levene_statistic': float(levene_stat) if levene_stat is not None else None,
                'levene_p_value': float(levene_p) if levene_p is not None else None,
                'group_count': len(groups),
                'message': f"ANOVAæ£€éªŒ: F={f_statistic:.4f}, p={p_value:.4f}, {'å­˜åœ¨æ˜¾è‘—å·®å¼‚' if significant else 'ä¸å­˜åœ¨æ˜¾è‘—å·®å¼‚'}"
            }
            
        except Exception as e:
            logger.error(f"ANOVAæ£€éªŒå¤±è´¥: {e}")
            return {
                'significant': False,
                'f_statistic': None,
                'p_value': None,
                'message': f'ANOVAæ£€éªŒå¤±è´¥: {str(e)}'
            }
    
    def _perform_posthoc_test(self, key_groups: Dict[int, List[float]]) -> Dict[str, Any]:
        """æ‰§è¡Œäº‹åæ£€éªŒï¼ˆTukey HSDï¼‰"""
        try:
            # å‡†å¤‡æ•°æ®ï¼šå°†æ‰€æœ‰å»¶æ—¶å€¼å’Œå¯¹åº”çš„æŒ‰é”®IDå±•å¹³
            data = []
            groups = []
            
            for key_id in sorted(key_groups.keys()):
                offsets = key_groups[key_id]
                if len(offsets) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬
                    data.extend(offsets)
                    groups.extend([str(key_id)] * len(offsets))
            
            if len(set(groups)) < 2:
                return {
                    'significant_pairs': [],
                    'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œäº‹åæ£€éªŒ'
                }
            
            # æ‰§è¡ŒTukey HSDæ£€éªŒ
            tukey_result = pairwise_tukeyhsd(data, groups, alpha=0.05)
            
            # æå–æ˜¾è‘—å·®å¼‚çš„æŒ‰é”®å¯¹
            significant_pairs = []
            
            # tukey_result æ˜¯ä¸€ä¸ª MultipleComparison å¯¹è±¡ï¼ŒåŒ…å« _results_table å±æ€§
            # ä»ç»“æœè¡¨ä¸­æå–æ˜¾è‘—å·®å¼‚å¯¹
            if hasattr(tukey_result, '_results_table'):
                results_table = tukey_result._results_table.data
                # results_table æ ¼å¼: [group1, group2, meandiff, lower, upper, reject, p-adj]
                for row in results_table[1:]:  # è·³è¿‡è¡¨å¤´
                    if len(row) >= 7:
                        group1 = row[0]
                        group2 = row[1]
                        meandiff = row[2]
                        p_adj = row[6]
                        reject = row[5]  # Trueè¡¨ç¤ºæ˜¾è‘—å·®å¼‚
                        
                        if reject:  # æˆ–è€…ä½¿ç”¨ p_adj < 0.05
                            significant_pairs.append({
                                'key1': int(group1),
                                'key2': int(group2),
                                'p_value': float(p_adj),
                                'meandiff': float(meandiff)
                            })
            
            return {
                'significant_pairs': significant_pairs,
                'total_pairs': len(significant_pairs),
                'message': f'äº‹åæ£€éªŒå®Œæˆï¼Œå‘ç°{len(significant_pairs)}å¯¹æŒ‰é”®å­˜åœ¨æ˜¾è‘—å·®å¼‚'
            }
            
        except Exception as e:
            logger.error(f"äº‹åæ£€éªŒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'significant_pairs': [],
                'message': f'äº‹åæ£€éªŒå¤±è´¥: {str(e)}'
            }
    
    def _identify_anomaly_keys(self, key_groups: Dict[int, List[float]], 
                               overall_stats: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å¼‚å¸¸æŒ‰é”®"""
        anomaly_keys = []
        overall_mean = overall_stats.get('overall_mean', 0.0)
        overall_std = overall_stats.get('overall_std', 0.0)
        
        threshold = 2.0  # 2å€æ ‡å‡†å·®é˜ˆå€¼
        
        for key_id in sorted(key_groups.keys()):
            offsets = key_groups[key_id]
            if not offsets:
                continue
            
            offsets_ms = [x / 10.0 for x in offsets]
            key_mean = np.mean(offsets_ms)
            key_std = np.std(offsets_ms)
            
            # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
            is_anomaly = False
            anomaly_type = None
            
            if abs(key_mean - overall_mean) > threshold * overall_std:
                is_anomaly = True
                if key_mean > overall_mean:
                    anomaly_type = 'å»¶æ—¶åé«˜'
                else:
                    anomaly_type = 'å»¶æ—¶åä½'
            
            if is_anomaly:
                anomaly_keys.append({
                    'key_id': key_id,
                    'mean_delay': key_mean,
                    'std_delay': key_std,
                    'count': len(offsets),
                    'anomaly_type': anomaly_type,
                    'deviation': key_mean - overall_mean,
                    'deviation_std': (key_mean - overall_mean) / overall_std if overall_std > 0 else 0
                })
        
        # æŒ‰åå·®å¤§å°æ’åº
        anomaly_keys.sort(key=lambda x: abs(x['deviation']), reverse=True)
        
        return anomaly_keys
    
    def _analyze_difference_pattern(self, descriptive_stats: List[Dict[str, Any]], 
                                   overall_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå·®å¼‚æ¨¡å¼"""
        if not descriptive_stats:
            return {
                'has_pattern': False,
                'pattern_type': None,
                'message': 'æ— æ•°æ®'
            }
        
        key_ids = [s['key_id'] for s in descriptive_stats]
        key_means = [s['mean'] for s in descriptive_stats]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çº¿æ€§è¶‹åŠ¿
        if len(key_ids) >= 3:
            # çº¿æ€§å›å½’
            slope, intercept, r_value, p_value, std_err = stats.linregress(key_ids, key_means)
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨è§„å¾‹
            has_linear_trend = abs(r_value) > 0.3 and p_value < 0.05
            
            # æŒ‰éŸ³åŸŸåˆ†ç»„åˆ†æï¼ˆå‡è®¾æŒ‰é”®IDèŒƒå›´å¤§è‡´å¯¹åº”éŸ³åŸŸï¼‰
            if key_ids:
                low_keys = [k for k in key_ids if k < 60]  # ä½éŸ³åŒº
                mid_keys = [k for k in key_ids if 60 <= k < 80]  # ä¸­éŸ³åŒº
                high_keys = [k for k in key_ids if k >= 80]  # é«˜éŸ³åŒº
                
                low_means = [s['mean'] for s in descriptive_stats if s['key_id'] in low_keys]
                mid_means = [s['mean'] for s in descriptive_stats if s['key_id'] in mid_keys]
                high_means = [s['mean'] for s in descriptive_stats if s['key_id'] in high_keys]
                
                region_stats = {
                    'low': {
                        'count': len(low_keys),
                        'mean': np.mean(low_means) if low_means else 0.0
                    },
                    'mid': {
                        'count': len(mid_keys),
                        'mean': np.mean(mid_means) if mid_means else 0.0
                    },
                    'high': {
                        'count': len(high_keys),
                        'mean': np.mean(high_means) if high_means else 0.0
                    }
                }
            else:
                region_stats = {}
            
            return {
                'has_pattern': has_linear_trend,
                'pattern_type': 'linear_trend' if has_linear_trend else 'no_trend',
                'linear_slope': slope,
                'linear_r_squared': r_value ** 2,
                'linear_p_value': p_value,
                'region_stats': region_stats,
                'message': f'çº¿æ€§è¶‹åŠ¿åˆ†æ: RÂ²={r_value**2:.4f}, p={p_value:.4f}'
            }
        else:
            return {
                'has_pattern': False,
                'pattern_type': None,
                'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†ææ¨¡å¼'
            }
    
    def _calculate_correlation(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """è®¡ç®—ç›¸å…³æ€§"""
        try:
            # çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆçº¿æ€§ç›¸å…³ï¼‰
            pearson_r, pearson_p = stats.pearsonr(velocities, delays)
            
            # æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°ï¼ˆéçº¿æ€§ç›¸å…³ï¼‰
            spearman_r, spearman_p = stats.spearmanr(velocities, delays)
            
            # åˆ¤æ–­ç›¸å…³å¼ºåº¦
            def interpret_correlation(r):
                abs_r = abs(r)
                if abs_r >= 0.7:
                    return 'å¼ºç›¸å…³'
                elif abs_r >= 0.3:
                    return 'ä¸­ç­‰ç›¸å…³'
                else:
                    return 'å¼±ç›¸å…³'
            
            return {
                'pearson_r': float(pearson_r),
                'pearson_p': float(pearson_p),
                'spearman_r': float(spearman_r),
                'spearman_p': float(spearman_p),
                'pearson_strength': interpret_correlation(pearson_r),
                'spearman_strength': interpret_correlation(spearman_r),
                'pearson_significant': pearson_p < 0.05,
                'spearman_significant': spearman_p < 0.05,
                'message': f'çš®å°”é€Šç›¸å…³: r={pearson_r:.4f}, p={pearson_p:.4f}; æ–¯çš®å°”æ›¼ç›¸å…³: r={spearman_r:.4f}, p={spearman_p:.4f}'
            }
            
        except Exception as e:
            logger.error(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            return {
                'pearson_r': None,
                'pearson_p': None,
                'spearman_r': None,
                'spearman_p': None,
                'message': f'ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {str(e)}'
            }
    
    def _perform_regression_analysis(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """æ‰§è¡Œå›å½’åˆ†æ"""
        try:
            # çº¿æ€§å›å½’
            slope, intercept, r_value, p_value, std_err = stats.linregress(velocities, delays)
            
            # å°è¯•å¤šé¡¹å¼å›å½’ï¼ˆ2æ¬¡ï¼‰
            try:
                poly_coeffs = np.polyfit(velocities, delays, 2)
                poly_func = np.poly1d(poly_coeffs)
                poly_r_squared = 1 - (np.sum((delays - poly_func(velocities))**2) / 
                                     np.sum((delays - np.mean(delays))**2))
            except:
                poly_coeffs = None
                poly_r_squared = None
            
            return {
                'linear': {
                    'slope': float(slope),
                    'intercept': float(intercept),
                    'r_squared': float(r_value ** 2),
                    'p_value': float(p_value),
                    'std_err': float(std_err)
                },
                'polynomial': {
                    'coefficients': [float(c) for c in poly_coeffs] if poly_coeffs is not None else None,
                    'r_squared': float(poly_r_squared) if poly_r_squared is not None else None
                },
                'best_fit': 'linear' if poly_r_squared is None or (r_value ** 2) >= poly_r_squared else 'polynomial',
                'message': f'çº¿æ€§å›å½’: RÂ²={r_value**2:.4f}, p={p_value:.4f}'
            }
            
        except Exception as e:
            logger.error(f"å›å½’åˆ†æå¤±è´¥: {e}")
            return {
                'linear': None,
                'polynomial': None,
                'message': f'å›å½’åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_by_velocity_groups(self, velocities: List[float], delays: List[float]) -> Dict[str, Any]:
        """æŒ‰é”¤é€ŸåŒºé—´åˆ†ç»„åˆ†æ"""
        try:
            # å®šä¹‰é”¤é€ŸåŒºé—´
            velocity_ranges = [
                (0, 100, 'ä½é”¤é€Ÿ (0-100)'),
                (100, 200, 'ä¸­é”¤é€Ÿ (100-200)'),
                (200, 300, 'é«˜é”¤é€Ÿ (200-300)'),
                (300, float('inf'), 'è¶…é«˜é”¤é€Ÿ (>300)')
            ]
            
            group_stats = []
            
            for v_min, v_max, label in velocity_ranges:
                group_velocities = []
                group_delays = []
                
                for v, d in zip(velocities, delays):
                    if v_min <= v < v_max:
                        group_velocities.append(v)
                        group_delays.append(d)
                
                if group_delays:
                    group_stats.append({
                        'range_label': label,
                        'velocity_min': v_min,
                        'velocity_max': v_max,
                        'count': len(group_delays),
                        'mean_delay': np.mean(group_delays),
                        'std_delay': np.std(group_delays),
                        'mean_velocity': np.mean(group_velocities),
                        'std_velocity': np.std(group_velocities)
                    })
            
            return {
                'groups': group_stats,
                'message': f'æŒ‰é”¤é€ŸåŒºé—´åˆ†ç»„ï¼Œå…±{len(group_stats)}ä¸ªåŒºé—´'
            }
            
        except Exception as e:
            logger.error(f"åˆ†ç»„åˆ†æå¤±è´¥: {e}")
            return {
                'groups': [],
                'message': f'åˆ†ç»„åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def analyze_force_delay_by_key(self) -> Dict[str, Any]:
        """
        åˆ†ææ¯ä¸ªæŒ‰é”®çš„åŠ›åº¦ä¸å»¶æ—¶å…³ç³»
        
        æŒ‰æŒ‰é”®IDåˆ†ç»„ï¼Œå¯¹æ¯ä¸ªæŒ‰é”®åˆ†æå…¶å†…éƒ¨çš„åŠ›åº¦ï¼ˆé”¤é€Ÿï¼‰ä¸å»¶æ—¶çš„ç»Ÿè®¡å…³ç³»ã€‚
        åŒ…æ‹¬ï¼šç›¸å…³æ€§åˆ†æã€å›å½’åˆ†æã€æè¿°æ€§ç»Ÿè®¡ç­‰ã€‚
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - key_analysis: æ¯ä¸ªæŒ‰é”®çš„åˆ†æç»“æœåˆ—è¡¨
                - overall_summary: æ•´ä½“æ‘˜è¦ç»Ÿè®¡
                - scatter_data: æ•£ç‚¹å›¾æ•°æ®ï¼ˆæŒ‰æŒ‰é”®åˆ†ç»„ï¼‰
                - status: çŠ¶æ€æ ‡è¯†
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æ")
                return self._create_empty_force_delay_result("åˆ†æå™¨ä¸å­˜åœ¨")
            
            matched_pairs = self.analyzer.note_matcher.get_matched_pairs()
            offset_data = self.analyzer.note_matcher.get_offset_alignment_data()
            
            if not matched_pairs or not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return self._create_empty_force_delay_result("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # æå–æ•°æ®ï¼šæŒ‰é”®IDã€é”¤é€Ÿã€å»¶æ—¶
            key_force_delay_data = self._extract_key_force_delay_data(matched_pairs, offset_data)
            
            if not key_force_delay_data:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æŒ‰é”®-åŠ›åº¦-å»¶æ—¶æ•°æ®")
                return self._create_empty_force_delay_result("æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            
            # è®¡ç®—æ‰€æœ‰æ•°æ®çš„å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
            all_delays = [delay for _, _, delay in key_force_delay_data]
            mean_delay = np.mean(all_delays) if all_delays else 0
            
            # æŒ‰æŒ‰é”®IDåˆ†ç»„ï¼ŒåŒæ—¶å­˜å‚¨åŸå§‹å»¶æ—¶å’Œç›¸å¯¹å»¶æ—¶
            key_groups = defaultdict(lambda: {'forces': [], 'absolute_delays': [], 'relative_delays': []})
            for key_id, force, delay in key_force_delay_data:
                key_groups[key_id]['forces'].append(force)
                key_groups[key_id]['absolute_delays'].append(delay)  # åŸå§‹å»¶æ—¶
                key_groups[key_id]['relative_delays'].append(delay - mean_delay)  # ç›¸å¯¹å»¶æ—¶
            
            # å¯¹æ¯ä¸ªæŒ‰é”®è¿›è¡Œåˆ†æ
            key_analysis = []
            scatter_data = {}
            
            for key_id in sorted(key_groups.keys()):
                forces = key_groups[key_id]['forces']
                absolute_delays = key_groups[key_id]['absolute_delays']
                relative_delays = key_groups[key_id]['relative_delays']
                
                if len(forces) < 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½è¿›è¡Œç›¸å…³æ€§åˆ†æ
                    continue
                
                # å¯¹å•ä¸ªæŒ‰é”®è¿›è¡ŒåŠ›åº¦-å»¶æ—¶åˆ†æï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰
                single_key_result = self._analyze_single_key_force_delay(
                    key_id, forces, relative_delays
                )
                key_analysis.append(single_key_result)
                
                # ä¿å­˜æ•£ç‚¹å›¾æ•°æ®ï¼ˆåŒ…å«åŸå§‹å»¶æ—¶å’Œç›¸å¯¹å»¶æ—¶ï¼‰
                scatter_data[key_id] = {
                    'forces': forces,
                    'delays': relative_delays,  # ç”¨äºæ˜¾ç¤ºçš„å»¶æ—¶ï¼ˆç›¸å¯¹å»¶æ—¶ï¼‰
                    'absolute_delays': absolute_delays,  # åŸå§‹å»¶æ—¶ï¼ˆç”¨äºæ‚¬åœæ˜¾ç¤ºï¼‰
                    'mean_delay': mean_delay  # å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè¯´æ˜ï¼‰
                }
            
            if not key_analysis:
                logger.warning("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æŒ‰é”®æ•°æ®è¿›è¡Œåˆ†æï¼ˆæ¯ä¸ªæŒ‰é”®è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬ï¼‰")
                return self._create_empty_force_delay_result("æ•°æ®ä¸è¶³")
            
            # è®¡ç®—æ•´ä½“æ‘˜è¦ç»Ÿè®¡
            overall_summary = self._calculate_force_delay_overall_summary(key_analysis)
            
            logger.info(f"âœ… æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(key_analysis)} ä¸ªæŒ‰é”®")
            
            return {
                'key_analysis': key_analysis,
                'overall_summary': overall_summary,
                'scatter_data': scatter_data,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._create_empty_force_delay_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def analyze_key_force_interaction(self) -> Dict[str, Any]:
        """
        åˆ†ææŒ‰é”®ä¸åŠ›åº¦çš„è”åˆç»Ÿè®¡å…³ç³»ï¼ˆå¤šå› ç´ åˆ†æï¼‰
        
        ä½¿ç”¨å¤šå› ç´ ANOVAå’Œäº¤äº’æ•ˆåº”åˆ†æï¼Œè¯„ä¼°ï¼š
        1. æŒ‰é”®å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        2. åŠ›åº¦å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        3. æŒ‰é”®Ã—åŠ›åº¦çš„äº¤äº’æ•ˆåº”
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - two_way_anova: åŒå› ç´ ANOVAç»“æœ
                - interaction_effect: äº¤äº’æ•ˆåº”åˆ†æç»“æœ
                - stratified_regression: åˆ†å±‚å›å½’åˆ†æç»“æœ
                - interaction_plot_data: äº¤äº’æ•ˆåº”å›¾æ•°æ®
                - status: çŠ¶æ€æ ‡è¯†
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æ")
                return self._create_empty_interaction_result("åˆ†æå™¨ä¸å­˜åœ¨")
            
            matched_pairs = self.analyzer.note_matcher.get_matched_pairs()
            offset_data = self.analyzer.note_matcher.get_offset_alignment_data()
            
            if not matched_pairs or not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return self._create_empty_interaction_result("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # æå–æ•°æ®ï¼šæŒ‰é”®IDã€é”¤é€Ÿã€å»¶æ—¶
            key_force_delay_data = self._extract_key_force_delay_data(matched_pairs, offset_data)
            
            if not key_force_delay_data:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æŒ‰é”®-åŠ›åº¦-å»¶æ—¶æ•°æ®")
                return self._create_empty_interaction_result("æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            
            # å‡†å¤‡å¤šå› ç´ ANOVAæ•°æ®
            key_ids_list = [item[0] for item in key_force_delay_data]
            forces_list = [item[1] for item in key_force_delay_data]
            delays_list = [item[2] for item in key_force_delay_data]
            
            # 1. åŒå› ç´ ANOVAï¼ˆæŒ‰é”® Ã— åŠ›åº¦ï¼‰
            two_way_anova = self._perform_two_way_anova(key_ids_list, forces_list, delays_list)
            
            # 2. äº¤äº’æ•ˆåº”åˆ†æ
            interaction_effect = self._analyze_interaction_effect(key_ids_list, forces_list, delays_list)
            
            # 3. åˆ†å±‚å›å½’åˆ†æ
            stratified_regression = self._perform_stratified_regression(key_ids_list, forces_list, delays_list)
            
            # 4. ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®
            interaction_plot_data = self._generate_interaction_plot_data(key_ids_list, forces_list, delays_list)
            
            logger.info("âœ… æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆ")
            
            return {
                'two_way_anova': two_way_anova,
                'interaction_effect': interaction_effect,
                'stratified_regression': stratified_regression,
                'interaction_plot_data': interaction_plot_data,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._create_empty_interaction_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def _extract_key_force_delay_data(self, matched_pairs: List[Tuple], 
                                      offset_data: List[Dict[str, Any]]) -> List[Tuple[int, float, float]]:
        """
        ä»åŒ¹é…å¯¹å’Œåç§»æ•°æ®ä¸­æå–æŒ‰é”®IDã€åŠ›åº¦ï¼ˆé”¤é€Ÿï¼‰ã€å»¶æ—¶æ•°æ®
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨
            offset_data: åç§»å¯¹é½æ•°æ®åˆ—è¡¨
            
        Returns:
            List[Tuple[int, float, float]]: [(key_id, force, delay), ...]
                - key_id: æŒ‰é”®ID
                - force: åŠ›åº¦ï¼ˆé”¤é€Ÿå€¼ï¼Œå•ä½ï¼šåŸå§‹å•ä½ï¼‰
                - delay: å»¶æ—¶ï¼ˆå•ä½ï¼šmsï¼Œå¸¦ç¬¦å·ï¼‰
        """
        # åˆ›å»ºåŒ¹é…å¯¹ç´¢å¼•åˆ°åç§»æ•°æ®çš„æ˜ å°„
        offset_map = {}
        for item in offset_data:
            record_idx = item.get('record_index')
            replay_idx = item.get('replay_index')
            if record_idx is not None and replay_idx is not None:
                offset_map[(record_idx, replay_idx)] = item
        
        result = []
        
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            # è·å–æŒ‰é”®ID
            key_id = record_note.id
            
            # è·å–æ’­æ”¾éŸ³ç¬¦çš„é”¤é€Ÿï¼ˆç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
            force = None
            if len(replay_note.hammers) > 0:
                min_timestamp = replay_note.hammers.index.min()
                first_hammer_velocity_raw = replay_note.hammers.loc[min_timestamp]
                if isinstance(first_hammer_velocity_raw, pd.Series):
                    force = first_hammer_velocity_raw.iloc[0]
                else:
                    force = first_hammer_velocity_raw
            
            if force is None or force <= 0:
                continue  # è·³è¿‡æ— æ•ˆçš„åŠ›åº¦æ•°æ®
            
            # è·å–å»¶æ—¶
            keyon_offset = None
            if (record_idx, replay_idx) in offset_map:
                keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è®¡ç®—
                try:
                    record_keyon, _ = self.analyzer.note_matcher._calculate_note_times(record_note)
                    replay_keyon, _ = self.analyzer.note_matcher._calculate_note_times(replay_note)
                    keyon_offset = replay_keyon - record_keyon
                except:
                    continue
            
            if keyon_offset is not None:
                # è½¬æ¢ä¸ºmså•ä½ï¼ˆå¸¦ç¬¦å·ï¼‰
                delay_ms = keyon_offset / 10.0
                result.append((key_id, float(force), delay_ms))
        
        return result
    
    def _analyze_single_key_force_delay(self, key_id: int, forces: List[float], 
                                       delays: List[float]) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæŒ‰é”®çš„åŠ›åº¦ä¸å»¶æ—¶å…³ç³»
        
        Args:
            key_id: æŒ‰é”®ID
            forces: åŠ›åº¦å€¼åˆ—è¡¨
            delays: å»¶æ—¶å€¼åˆ—è¡¨ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰
            
        Returns:
            Dict[str, Any]: å•ä¸ªæŒ‰é”®çš„åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - key_id: æŒ‰é”®ID
                - correlation: ç›¸å…³æ€§åˆ†æç»“æœ
                - regression: å›å½’åˆ†æç»“æœ
                - descriptive_stats: æè¿°æ€§ç»Ÿè®¡
                - sample_count: æ ·æœ¬æ•°é‡
        """
        # 1. ç›¸å…³æ€§åˆ†æ
        correlation = self._calculate_correlation(forces, delays)
        
        # 2. å›å½’åˆ†æ
        regression = self._perform_regression_analysis(forces, delays)
        
        # 3. æè¿°æ€§ç»Ÿè®¡
        descriptive_stats = {
            'force': {
                'mean': float(np.mean(forces)),
                'std': float(np.std(forces)),
                'min': float(np.min(forces)),
                'max': float(np.max(forces)),
                'median': float(np.median(forces))
            },
            'delay': {
                'mean': float(np.mean(delays)),
                'std': float(np.std(delays)),
                'min': float(np.min(delays)),
                'max': float(np.max(delays)),
                'median': float(np.median(delays))
            }
        }
        
        return {
            'key_id': key_id,
            'correlation': correlation,
            'regression': regression,
            'descriptive_stats': descriptive_stats,
            'sample_count': len(forces)
        }
    
    def _calculate_force_delay_overall_summary(self, key_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æçš„æ•´ä½“æ‘˜è¦ç»Ÿè®¡
        
        Args:
            key_analysis: æ¯ä¸ªæŒ‰é”®çš„åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: æ•´ä½“æ‘˜è¦ç»Ÿè®¡
        """
        if not key_analysis:
            return {}
        
        # ç»Ÿè®¡æ˜¾è‘—ç›¸å…³çš„æŒ‰é”®æ•°é‡
        significant_correlation_count = 0
        positive_correlation_count = 0
        negative_correlation_count = 0
        
        # ç»Ÿè®¡å¹³å‡ç›¸å…³ç³»æ•°
        pearson_rs = []
        spearman_rs = []
        
        for analysis in key_analysis:
            corr = analysis.get('correlation', {})
            if corr.get('pearson_significant', False):
                significant_correlation_count += 1
                pearson_r = corr.get('pearson_r', 0)
                if pearson_r > 0:
                    positive_correlation_count += 1
                elif pearson_r < 0:
                    negative_correlation_count += 1
                pearson_rs.append(pearson_r)
            
            if corr.get('spearman_r') is not None:
                spearman_rs.append(corr.get('spearman_r', 0))
        
        return {
            'total_keys_analyzed': len(key_analysis),
            'significant_correlation_count': significant_correlation_count,
            'positive_correlation_count': positive_correlation_count,
            'negative_correlation_count': negative_correlation_count,
            'mean_pearson_r': float(np.mean(pearson_rs)) if pearson_rs else None,
            'mean_spearman_r': float(np.mean(spearman_rs)) if spearman_rs else None,
            'correlation_rate': significant_correlation_count / len(key_analysis) if key_analysis else 0.0
        }
    
    def _perform_two_way_anova(self, key_ids: List[int], forces: List[float], 
                               delays: List[float]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒåŒå› ç´ ANOVAï¼ˆæŒ‰é”® Ã— åŠ›åº¦ï¼‰
        
        ä½¿ç”¨çº¿æ€§æ¨¡å‹è¯„ä¼°ï¼š
        1. æŒ‰é”®å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        2. åŠ›åº¦å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        3. æŒ‰é”®Ã—åŠ›åº¦çš„äº¤äº’æ•ˆåº”
        
        Args:
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            forces: åŠ›åº¦å€¼åˆ—è¡¨
            delays: å»¶æ—¶å€¼åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: åŒå› ç´ ANOVAç»“æœ
        """
        try:
            from statsmodels.formula.api import ols
            from statsmodels.stats.anova import anova_lm
            import pandas as pd
            
            # å‡†å¤‡DataFrame
            df = pd.DataFrame({
                'key_id': key_ids,
                'force': forces,
                'delay': delays
            })
            
            # å°†æŒ‰é”®IDè½¬æ¢ä¸ºåˆ†ç±»å˜é‡ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºæ¨¡å‹è¯†åˆ«ï¼‰
            df['key_id'] = df['key_id'].astype(str)
            
            # æ„å»ºçº¿æ€§æ¨¡å‹ï¼šdelay ~ key_id + force + key_id:force
            # key_id:force è¡¨ç¤ºäº¤äº’é¡¹
            model = ols('delay ~ C(key_id) + force + C(key_id):force', data=df).fit()
            
            # æ‰§è¡ŒANOVA
            anova_table = anova_lm(model, typ=2)
            
            # æå–ç»“æœ
            key_effect = anova_table.loc['C(key_id)', :] if 'C(key_id)' in anova_table.index else None
            force_effect = anova_table.loc['force', :] if 'force' in anova_table.index else None
            interaction_effect = anova_table.loc['C(key_id):force', :] if 'C(key_id):force' in anova_table.index else None
            
            result = {
                'key_main_effect': {
                    'f_statistic': float(key_effect['F']) if key_effect is not None else None,
                    'p_value': float(key_effect['PR(>F)']) if key_effect is not None else None,
                    'significant': float(key_effect['PR(>F)']) < 0.05 if key_effect is not None else False
                } if key_effect is not None else None,
                'force_main_effect': {
                    'f_statistic': float(force_effect['F']) if force_effect is not None else None,
                    'p_value': float(force_effect['PR(>F)']) if force_effect is not None else None,
                    'significant': float(force_effect['PR(>F)']) < 0.05 if force_effect is not None else False
                } if force_effect is not None else None,
                'interaction_effect': {
                    'f_statistic': float(interaction_effect['F']) if interaction_effect is not None else None,
                    'p_value': float(interaction_effect['PR(>F)']) if interaction_effect is not None else None,
                    'significant': float(interaction_effect['PR(>F)']) < 0.05 if interaction_effect is not None else False
                } if interaction_effect is not None else None,
                'model_r_squared': float(model.rsquared),
                'model_adj_r_squared': float(model.rsquared_adj)
            }
            
            # ç”Ÿæˆè§£é‡Šæ€§æ¶ˆæ¯
            messages = []
            if result['key_main_effect']:
                key_sig = result['key_main_effect']['significant']
                messages.append(f"æŒ‰é”®ä¸»æ•ˆåº”: {'æ˜¾è‘—' if key_sig else 'ä¸æ˜¾è‘—'} (p={result['key_main_effect']['p_value']:.4f})")
            
            if result['force_main_effect']:
                force_sig = result['force_main_effect']['significant']
                messages.append(f"åŠ›åº¦ä¸»æ•ˆåº”: {'æ˜¾è‘—' if force_sig else 'ä¸æ˜¾è‘—'} (p={result['force_main_effect']['p_value']:.4f})")
            
            if result['interaction_effect']:
                inter_sig = result['interaction_effect']['significant']
                messages.append(f"äº¤äº’æ•ˆåº”: {'æ˜¾è‘—' if inter_sig else 'ä¸æ˜¾è‘—'} (p={result['interaction_effect']['p_value']:.4f})")
            
            result['message'] = '; '.join(messages)
            
            return result
            
        except Exception as e:
            logger.error(f"åŒå› ç´ ANOVAå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'key_main_effect': None,
                'force_main_effect': None,
                'interaction_effect': None,
                'message': f'åŒå› ç´ ANOVAå¤±è´¥: {str(e)}'
            }
    
    def _analyze_interaction_effect(self, key_ids: List[int], forces: List[float], 
                                   delays: List[float]) -> Dict[str, Any]:
        """
        åˆ†æäº¤äº’æ•ˆåº”çš„è¯¦ç»†æ¨¡å¼
        
        è¯„ä¼°ä¸åŒæŒ‰é”®ä¸‹ï¼ŒåŠ›åº¦å¯¹å»¶æ—¶çš„å½±å“æ˜¯å¦ä¸åŒ
        
        Args:
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            forces: åŠ›åº¦å€¼åˆ—è¡¨
            delays: å»¶æ—¶å€¼åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: äº¤äº’æ•ˆåº”åˆ†æç»“æœ
        """
        try:
            from collections import defaultdict
            
            # æŒ‰æŒ‰é”®åˆ†ç»„
            key_groups = defaultdict(lambda: {'forces': [], 'delays': []})
            for key_id, force, delay in zip(key_ids, forces, delays):
                key_groups[key_id]['forces'].append(force)
                key_groups[key_id]['delays'].append(delay)
            
            # å¯¹æ¯ä¸ªæŒ‰é”®è®¡ç®—åŠ›åº¦-å»¶æ—¶çš„æ–œç‡ï¼ˆå›å½’ç³»æ•°ï¼‰
            key_slopes = {}
            key_intercepts = {}
            key_r_squared = {}
            
            for key_id in sorted(key_groups.keys()):
                forces_key = key_groups[key_id]['forces']
                delays_key = key_groups[key_id]['delays']
                
                if len(forces_key) < 2:
                    continue
                
                # çº¿æ€§å›å½’
                slope, intercept, r_value, p_value, std_err = stats.linregress(forces_key, delays_key)
                key_slopes[key_id] = slope
                key_intercepts[key_id] = intercept
                key_r_squared[key_id] = r_value ** 2
            
            if not key_slopes:
                return {
                    'key_slopes': {},
                    'slope_variance': None,
                    'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æäº¤äº’æ•ˆåº”'
                }
            
            # è®¡ç®—æ–œç‡çš„æ–¹å·®ï¼ˆç”¨äºè¯„ä¼°äº¤äº’æ•ˆåº”å¼ºåº¦ï¼‰
            slopes_list = list(key_slopes.values())
            slope_mean = np.mean(slopes_list)
            slope_std = np.std(slopes_list)
            slope_variance = np.var(slopes_list)
            
            # åˆ¤æ–­äº¤äº’æ•ˆåº”å¼ºåº¦
            # äº¤äº’æ•ˆåº”å¼ºåº¦åæ˜ ä¸åŒæŒ‰é”®çš„æ–œç‡å·®å¼‚ç¨‹åº¦
            # å¦‚æœæ–œç‡çš„æ ‡å‡†å·®å¤§ï¼Œè¯´æ˜ä¸åŒæŒ‰é”®çš„åŠ›åº¦-å»¶æ—¶å…³ç³»å·®å¼‚å¤§ï¼Œäº¤äº’æ•ˆåº”å¼º
            # å¦‚æœæ–œç‡çš„æ ‡å‡†å·®å°ï¼Œè¯´æ˜ä¸åŒæŒ‰é”®çš„åŠ›åº¦-å»¶æ—¶å…³ç³»ç›¸ä¼¼ï¼Œäº¤äº’æ•ˆåº”å¼±
            
            abs_slope_mean = abs(slope_mean)
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼Œé¿å…æ€»æ˜¯æ˜¾ç¤º"å¼º"
            # æ–¹æ³•1: å¦‚æœå‡å€¼æ¥è¿‘0ï¼Œä½¿ç”¨ç»å¯¹æ ‡å‡†å·®åˆ¤æ–­
            if abs_slope_mean < 1e-6:
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„ç»å¯¹é˜ˆå€¼ï¼šstd > 0.002 ä¸ºå¼ºï¼Œ> 0.001 ä¸ºä¸­ç­‰ï¼Œå¦åˆ™ä¸ºå¼±
                # è¿™äº›é˜ˆå€¼åŸºäºå®é™…æ•°æ®çš„ç»éªŒå€¼ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æ•°æ®è°ƒæ•´
                if slope_std > 0.002:
                    interaction_strength = 'å¼º'
                elif slope_std > 0.001:
                    interaction_strength = 'ä¸­ç­‰'
                else:
                    interaction_strength = 'å¼±'
            else:
                # æ–¹æ³•2: ä½¿ç”¨å˜å¼‚ç³»æ•°ï¼ˆCV = std/meanï¼‰æ¥åˆ¤æ–­
                # CVåæ˜ ç›¸å¯¹å˜å¼‚æ€§ï¼Œæ›´ç¨³å¥
                cv = slope_std / abs_slope_mean
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼ŒåŒæ—¶è€ƒè™‘ç»å¯¹æ ‡å‡†å·®
                # å¦‚æœæ ‡å‡†å·®æœ¬èº«å¾ˆå°ï¼ˆ< 0.0005ï¼‰ï¼Œå³ä½¿CVå¤§ä¹Ÿè®¤ä¸ºæ˜¯å¼±äº¤äº’
                if slope_std < 0.0005:
                    interaction_strength = 'å¼±'
                elif cv > 1.0:  # CV > 1.0 ä¸ºå¼ºï¼ˆæ›´ä¸¥æ ¼ï¼‰
                    interaction_strength = 'å¼º'
                elif cv > 0.5:  # CV > 0.5 ä¸ºä¸­ç­‰ï¼ˆæ›´ä¸¥æ ¼ï¼‰
                    interaction_strength = 'ä¸­ç­‰'
                else:
                    interaction_strength = 'å¼±'
            
            # è®°å½•è®¡ç®—è¯¦æƒ…ï¼Œä¾¿äºè°ƒè¯•
            logger.info(f"ğŸ“Š äº¤äº’æ•ˆåº”å¼ºåº¦è®¡ç®—: slope_mean={slope_mean:.6f}, slope_std={slope_std:.6f}, "
                       f"abs_mean={abs_slope_mean:.6f}, CV={slope_std/abs_slope_mean if abs_slope_mean > 1e-6 else 'N/A'}, "
                       f"å¼ºåº¦={interaction_strength}")
            
            return {
                'key_slopes': {k: float(v) for k, v in key_slopes.items()},
                'key_intercepts': {k: float(v) for k, v in key_intercepts.items()},
                'key_r_squared': {k: float(v) for k, v in key_r_squared.items()},
                'slope_mean': float(slope_mean),
                'slope_std': float(slope_std),
                'slope_variance': float(slope_variance),
                'interaction_strength': interaction_strength,
                'message': f'äº¤äº’æ•ˆåº”åˆ†æ: æ–œç‡å‡å€¼={slope_mean:.6f}, æ ‡å‡†å·®={slope_std:.6f}, å¼ºåº¦={interaction_strength}'
            }
            
        except Exception as e:
            logger.error(f"äº¤äº’æ•ˆåº”åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'key_slopes': {},
                'message': f'äº¤äº’æ•ˆåº”åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _perform_stratified_regression(self, key_ids: List[int], forces: List[float], 
                                      delays: List[float]) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†å±‚å›å½’åˆ†æ
        
        1. æ§åˆ¶æŒ‰é”®åï¼Œåˆ†æåŠ›åº¦å¯¹å»¶æ—¶çš„å½±å“
        2. æ§åˆ¶åŠ›åº¦åï¼Œåˆ†ææŒ‰é”®å¯¹å»¶æ—¶çš„å½±å“
        
        Args:
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            forces: åŠ›åº¦å€¼åˆ—è¡¨
            delays: å»¶æ—¶å€¼åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: åˆ†å±‚å›å½’åˆ†æç»“æœ
        """
        try:
            from statsmodels.formula.api import ols
            import pandas as pd
            
            df = pd.DataFrame({
                'key_id': key_ids,
                'force': forces,
                'delay': delays
            })
            df['key_id'] = df['key_id'].astype(str)
            
            # æ¨¡å‹1ï¼šåªåŒ…å«æŒ‰é”®ï¼ˆåŸºå‡†æ¨¡å‹ï¼‰
            model1 = ols('delay ~ C(key_id)', data=df).fit()
            r_squared_key_only = model1.rsquared
            
            # æ¨¡å‹2ï¼šæŒ‰é”® + åŠ›åº¦ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰
            model2 = ols('delay ~ C(key_id) + force', data=df).fit()
            r_squared_key_force = model2.rsquared
            
            # æ¨¡å‹3ï¼šåªåŒ…å«åŠ›åº¦ï¼ˆåŸºå‡†æ¨¡å‹ï¼‰
            model3 = ols('delay ~ force', data=df).fit()
            r_squared_force_only = model3.rsquared
            
            # æ¨¡å‹4ï¼šåŠ›åº¦ + æŒ‰é”®ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰
            model4 = ols('delay ~ force + C(key_id)', data=df).fit()
            r_squared_force_key = model4.rsquared
            
            # è®¡ç®—å¢é‡RÂ²ï¼ˆæ§åˆ¶ä¸€ä¸ªå˜é‡åï¼Œå¦ä¸€ä¸ªå˜é‡çš„è´¡çŒ®ï¼‰
            force_incremental_r2 = r_squared_key_force - r_squared_key_only
            key_incremental_r2 = r_squared_force_key - r_squared_force_only
            
            return {
                'force_effect_controlling_key': {
                    'r_squared_incremental': float(force_incremental_r2),
                    'r_squared_full': float(r_squared_key_force),
                    'r_squared_base': float(r_squared_key_only),
                    'force_coefficient': float(model2.params.get('force', 0)),
                    'force_p_value': float(model2.pvalues.get('force', 1.0))
                },
                'key_effect_controlling_force': {
                    'r_squared_incremental': float(key_incremental_r2),
                    'r_squared_full': float(r_squared_force_key),
                    'r_squared_base': float(r_squared_force_only),
                    'key_f_statistic': float(model4.fvalue) if hasattr(model4, 'fvalue') else None,
                    'key_p_value': float(model4.f_pvalue) if hasattr(model4, 'f_pvalue') else None
                },
                'message': f'åˆ†å±‚å›å½’: æ§åˆ¶æŒ‰é”®ååŠ›åº¦å¢é‡RÂ²={force_incremental_r2:.4f}, æ§åˆ¶åŠ›åº¦åæŒ‰é”®å¢é‡RÂ²={key_incremental_r2:.4f}'
            }
            
        except Exception as e:
            logger.error(f"åˆ†å±‚å›å½’åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'force_effect_controlling_key': None,
                'key_effect_controlling_force': None,
                'message': f'åˆ†å±‚å›å½’åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _generate_interaction_plot_data(self, key_ids: List[int], forces: List[float], 
                                       delays: List[float]) -> Dict[str, Any]:
        """
        ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰
        
        ä¸ºæ¯ä¸ªæŒ‰é”®ç”ŸæˆåŠ›åº¦-å»¶æ—¶çš„å›å½’çº¿æ•°æ®ï¼Œç”¨äºç»˜åˆ¶äº¤äº’æ•ˆåº”å›¾
        å»¶æ—¶æ•°æ®ä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼ˆåŸå§‹å»¶æ—¶å‡å»æ•´ä½“å¹³å‡å»¶æ—¶ï¼‰
        
        Args:
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            forces: åŠ›åº¦å€¼åˆ—è¡¨
            delays: å»¶æ—¶å€¼åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: äº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ŒåŒ…å«æ¯ä¸ªæŒ‰é”®çš„å›å½’çº¿æ•°æ®
        """
        try:
            from collections import defaultdict
            
            # è®¡ç®—æ•´ä½“å¹³å‡å»¶æ—¶
            mean_delay = np.mean(delays) if delays else 0
            logger.info(f"ğŸ“Š æ•´ä½“å¹³å‡å»¶æ—¶: {mean_delay:.2f}ms")
            
            # è®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼ˆå»¶æ—¶ - å¹³å‡å»¶æ—¶ï¼‰
            relative_delays = [delay - mean_delay for delay in delays]
            
            # æŒ‰æŒ‰é”®åˆ†ç»„
            key_groups = defaultdict(lambda: {'forces': [], 'relative_delays': [], 'absolute_delays': []})
            for key_id, force, rel_delay, abs_delay in zip(key_ids, forces, relative_delays, delays):
                key_groups[key_id]['forces'].append(force)
                key_groups[key_id]['relative_delays'].append(rel_delay)
                key_groups[key_id]['absolute_delays'].append(abs_delay)
            
            interaction_data = {}
            
            for key_id in sorted(key_groups.keys()):
                forces_key = key_groups[key_id]['forces']
                relative_delays_key = key_groups[key_id]['relative_delays']  # ä½¿ç”¨ç›¸å¯¹å»¶æ—¶
                absolute_delays_key = key_groups[key_id]['absolute_delays']  # ä¿ç•™åŸå§‹å»¶æ—¶ç”¨äºå‚è€ƒ
                
                if len(forces_key) < 2:
                    continue
                
                # ä½¿ç”¨ç›¸å¯¹å»¶æ—¶è¿›è¡Œçº¿æ€§å›å½’
                slope, intercept, r_value, p_value, std_err = stats.linregress(forces_key, relative_delays_key)
                
                # ç”Ÿæˆå›å½’çº¿çš„xå’Œyå€¼ï¼ˆç”¨äºç»˜å›¾ï¼‰
                force_min = min(forces_key)
                force_max = max(forces_key)
                force_range = force_max - force_min
                
                # ç”Ÿæˆ10ä¸ªç‚¹ç”¨äºç»˜åˆ¶å›å½’çº¿
                force_line = np.linspace(force_min - force_range * 0.1, 
                                        force_max + force_range * 0.1, 10)
                delay_line = slope * force_line + intercept  # ç›¸å¯¹å»¶æ—¶çš„å›å½’çº¿
                
                interaction_data[key_id] = {
                    'forces': forces_key,
                    'delays': relative_delays_key,  # ä½¿ç”¨ç›¸å¯¹å»¶æ—¶
                    'absolute_delays': absolute_delays_key,  # ä¿ç•™åŸå§‹å»¶æ—¶ç”¨äºå‚è€ƒ
                    'mean_delay': float(mean_delay),  # æ•´ä½“å¹³å‡å»¶æ—¶
                    'regression_line': {
                        'force': [float(f) for f in force_line],
                        'delay': [float(d) for d in delay_line]
                    },
                    'slope': float(slope),
                    'intercept': float(intercept),
                    'r_squared': float(r_value ** 2),
                    'p_value': float(p_value),
                    'sample_count': len(forces_key)
                }
            
            return {
                'key_data': interaction_data,
                'mean_delay': float(mean_delay),  # æ·»åŠ æ•´ä½“å¹³å‡å»¶æ—¶åˆ°è¿”å›ç»“æœ
                'message': f'ç”Ÿæˆ {len(interaction_data)} ä¸ªæŒ‰é”®çš„äº¤äº’æ•ˆåº”å›¾æ•°æ®ï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰'
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®å¤±è´¥: {e}")
            return {
                'key_data': {},
                'message': f'ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾æ•°æ®å¤±è´¥: {str(e)}'
            }
    
    def _create_empty_force_delay_result(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æç»“æœ"""
        return {
            'status': 'error',
            'message': message,
            'key_analysis': [],
            'overall_summary': {},
            'scatter_data': {}
        }
    
    def _create_empty_interaction_result(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„äº¤äº’æ•ˆåº”åˆ†æç»“æœ"""
        return {
            'status': 'error',
            'message': message,
            'two_way_anova': {},
            'interaction_effect': {},
            'stratified_regression': {},
            'interaction_plot_data': {}
        }
    
    def _create_empty_result(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'status': 'error',
            'message': message,
            'descriptive_stats': [],
            'overall_stats': {},
            'anova_result': {},
            'posthoc_result': None,
            'anomaly_keys': [],
            'difference_pattern': {}
        }

