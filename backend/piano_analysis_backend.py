#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é‡æ„åçš„é’¢ç´åˆ†æåç«¯API
ä½¿ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œå°†åŸæ¥çš„å¤§ç±»æ‹†åˆ†ä¸ºå¤šä¸ªä¸“é—¨çš„æ¨¡å—
"""

import os
import tempfile
import traceback
import hashlib
import json
import pandas as pd
import numpy as np
from typing import Optional, Tuple, Dict, Any, List, Union
from plotly.graph_objects import Figure
from utils.logger import Logger

# Dash UI imports for report generation
import dash_bootstrap_components as dbc
from dash import html

# SPMIDç›¸å…³å¯¼å…¥
import spmid
from spmid.spmid_analyzer import SPMIDAnalyzer
import spmid.spmid_plot as spmid

# å¯¼å…¥å„ä¸ªæ¨¡å—
from .data_manager import DataManager
from .plot_generator import PlotGenerator
from .data_filter import DataFilter
from .time_filter import TimeFilter
from .table_data_generator import TableDataGenerator
from .history_manager import HistoryManager
from .delay_analysis import DelayAnalysis
from .multi_algorithm_manager import MultiAlgorithmManager, AlgorithmDataset, AlgorithmStatus
from .force_curve_analyzer import ForceCurveAnalyzer

logger = Logger.get_logger()


# ç»Ÿä¸€ç»Ÿè®¡æœåŠ¡ç±»
class AlgorithmStatistics:
    """ç®—æ³•ç»Ÿè®¡æœåŠ¡ - ç»Ÿä¸€å¤„ç†å‡†ç¡®ç‡å’Œé”™è¯¯è®¡ç®—"""

    def __init__(self, algorithm):
        self.algorithm = algorithm
        self._cache = {}  # è®¡ç®—ç»“æœç¼“å­˜

    def get_accuracy_info(self) -> dict:
        """è·å–å‡†ç¡®ç‡ç›¸å…³ä¿¡æ¯"""
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»å‡†å¤‡å¥½ï¼ˆæ˜¯å¦æœ‰åŒ¹é…ç»“æœï¼‰
        note_matcher = getattr(getattr(self.algorithm, 'analyzer', None), 'note_matcher', None)
        has_match_data = (note_matcher and
                         hasattr(note_matcher, 'match_results') and
                         len(getattr(note_matcher, 'match_results', [])) > 0)

        if not has_match_data:
            # æ•°æ®è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼Œæ¸…é™¤ç¼“å­˜å¹¶è¿”å›ç©ºæ•°æ®
            self._cache.pop('accuracy_info', None)
            return {
                'accuracy': 0.0,
                'matched_count': 0,
                'total_effective_keys': 0,
                'precision_matches': 0,
                'approximate_matches': 0
            }

        if 'accuracy_info' not in self._cache:
            self._cache['accuracy_info'] = self._calculate_accuracy_info()
        return self._cache['accuracy_info']

    def get_error_info(self) -> dict:
        """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»å‡†å¤‡å¥½
        note_matcher = getattr(getattr(self.algorithm, 'analyzer', None), 'note_matcher', None)
        has_match_data = (note_matcher and
                         hasattr(note_matcher, 'match_results') and
                         len(getattr(note_matcher, 'match_results', [])) > 0)

        if not has_match_data:
            # æ•°æ®è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼Œæ¸…é™¤ç¼“å­˜å¹¶è¿”å›ç©ºæ•°æ®
            self._cache.pop('error_info', None)
            return {
                'drop_count': 0,
                'multi_count': 0,
                'silent_count': 0
            }

        if 'error_info' not in self._cache:
            self._cache['error_info'] = self._calculate_error_info()
        return self._cache['error_info']

    def get_full_statistics(self) -> dict:
        """è·å–å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯"""
        accuracy_info = self.get_accuracy_info()
        error_info = self.get_error_info()

        return {
            **accuracy_info,
            **error_info,
            'total_errors': error_info['drop_count'] + error_info['multi_count']
        }

    def _calculate_accuracy_info(self) -> dict:
        """è®¡ç®—å‡†ç¡®ç‡ç›¸å…³ä¿¡æ¯"""
        # åˆ†æ¯ï¼šæ€»æœ‰æ•ˆæŒ‰é”®æ•°
        total_effective_keys = self._get_total_effective_keys()

        if total_effective_keys == 0:
            return {
                'accuracy': 0.0,
                'matched_count': 0,
                'total_effective_keys': 0,
                'precision_matches': 0,
                'approximate_matches': 0
            }

        # ä½¿ç”¨åŒ¹é…è´¨é‡è¯„çº§ç»Ÿè®¡ä¸­çš„æ€»åŒ¹é…å¯¹æ•°ï¼Œç¡®ä¿æ•°æ®æºä¸€è‡´
        note_matcher = getattr(self.algorithm.analyzer, 'note_matcher', None)
        if note_matcher and hasattr(note_matcher, 'get_graded_error_stats'):
            graded_stats = note_matcher.get_graded_error_stats()
            total_matched_count = graded_stats.get('total_successful_matches', 0)
        else:
            total_matched_count = 0

        # åˆ†å­ï¼šåŒ¹é…çš„éŸ³ç¬¦æ€»æ•°ï¼ˆæ¯ä¸ªåŒ¹é…å¯¹åŒ…å«2ä¸ªéŸ³ç¬¦ï¼‰
        # æ³¨æ„ï¼šåªè®¡ç®—æˆåŠŸåŒ¹é…çš„éŸ³ç¬¦ï¼Œä¸åŒ…æ‹¬å¤±è´¥åŒ¹é…
        matched_keys_count = total_matched_count * 2

        # å‡†ç¡®ç‡è®¡ç®—
        # å‡†ç¡®ç‡ = (æˆåŠŸåŒ¹é…çš„éŸ³ç¬¦æ•° / æ€»æœ‰æ•ˆéŸ³ç¬¦æ•°) * 100
        # æ¯ä¸ªåŒ¹é…å¯¹åŒ…å«2ä¸ªéŸ³ç¬¦ï¼ˆ1ä¸ªå½•åˆ¶ + 1ä¸ªæ’­æ”¾ï¼‰
        accuracy = (matched_keys_count / total_effective_keys) * 100 if total_effective_keys > 0 else 0.0

        # è°ƒè¯•ä¿¡æ¯
        print(f"[DEBUG] å‡†ç¡®ç‡è®¡ç®—: matched_keys_count={matched_keys_count}, total_effective_keys={total_effective_keys}, accuracy={accuracy:.2f}%")
        print(f"[DEBUG]   total_matched_count={total_matched_count}, matched_keys_count={matched_keys_count}")

        # è·å–åŒ¹é…ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå…¶ä»–ç”¨é€”ï¼‰
        # æ³¨æ„ï¼šç°åœ¨statsä¸å†åŒ…å«failed_matchesï¼Œæ‰€ä»¥æˆ‘ä»¬ä»match_statisticsè·å–å…¼å®¹å­—æ®µ
        match_stats = self.algorithm.analyzer.match_statistics
        precision_matches = getattr(match_stats, 'precision_matches', 0)
        approximate_matches = getattr(match_stats, 'approximate_matches', 0)

        return {
            'accuracy': accuracy,
            'matched_count': total_matched_count,  # è¿”å›æ€»åŒ¹é…å¯¹æ•°
            'total_effective_keys': total_effective_keys,
            'precision_matches': precision_matches,
            'approximate_matches': approximate_matches
        }

    def _calculate_error_info(self) -> dict:
        """è®¡ç®—é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
        # ç›´æ¥ä½¿ç”¨analyzerä¸­å·²è®¡ç®—çš„é”™è¯¯æ•°æ®ï¼Œä¿æŒä¸è¡¨æ ¼æ˜¾ç¤ºä¸€è‡´
        drop_hammers = getattr(self.algorithm.analyzer, 'drop_hammers', [])
        multi_hammers = getattr(self.algorithm.analyzer, 'multi_hammers', [])

        logger.info(f"ğŸ“Š ç»Ÿè®¡æ•°æ®æºæ£€æŸ¥: analyzer.drop_hammers={len(drop_hammers)}, analyzer.multi_hammers={len(multi_hammers)}")

        # è·å–åŸå§‹æ•°æ®ç”¨äºè¯¦ç»†ä¿¡æ¯æ˜¾ç¤º
        initial_valid_record_data = getattr(self.algorithm.analyzer, 'initial_valid_record_data', [])
        initial_valid_replay_data = getattr(self.algorithm.analyzer, 'initial_valid_replay_data', [])

        # è¯¦ç»†è®°å½•ä¸¢é”¤æŒ‰é”®ä¿¡æ¯
        if drop_hammers:
            logger.info("ğŸ” ğŸª“ ä¸¢é”¤æŒ‰é”®è¯¦ç»†ä¿¡æ¯:")
            for i, error_note in enumerate(drop_hammers):
                if len(error_note.infos) > 0:
                    rec = error_note.infos[0]

                    # è·å–å®é™…éŸ³ç¬¦æ•°æ®ç”¨äºæ—¶é—´ä¿¡æ¯
                    time_info = f"keyOn={rec.keyOn/10:.2f}ms, keyOff={rec.keyOff/10:.2f}ms"
                    if rec.index < len(initial_valid_record_data):
                        record_note = initial_valid_record_data[rec.index]
                        if hasattr(record_note, 'after_touch') and record_note.after_touch is not None and len(record_note.after_touch.index) > 0:
                            key_on = (record_note.after_touch.index[0] + record_note.offset) / 10.0
                            key_off = (record_note.after_touch.index[-1] + record_note.offset) / 10.0
                            time_info = f"æŒ‰ä¸‹={key_on:.2f}ms, é‡Šæ”¾={key_off:.2f}ms"

                    logger.info(f"  ğŸª“ ä¸¢é”¤{i+1}: æŒ‰é”®ID={rec.keyId}, ç´¢å¼•={rec.index}, {time_info}")

        # è¯¦ç»†è®°å½•å¤šé”¤æŒ‰é”®ä¿¡æ¯
        if multi_hammers:
            logger.info("ğŸ” ğŸ”¨ å¤šé”¤æŒ‰é”®è¯¦ç»†ä¿¡æ¯:")
            for i, error_note in enumerate(multi_hammers):
                if len(error_note.infos) > 0:
                    play = error_note.infos[0]

                    # è·å–å®é™…éŸ³ç¬¦æ•°æ®ç”¨äºæ—¶é—´ä¿¡æ¯
                    time_info = f"keyOn={play.keyOn/10:.2f}ms, keyOff={play.keyOff/10:.2f}ms"
                    if play.index < len(initial_valid_replay_data):
                        replay_note = initial_valid_replay_data[play.index]
                        if hasattr(replay_note, 'after_touch') and replay_note.after_touch is not None and len(replay_note.after_touch.index) > 0:
                            key_on = (replay_note.after_touch.index[0] + replay_note.offset) / 10.0
                            key_off = (replay_note.after_touch.index[-1] + replay_note.offset) / 10.0
                            time_info = f"æŒ‰ä¸‹={key_on:.2f}ms, é‡Šæ”¾={key_off:.2f}ms"

                    logger.info(f"  ğŸ”¨ å¤šé”¤{i+1}: æŒ‰é”®ID={play.keyId}, ç´¢å¼•={play.index}, {time_info}")

        logger.info(f"ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ: ä¸¢é”¤={len(drop_hammers)}ä¸ª, å¤šé”¤={len(multi_hammers)}ä¸ª")

        return {
            'drop_hammers': drop_hammers,
            'multi_hammers': multi_hammers,
            'drop_count': len(drop_hammers),
            'multi_count': len(multi_hammers)
        }

    def _get_total_effective_keys(self) -> int:
        """è·å–æ€»æœ‰æ•ˆæŒ‰é”®æ•°"""
        initial_valid_record = getattr(self.algorithm.analyzer, 'initial_valid_record_data', None)
        initial_valid_replay = getattr(self.algorithm.analyzer, 'initial_valid_replay_data', None)

        total_valid_record = len(initial_valid_record) if initial_valid_record else 0
        total_valid_replay = len(initial_valid_replay) if initial_valid_replay else 0

        total_keys = total_valid_record + total_valid_replay
        print(f"[DEBUG] æ€»æœ‰æ•ˆæŒ‰é”®æ•°: record={total_valid_record}, replay={total_valid_replay}, total={total_keys}")
        return total_keys


class PianoAnalysisBackend:

    def __init__(self, session_id=None, history_manager=None):
        """
        åˆå§‹åŒ–é’¢ç´åˆ†æåç«¯
        
        Args:
            session_id: ä¼šè¯IDï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„åˆ†æä¼šè¯
            history_manager: å…¨å±€å†å²ç®¡ç†å™¨å®ä¾‹
        """
        self.session_id = session_id
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.data_manager = DataManager()
        self.data_filter = DataFilter()
        self.plot_generator = PlotGenerator(self.data_filter)
        self.time_filter = TimeFilter()
        self.table_generator = TableDataGenerator()

        # åˆå§‹åŒ–å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨ï¼ˆå¤ç”¨å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
        from backend.multi_algorithm_plot_generator import MultiAlgorithmPlotGenerator
        self.multi_algorithm_plot_generator = MultiAlgorithmPlotGenerator(self.data_filter)

        # åˆå§‹åŒ–åŠ›åº¦æ›²çº¿åˆ†æå™¨
        self.force_curve_analyzer = ForceCurveAnalyzer()
        
        # ä½¿ç”¨å…¨å±€çš„å†å²ç®¡ç†å™¨å®ä¾‹
        self.history_manager = history_manager
        
        # åˆå§‹åŒ–åˆ†æå™¨å®ä¾‹ï¼ˆå·²åºŸå¼ƒï¼Œä»…ç”¨äºå‘åå…¼å®¹ï¼‰
        self.analyzer = None
        
        # åˆå§‹åŒ–å»¶æ—¶åˆ†æå™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œå› ä¸ºéœ€è¦analyzerï¼‰
        self.delay_analysis = None
        
        # ==================== å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼ ====================
        # æ¨¡å¼å¼€å…³ï¼šå§‹ç»ˆä¸ºTrueï¼ˆä»…æ”¯æŒå¤šç®—æ³•æ¨¡å¼ï¼‰
        self.multi_algorithm_mode: bool = True
        
        # å¤šç®—æ³•ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œä»…åœ¨å¯ç”¨å¤šç®—æ³•æ¨¡å¼æ—¶åˆ›å»ºï¼‰
        self.multi_algorithm_manager: Optional[MultiAlgorithmManager] = None
        
        logger.info(f"âœ… PianoAnalysisBackendåˆå§‹åŒ–å®Œæˆ (Session: {session_id})")

    
    # ==================== æ•°æ®ç®¡ç†ç›¸å…³æ–¹æ³• ====================
    
    def clear_data_state(self) -> None:
        """æ¸…ç†æ‰€æœ‰æ•°æ®çŠ¶æ€"""
        self.data_manager.clear_data_state()
        self.plot_generator.set_data()
        self.data_filter.set_data(None, None)
        self.table_generator.set_data()
        self.analyzer = None
        
        # æ¸…ç†å¤šç®—æ³•ç®¡ç†å™¨
        if self.multi_algorithm_manager:
            self.multi_algorithm_manager.clear_all()

        # æ¸…é™¤ä¸Šä¼ çŠ¶æ€ï¼Œå…è®¸é‡æ–°ä¸Šä¼ åŒä¸€æ–‡ä»¶
        self._last_upload_content = None
        self._last_upload_time = None
        self._last_selected_history_id = None
        self._last_history_time = None

        logger.info("âœ… æ‰€æœ‰æ•°æ®çŠ¶æ€å·²æ¸…ç†")
    
    def set_upload_data_source(self, filename: str) -> None:
        """è®¾ç½®ä¸Šä¼ æ•°æ®æºä¿¡æ¯"""
        self.data_manager.set_upload_data_source(filename)
    
    def set_history_data_source(self, history_id: str, filename: str) -> None:
        """è®¾ç½®å†å²æ•°æ®æºä¿¡æ¯"""
        self.data_manager.set_history_data_source(history_id, filename)
    
    def get_data_source_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æºä¿¡æ¯"""
        return self.data_manager.get_data_source_info()
    
    def process_file_upload(self, contents, filename):
        """
        å¤„ç†æ–‡ä»¶ä¸Šä¼  - ç»Ÿä¸€çš„æ–‡ä»¶ä¸Šä¼ å…¥å£

        Args:
            contents: ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹ï¼ˆbase64ç¼–ç ï¼‰
            filename: ä¸Šä¼ æ–‡ä»¶çš„æ–‡ä»¶å

        Returns:
            tuple: (info_content, error_content, error_msg)
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„ä¸Šä¼ ç®¡ç†å™¨å¤„ç†
        from backend.upload_manager import UploadManager
        upload_manager = UploadManager(self)
        return upload_manager.process_upload(contents, filename)

    def process_spmid_upload(self, contents, filename):
        """
        å¤„ç†SPMIDæ–‡ä»¶ä¸Šä¼ å¹¶æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            contents: æ–‡ä»¶å†…å®¹ï¼ˆbase64ç¼–ç ï¼‰
            filename: æ–‡ä»¶å

        Returns:
            tuple: (success, result_data, error_msg)
        """
        try:
            # ä½¿ç”¨ä¸Šä¼ ç®¡ç†å™¨å¤„ç†æ–‡ä»¶
            from backend.upload_manager import UploadManager
            upload_manager = UploadManager(self)
            success, result_data, error_msg = upload_manager.process_upload(contents, filename)

            if not success:
                return success, result_data, error_msg

            # æ‰§è¡Œæ•°æ®åˆ†æ
            self.analyze_data()

            return True, result_data, None

        except Exception as e:
            logger.error(f"SPMIDæ–‡ä»¶å¤„ç†å¼‚å¸¸: {e}")
            return False, None, f"å¤„ç†å¼‚å¸¸: {str(e)}"

    def analyze_data(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹

        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸ¯ å¼€å§‹æ•°æ®åˆ†ææµç¨‹")

            # è·å–è¿‡æ»¤åçš„æ•°æ®
            record_data = self.data_manager.get_record_data()
            replay_data = self.data_manager.get_replay_data()

            if not record_data or not replay_data:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å½•åˆ¶æˆ–æ’­æ”¾æ•°æ®")
                return False

            # åˆ›å»ºåˆ†æå™¨
            from spmid.spmid_analyzer import SPMIDAnalyzer
            self.analyzer = SPMIDAnalyzer()

            # æ‰§è¡Œåˆ†æ
            success = self.analyzer.analyze(record_data, replay_data)

            if success:
                logger.info("âœ… æ•°æ®åˆ†æå®Œæˆ")
                return True
            else:
                logger.error("âŒ æ•°æ®åˆ†æå¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åˆ†æå¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def generate_report_content(self):
        """
        ç”ŸæˆæŠ¥å‘Šå†…å®¹ï¼ˆç”¨äºå•ç®—æ³•æ¨¡å¼ï¼‰

        Returns:
            html.Div: æŠ¥å‘Šå†…å®¹
        """
        try:
            from ui.layout_components import create_report_layout
            return create_report_layout(self)
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå†…å®¹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return html.Div([
                dbc.Alert(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", color="danger")
            ])

    def export_pre_match_data_to_csv(self, filename: str = None) -> Optional[str]:
        """
        å¯¼å‡ºåŒ¹é…å‰çš„æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆæµ‹è¯•åŠŸèƒ½ï¼‰

        åœ¨æŒ‰é”®åŒ¹é…ä¹‹å‰è¿›è¡Œç¼–å·ï¼Œä¸ºå½•åˆ¶å’Œæ’­æ”¾éŸ³ç¬¦åˆ†åˆ«åˆ†é…ç´¢å¼•å¹¶å¯¼å‡ºCSVã€‚

        Args:
            filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            str: CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¯¼å‡ºå¤±è´¥åˆ™è¿”å›None
        """
        try:
            import csv
            import os
            from datetime import datetime

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åˆ†æå™¨å’Œæ•°æ®
            if not self.analyzer:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨ï¼Œæ— æ³•å¯¼å‡ºåŒ¹é…å‰æ•°æ®")
                return None

            # è·å–åŒ¹é…å‰çš„æ•°æ®ï¼ˆç©ºæ•°æ®è¿‡æ»¤ä¹‹åï¼ŒæŒ‰é”®åŒ¹é…ä¹‹å‰ï¼‰
            initial_valid_record = self.analyzer.get_initial_valid_record_data() if hasattr(self.analyzer, 'get_initial_valid_record_data') else None
            initial_valid_replay = self.analyzer.get_initial_valid_replay_data() if hasattr(self.analyzer, 'get_initial_valid_replay_data') else None

            if not initial_valid_record or not initial_valid_replay:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…å‰çš„æ•°æ®ï¼Œæ— æ³•å¯¼å‡º")
                return None


            # ç”Ÿæˆæ–‡ä»¶å
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"pre_match_data_{timestamp}.csv"

            # ç¡®ä¿æ–‡ä»¶åæœ‰.csvæ‰©å±•å
            if not filename.endswith('.csv'):
                filename += '.csv'

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = "exports"
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            filepath = os.path.join(output_dir, filename)

            # å‡†å¤‡CSVæ•°æ® - å°†å½•åˆ¶å’Œæ’­æ”¾ç´¢å¼•å¹¶æ’ç¼–å·
            csv_data = []

            # è·å–å½•åˆ¶å’Œæ’­æ”¾æ•°æ®çš„æ•°é‡
            record_count = len(initial_valid_record)
            replay_count = len(initial_valid_replay)

            # ä½¿ç”¨è¾ƒå¤§çš„æ•°é‡ä½œä¸ºè¡Œæ•°
            max_count = max(record_count, replay_count)

            # å¹¶æ’ç¼–å·å½•åˆ¶å’Œæ’­æ”¾ç´¢å¼•
            for i in range(max_count):
                # å½•åˆ¶æ•°æ®
                if i < record_count:
                    record_note = initial_valid_record[i]
                    record_index = i  # å½•åˆ¶ç´¢å¼•
                    record_key_id = getattr(record_note, 'id', 'N/A')

                    # è·å–å½•åˆ¶éŸ³ç¬¦çš„æ—¶é—´ä¿¡æ¯
                    record_keyon_time = 0
                    if hasattr(record_note, 'after_touch') and record_note.after_touch is not None and not record_note.after_touch.empty:
                        record_keyon_time = record_note.after_touch.index[0] + record_note.offset
                    elif hasattr(record_note, 'hammers') and record_note.hammers is not None and not record_note.hammers.empty:
                        record_keyon_time = record_note.hammers.index[0] + record_note.offset
                else:
                    record_index = -1  # æ²¡æœ‰å½•åˆ¶æ•°æ®
                    record_key_id = 'N/A'
                    record_keyon_time = 0

                # æ’­æ”¾æ•°æ®
                if i < replay_count:
                    replay_note = initial_valid_replay[i]
                    replay_index = i  # æ’­æ”¾ç´¢å¼•
                    replay_key_id = getattr(replay_note, 'id', 'N/A')

                    # è·å–æ’­æ”¾éŸ³ç¬¦çš„æ—¶é—´ä¿¡æ¯
                    replay_keyon_time = 0
                    if hasattr(replay_note, 'after_touch') and replay_note.after_touch is not None and not replay_note.after_touch.empty:
                        replay_keyon_time = replay_note.after_touch.index[0] + replay_note.offset
                    elif hasattr(replay_note, 'hammers') and replay_note.hammers is not None and not replay_note.hammers.empty:
                        replay_keyon_time = replay_note.hammers.index[0] + replay_note.offset
                else:
                    replay_index = -1  # æ²¡æœ‰æ’­æ”¾æ•°æ®
                    replay_key_id = 'N/A'
                    replay_keyon_time = 0

                csv_data.append({
                    'ç®—æ³•åç§°': 'å•ç®—æ³•æ¨¡å¼',  # å›ºå®šå€¼
                    'æ˜¾ç¤ºåç§°': 'å•ç®—æ³•æ¨¡å¼',  # å›ºå®šå€¼
                    'å½•åˆ¶ç´¢å¼•': record_index,
                    'å›æ”¾ç´¢å¼•': replay_index,
                    'å½•åˆ¶æŒ‰é”®ID': record_key_id,
                    'å›æ”¾æŒ‰é”®ID': replay_key_id,
                    'å½•åˆ¶æŒ‰é”®æ—¶é—´(ms)': record_keyon_time / 10.0 if record_keyon_time else 0,
                    'å›æ”¾æŒ‰é”®æ—¶é—´(ms)': replay_keyon_time / 10.0 if replay_keyon_time else 0
                })

            # å†™å…¥CSV
            fieldnames = ['ç®—æ³•åç§°', 'æ˜¾ç¤ºåç§°', 'å½•åˆ¶ç´¢å¼•', 'å›æ”¾ç´¢å¼•',
                         'å½•åˆ¶æŒ‰é”®ID', 'å›æ”¾æŒ‰é”®ID',
                         'å½•åˆ¶æŒ‰é”®æ—¶é—´(ms)', 'å›æ”¾æŒ‰é”®æ—¶é—´(ms)']

            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(csv_data)

            logger.info(f"âœ… åŒ¹é…å‰æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
            logger.info(f"ğŸ“Š å½•åˆ¶éŸ³ç¬¦: {len(initial_valid_record)} ä¸ª, æ’­æ”¾éŸ³ç¬¦: {len(initial_valid_replay)} ä¸ª")
            logger.info(f"ğŸ“Š å¯¼å‡ºæ€»è®°å½•æ•°: {len(csv_data)} æ¡")
            return filepath

        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºåŒ¹é…å‰æ•°æ®å¤±è´¥: {e}")
            return None

    def process_history_selection(self, history_id):
        """
        å¤„ç†å†å²è®°å½•é€‰æ‹© - ç»Ÿä¸€çš„å†å²è®°å½•å…¥å£
        
        Args:
            history_id: å†å²è®°å½•ID
            
        Returns:
            tuple: (success, result_data, error_msg)
        """
        return self.history_manager.process_history_selection(history_id, self)
    
    def load_spmid_data(self, spmid_bytes: bytes) -> bool:
        """
        åŠ è½½SPMIDæ•°æ®
        
        Args:
            spmid_bytes: SPMIDæ–‡ä»¶å­—èŠ‚æ•°æ®
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            # ä½¿ç”¨æ•°æ®ç®¡ç†å™¨åŠ è½½æ•°æ®
            success = self.data_manager.load_spmid_data(spmid_bytes)
            
            if success:
                # åŒæ­¥æ•°æ®åˆ°å„ä¸ªæ¨¡å—
                self._sync_data_to_modules()
                logger.info("âœ… SPMIDæ•°æ®åŠ è½½æˆåŠŸ")
            else:
                logger.error("âŒ SPMIDæ•°æ®åŠ è½½å¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ SPMIDæ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            return False
            
    def _sync_data_to_modules(self) -> None:
        """åŒæ­¥æ•°æ®åˆ°å„ä¸ªæ¨¡å—"""
        # è·å–æ•°æ®
        record_data = self.data_manager.get_record_data()
        replay_data = self.data_manager.get_replay_data()
        valid_record_data = self.data_manager.get_valid_record_data()
        valid_replay_data = self.data_manager.get_valid_replay_data()
        
        # åŒæ­¥åˆ°å„ä¸ªæ¨¡å—
        self.plot_generator.set_data(valid_record_data, valid_replay_data, analyzer=self.analyzer)
        self.data_filter.set_data(valid_record_data, valid_replay_data)
        self.time_filter.set_data(valid_record_data, valid_replay_data)
        
        # å¦‚æœæœ‰åˆ†æå™¨ï¼ŒåŒæ­¥åˆ†æç»“æœ
        if self.analyzer:
            self._sync_analysis_results()
    
    def _sync_analysis_results(self) -> None:
        """åŒæ­¥åˆ†æç»“æœåˆ°å„ä¸ªæ¨¡å—"""
        if not self.analyzer:
            return
        
        try:
            # è·å–åˆ†æç»“æœï¼ˆå¦‚æœæ²¡æœ‰ï¼Œåˆ™å°è¯•ä»_error_infoä¸­è·å–ï¼‰
            multi_hammers = getattr(self.analyzer, 'multi_hammers', [])
            drop_hammers = getattr(self.analyzer, 'drop_hammers', [])
            silent_hammers = getattr(self.analyzer, 'silent_hammers', [])
            invalid_notes_table_data = getattr(self.analyzer, 'invalid_notes_table_data', {})
            matched_pairs = getattr(self.analyzer, 'matched_pairs', [])

            # å¦‚æœæ²¡æœ‰é”™è¯¯æ•°æ®ï¼Œå°è¯•ä»_error_infoä¸­è·å–
            if not drop_hammers and hasattr(self.analyzer, '_error_info'):
                drop_hammers = self.analyzer._error_info.get('drop_hammers', [])
                multi_hammers = self.analyzer._error_info.get('multi_hammers', [])
                silent_hammers = self.analyzer._error_info.get('silent_hammers', [])
                logger.info(f"ğŸ“Š ä»_error_infoè·å–é”™è¯¯æ•°æ®: ä¸¢é”¤={len(drop_hammers)}, å¤šé”¤={len(multi_hammers)}")

            # ç¡®ä¿analyzerå¯¹è±¡æœ‰è¿™äº›å±æ€§ï¼ˆä¾›ç€‘å¸ƒå›¾ç”Ÿæˆå™¨ä½¿ç”¨ï¼‰
            self.analyzer.drop_hammers = drop_hammers
            self.analyzer.multi_hammers = multi_hammers
            self.analyzer.silent_hammers = silent_hammers
            logger.info(f"âœ… è®¾ç½®analyzeré”™è¯¯æ•°æ®: ä¸¢é”¤={len(drop_hammers)}, å¤šé”¤={len(multi_hammers)}")
            
            # åˆå¹¶æ‰€æœ‰é”™è¯¯éŸ³ç¬¦
            all_error_notes = multi_hammers + drop_hammers + silent_hammers
            
            # è®¾ç½®all_error_noteså±æ€§ä¾›UIå±‚ä½¿ç”¨
            self.all_error_notes = all_error_notes
            
            # åŒæ­¥åˆ°å„ä¸ªæ¨¡å—
            valid_record_data = getattr(self.analyzer, 'valid_record_data', [])
            valid_replay_data = getattr(self.analyzer, 'valid_replay_data', [])
            self.data_filter.set_data(valid_record_data, valid_replay_data)
            
            # è·å–æœ‰æ•ˆæ•°æ®
            valid_record_data = self.data_manager.get_valid_record_data()
            valid_replay_data = self.data_manager.get_valid_replay_data()
            
            self.plot_generator.set_data(valid_record_data, valid_replay_data, matched_pairs, analyzer=self.analyzer)
            
            # åŒæ­¥åˆ°TimeFilter
            self.time_filter.set_data(valid_record_data, valid_replay_data)
            
            logger.info(f"ğŸ”„ åŒæ­¥é”™è¯¯æ•°æ®åˆ°table_generator: ä¸¢é”¤={len(drop_hammers)}, å¤šé”¤={len(multi_hammers)}")

            self.table_generator.set_data(
                valid_record_data=valid_record_data,
                valid_replay_data=valid_replay_data,
                multi_hammers=multi_hammers,
                drop_hammers=drop_hammers,
                silent_hammers=silent_hammers,
                all_error_notes=all_error_notes,
                invalid_notes_table_data=invalid_notes_table_data,
                matched_pairs=matched_pairs,
                analyzer=self.analyzer
            )

            # éªŒè¯æ•°æ®åŒæ­¥æ˜¯å¦æˆåŠŸ
            if hasattr(self.analyzer, 'drop_hammers'):
                drop_hammers_count = len(self.analyzer.drop_hammers)
                multi_hammers_count = len(getattr(self.analyzer, 'multi_hammers', []))
                logger.info(f"âœ… analyzeré”™è¯¯æ•°æ®åŒæ­¥éªŒè¯: ä¸¢é”¤={drop_hammers_count}, å¤šé”¤={multi_hammers_count}")

                # è·å–åŸå§‹æ•°æ®ç”¨äºè¯¦ç»†ä¿¡æ¯æ˜¾ç¤º
                initial_valid_record_data = getattr(self.analyzer, 'initial_valid_record_data', [])
                initial_valid_replay_data = getattr(self.analyzer, 'initial_valid_replay_data', [])

                # è¯¦ç»†è®°å½•åŒæ­¥åçš„ä¸¢é”¤æŒ‰é”®ä¿¡æ¯
                if drop_hammers_count > 0:
                    logger.info("ğŸ” ğŸ“Š æ•°æ®åŒæ­¥åä¸¢é”¤æŒ‰é”®è¯¦ç»†ä¿¡æ¯:")
                    for i, error_note in enumerate(self.analyzer.drop_hammers):  # æ˜¾ç¤ºæ‰€æœ‰ä¸¢é”¤
                        if len(error_note.infos) > 0:
                            rec = error_note.infos[0]

                            # è·å–å®é™…éŸ³ç¬¦æ•°æ®ç”¨äºæ—¶é—´ä¿¡æ¯
                            time_info = "æ—¶é—´ä¿¡æ¯ä¸å¯ç”¨"
                            if rec.index < len(initial_valid_record_data):
                                record_note = initial_valid_record_data[rec.index]
                                if hasattr(record_note, 'after_touch') and record_note.after_touch is not None and len(record_note.after_touch.index) > 0:
                                    key_on = (record_note.after_touch.index[0] + record_note.offset) / 10.0
                                    key_off = (record_note.after_touch.index[-1] + record_note.offset) / 10.0
                                    time_info = f"æŒ‰ä¸‹={key_on:.2f}ms, é‡Šæ”¾={key_off:.2f}ms"

                            logger.info(f"  ğŸª“ ä¸¢é”¤{i+1}: æŒ‰é”®ID={rec.keyId}, ç´¢å¼•={rec.index}, {time_info}")

                # è¯¦ç»†è®°å½•åŒæ­¥åçš„å¤šé”¤æŒ‰é”®ä¿¡æ¯
                if multi_hammers_count > 0:
                    logger.info("ğŸ” ğŸ“Š æ•°æ®åŒæ­¥åå¤šé”¤æŒ‰é”®è¯¦ç»†ä¿¡æ¯:")
                    for i, error_note in enumerate(self.analyzer.multi_hammers):  # æ˜¾ç¤ºæ‰€æœ‰å¤šé”¤
                        if len(error_note.infos) > 0:
                            play = error_note.infos[0]

                            # è·å–å®é™…éŸ³ç¬¦æ•°æ®ç”¨äºæ—¶é—´ä¿¡æ¯
                            time_info = "æ—¶é—´ä¿¡æ¯ä¸å¯ç”¨"
                            if play.index < len(initial_valid_replay_data):
                                replay_note = initial_valid_replay_data[play.index]
                                if hasattr(replay_note, 'after_touch') and replay_note.after_touch is not None and len(replay_note.after_touch.index) > 0:
                                    key_on = (replay_note.after_touch.index[0] + replay_note.offset) / 10.0
                                    key_off = (replay_note.after_touch.index[-1] + replay_note.offset) / 10.0
                                    time_info = f"æŒ‰ä¸‹={key_on:.2f}ms, é‡Šæ”¾={key_off:.2f}ms"

                            logger.info(f"  ğŸ”¨ å¤šé”¤{i+1}: æŒ‰é”®ID={play.keyId}, ç´¢å¼•={play.index}, {time_info}")

                # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡æ±‡æ€»
                logger.info("ğŸ“ˆ ğŸ¯ æœ€ç»ˆé”™è¯¯ç»Ÿè®¡æ±‡æ€»:")
                logger.info(f"   æ€»ä¸¢é”¤æ•°é‡: {drop_hammers_count} ä¸ª")
                logger.info(f"   æ€»å¤šé”¤æ•°é‡: {multi_hammers_count} ä¸ª")
                logger.info(f"   æ€»é”™è¯¯æ•°é‡: {drop_hammers_count + multi_hammers_count} ä¸ª")
            else:
                logger.warning("âš ï¸ analyzeré”™è¯¯æ•°æ®åŒæ­¥å¤±è´¥ï¼šdrop_hammerså±æ€§ä¸å­˜åœ¨")
            
            logger.info("âœ… åˆ†æç»“æœåŒæ­¥å®Œæˆ")

        except Exception as e:
            logger.error(f"åŒæ­¥åˆ†æç»“æœå¤±è´¥: {e}")
    
    def get_global_average_delay(self) -> float:
        """
        è·å–æ•´é¦–æ›²å­çš„å¹³å‡æ—¶å»¶ï¼ˆåŸºäºå·²é…å¯¹æ•°æ®ï¼‰
        
        Returns:
            float: å¹³å‡æ—¶å»¶ï¼ˆ0.1mså•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        # ä¿æŒå†…éƒ¨å•ä½ä¸º0.1msï¼Œç”±UIå±‚è´Ÿè´£æ˜¾ç¤ºæ—¶æ¢ç®—ä¸ºms
        average_delay_0_1ms = self.analyzer.get_global_average_delay()
        return average_delay_0_1ms
    
    def get_variance(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„æ€»ä½“æ–¹å·®
        
        Returns:
            float: æ€»ä½“æ–¹å·®ï¼ˆ(0.1ms)Â²å•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        variance_0_1ms_squared = self.analyzer.get_variance()
        return variance_0_1ms_squared
    
    def get_standard_deviation(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„æ€»ä½“æ ‡å‡†å·®
        
        Returns:
            float: æ€»ä½“æ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        std_0_1ms = self.analyzer.get_standard_deviation()
        return std_0_1ms
    
    def get_mean_absolute_error(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰
        
        Returns:
            float: å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆ0.1mså•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        mae_0_1ms = self.analyzer.get_mean_absolute_error()
        return mae_0_1ms
    
    def get_mean_squared_error(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
        
        Returns:
            float: å‡æ–¹è¯¯å·®ï¼ˆ(0.1ms)Â²å•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        mse_0_1ms_squared = self.analyzer.get_mean_squared_error()
        return mse_0_1ms_squared

    def get_root_mean_squared_error(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰
        
        Returns:
            float: å‡æ–¹æ ¹è¯¯å·®ï¼ˆ0.1mså•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        rmse_0_1ms = self.analyzer.get_root_mean_squared_error()
        return rmse_0_1ms
    
    def get_mean_error(self) -> float:
        """
        è·å–å·²åŒ¹é…æŒ‰é”®å¯¹çš„å¹³å‡è¯¯å·®ï¼ˆMEï¼‰
        
        Returns:
            float: å¹³å‡è¯¯å·®MEï¼ˆ0.1mså•ä½ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        me_0_1ms = self.analyzer.get_mean_error()
        return me_0_1ms
    
    def get_coefficient_of_variation(self) -> float:
        """
        è·å–å·²é…å¯¹æŒ‰é”®çš„å˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variation, CVï¼‰
        
        Returns:
            float: å˜å¼‚ç³»æ•°ï¼ˆç™¾åˆ†æ¯”ï¼Œä¾‹å¦‚ 15.5 è¡¨ç¤º 15.5%ï¼‰
        """
        if not self.analyzer:
            return 0.0
        
        cv = self.analyzer.get_coefficient_of_variation()
        return cv
    
    def _get_delay_time_series_raw_data(self) -> Optional[List[Dict[str, Any]]]:
        """
        è·å–å»¶æ—¶æ—¶é—´åºåˆ—å›¾çš„åŸå§‹æ•°æ®

        Returns:
            Optional[List[Dict[str, Any]]]: ç²¾ç¡®åŒ¹é…æ•°æ®åˆ—è¡¨ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("[WARNING] æ²¡æœ‰åˆ†æå™¨æˆ–éŸ³ç¬¦åŒ¹é…å™¨ï¼Œæ— æ³•è·å–æ•°æ®")
                return None

            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            logger.info(f"[DEBUG] è·å–åˆ°ç²¾ç¡®åŒ¹é…æ•°æ®: {len(offset_data)} æ¡è®°å½•")

            if not offset_data:
                logger.warning("[WARNING] æ— ç²¾ç¡®åŒ¹é…æ•°æ®ï¼ˆâ‰¤50msï¼‰")
                return None

            return offset_data

        except Exception as e:
            logger.error(f"[ERROR] è·å–å»¶æ—¶æ—¶é—´åºåˆ—åŸå§‹æ•°æ®å¤±è´¥: {e}")
            return None

    def _process_delay_time_series_data(self, offset_data: List[Dict[str, Any]]) -> Optional[List[Dict[str, Any]]]:
        """
        å¤„ç†å’Œè¿‡æ»¤å»¶æ—¶æ—¶é—´åºåˆ—æ•°æ®

        Args:
            offset_data: åŸå§‹åç§»æ•°æ®

        Returns:
            Optional[List[Dict[str, Any]]]: å¤„ç†åçš„æ•°æ®ç‚¹åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´æ’åº
        """
        data_points = []  # å­˜å‚¨æ‰€æœ‰æ•°æ®ç‚¹ï¼Œç”¨äºæ’åº
        skipped_count = 0

        for i, item in enumerate(offset_data):
            record_keyon = item.get('record_keyon')  # å•ä½ï¼š0.1ms
            keyon_offset = item.get('keyon_offset')  # å•ä½ï¼š0.1ms
            key_id = item.get('key_id')
            record_index = item.get('record_index')
            replay_index = item.get('replay_index')

            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone
            if record_keyon is None or keyon_offset is None:
                logger.debug(f"[DEBUG] è·³è¿‡ç¬¬{i}æ¡è®°å½•: record_keyonæˆ–keyon_offsetä¸ºNone")
                skipped_count += 1
                continue

            # æ£€æŸ¥æ—¶é—´å’Œåç§»é‡æ˜¯å¦æœ‰æ•ˆï¼ˆä¸ºæœ‰æ•ˆæ•°å­—ï¼‰
            if not isinstance(record_keyon, (int, float)):
                logger.debug(f"[DEBUG] è·³è¿‡ç¬¬{i}æ¡è®°å½•: record_keyonæ— æ•ˆ ({record_keyon})")
                skipped_count += 1
                continue
            if not isinstance(keyon_offset, (int, float)):
                logger.debug(f"[DEBUG] è·³è¿‡ç¬¬{i}æ¡è®°å½•: keyon_offsetæ— æ•ˆ ({keyon_offset})")
                skipped_count += 1
                continue

            # è½¬æ¢ä¸ºmså•ä½
            time_ms = record_keyon / 10.0
            delay_ms = keyon_offset / 10.0

            data_points.append({
                'time': time_ms,
                'delay': delay_ms,
                'key_id': key_id if key_id is not None else 'N/A',
                'record_index': record_index,
                'replay_index': replay_index
            })

        logger.info(f"[DEBUG] æ•°æ®å¤„ç†å®Œæˆ: åŸå§‹æ•°æ® {len(offset_data)} æ¡, æœ‰æ•ˆæ•°æ®ç‚¹ {len(data_points)} æ¡, è·³è¿‡ {skipped_count} æ¡")

        if not data_points:
            logger.warning("[WARNING] æ— æœ‰æ•ˆæ—¶é—´åºåˆ—æ•°æ®")
            return None

        # æŒ‰æ—¶é—´æ’åºï¼Œç¡®ä¿æŒ‰æ—¶é—´é¡ºåºæ˜¾ç¤º
        data_points.sort(key=lambda x: x['time'])
        return data_points

    def _calculate_relative_delays(self, data_points: List[Dict[str, Any]]) -> Tuple[List[float], float]:
        """
        è®¡ç®—ç›¸å¯¹å»¶æ—¶æ•°æ®

        Args:
            data_points: æ’åºåçš„æ•°æ®ç‚¹åˆ—è¡¨

        Returns:
            Tuple[List[float], float]: (ç›¸å¯¹å»¶æ—¶åˆ—è¡¨, å¹³å‡å»¶æ—¶ms)
        """
        # è®¡ç®—å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
        me_0_1ms = self.get_mean_error()  # å¹³å‡å»¶æ—¶ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
        mean_delay = me_0_1ms / 10.0  # å¹³å‡å»¶æ—¶ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰

        # è®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼šæ¯ä¸ªç‚¹çš„å»¶æ—¶å‡å»å¹³å‡å»¶æ—¶
        # æ ‡å‡†å…¬å¼ï¼šç›¸å¯¹å»¶æ—¶ = å»¶æ—¶ - å¹³å‡å»¶æ—¶ï¼ˆå¯¹æ‰€æœ‰ç‚¹ç»Ÿä¸€é€‚ç”¨ï¼‰
        relative_delays_ms = []
        for point in data_points:
            delay_ms = point['delay']
            relative_delay = delay_ms - mean_delay
            relative_delays_ms.append(relative_delay)

        return relative_delays_ms, mean_delay

    def _prepare_time_series_plot_data(self, data_points: List[Dict[str, Any]], mean_delay: float) -> Tuple[List[float], List[float], List[List[Any]]]:
        """
        å‡†å¤‡å›¾è¡¨ç»˜åˆ¶æ‰€éœ€çš„æ•°æ®æ ¼å¼

        Args:
            data_points: æ’åºåçš„æ•°æ®ç‚¹åˆ—è¡¨
            mean_delay: å¹³å‡å»¶æ—¶ï¼ˆmsï¼‰

        Returns:
            Tuple[List[float], List[float], List[List[Any]]]: (æ—¶é—´åˆ—è¡¨, å»¶æ—¶åˆ—è¡¨, è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨)
        """
        # æå–æ’åºåçš„æ•°æ®
        times_ms = [point['time'] for point in data_points]
        delays_ms = [point['delay'] for point in data_points]  # ä¿ç•™åŸå§‹å»¶æ—¶ç”¨äºhoveræ˜¾ç¤º
        # customdata åŒ…å« [key_id, record_index, replay_index, åŸå§‹å»¶æ—¶, å¹³å‡å»¶æ—¶]ï¼Œç”¨äºç‚¹å‡»æ—¶æŸ¥æ‰¾åŒ¹é…å¯¹å’Œæ˜¾ç¤ºåŸå§‹å€¼
        customdata_list = [[point['key_id'], point['record_index'], point['replay_index'], point['delay'], mean_delay]
                          for point in data_points]

        return times_ms, delays_ms, customdata_list

    def _configure_delay_plot_axes(self, delays_ms: List[float], is_relative: bool = False) -> Tuple[float, float, float]:
        """
        é…ç½®å»¶æ—¶å›¾è¡¨çš„Yè½´å‚æ•°

        Args:
            delays_ms: å»¶æ—¶æ•°æ®åˆ—è¡¨
            is_relative: æ˜¯å¦ä¸ºç›¸å¯¹å»¶æ—¶å›¾

        Returns:
            Tuple[float, float, float]: (y_axis_min, y_axis_max, dtick)
        """
        if not delays_ms:
            # é»˜è®¤é…ç½®
            return (-50, 50, 10) if not is_relative else (-15, 15, 3)

        y_min = min(delays_ms)
        y_max = max(delays_ms)

        # ä½¿ç”¨å·²æœ‰çš„ç»Ÿè®¡ä¿¡æ¯æ¥ç¡®å®šåˆé€‚çš„Yè½´èŒƒå›´
        mean_delay_0_1ms = self.get_mean_error()  # å¹³å‡å»¶æ—¶ï¼ˆ0.1mså•ä½ï¼‰
        std_dev_0_1ms = self.get_standard_deviation()  # æ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼‰

        # è½¬æ¢ä¸ºmså•ä½
        mean_delay_ms = mean_delay_0_1ms / 10.0
        std_dev_ms = std_dev_0_1ms / 10.0

        if is_relative:
            # ç›¸å¯¹å»¶æ—¶é€šå¸¸é›†ä¸­åœ¨0é™„è¿‘ï¼ŒåŸºäºæ ‡å‡†å·®è®¾ç½®åˆç†çš„å¯¹ç§°èŒƒå›´
            if std_dev_ms <= 2:  # æ•°æ®é«˜åº¦é›†ä¸­
                y_half_range = 8  # Â±8ms
                dtick = 2
            elif std_dev_ms <= 5:  # ä¸­ç­‰é›†ä¸­
                y_half_range = 12  # Â±12ms
                dtick = 3
            elif std_dev_ms <= 10:  # é€‚ä¸­ç¦»æ•£
                y_half_range = 20  # Â±20ms
                dtick = 4
            elif std_dev_ms <= 25:  # è¾ƒå¤§ç¦»æ•£
                y_half_range = 40  # Â±40ms
                dtick = 8
            else:  # è¶…å¤§ç¦»æ•£
                y_half_range = max(40, std_dev_ms * 1.5)  # è‡³å°‘Â±40msï¼Œæˆ–1.5å€æ ‡å‡†å·®
                dtick = 10

            # ä»¥0ä¸ºä¸­å¿ƒå¯¹ç§°æ˜¾ç¤ºï¼ˆç›¸å¯¹å»¶æ—¶çš„ç‰¹ç‚¹ï¼‰
            # ä½†ç¡®ä¿èƒ½æ˜¾ç¤ºæ‰€æœ‰å®é™…æ•°æ®ç‚¹
            y_axis_min = min(y_min - 1, -y_half_range)  # æ˜¾ç¤ºå®é™…æœ€å°å€¼ï¼Œæˆ–å¯¹ç§°èŒƒå›´çš„æœ€å°å€¼
            y_axis_max = max(y_max + 1, y_half_range)   # æ˜¾ç¤ºå®é™…æœ€å¤§å€¼ï¼Œæˆ–å¯¹ç§°èŒƒå›´çš„æœ€å¤§å€¼

            # ç¡®ä¿æœ€å°èŒƒå›´ä¸ºÂ±5ms
            if y_axis_max - y_axis_min < 10:
                y_axis_min = -5
                y_axis_max = 5
                dtick = 1
        else:
            # åŸå§‹å»¶æ—¶ï¼šæ ¹æ®æ•°æ®åˆ†å¸ƒæ™ºèƒ½é€‰æ‹©Yè½´èŒƒå›´å’Œåˆ»åº¦
            # ä½¿ç”¨3å€æ ‡å‡†å·®ä½œä¸ºåˆç†çš„æ˜¾ç¤ºèŒƒå›´
            suggested_half_range = max(15, std_dev_ms * 3)  # è‡³å°‘æ˜¾ç¤ºÂ±15msï¼Œæˆ–3å€æ ‡å‡†å·®

            if std_dev_ms <= 5:  # æ•°æ®é›†ä¸­åˆ†å¸ƒ
                dtick = 2
            elif std_dev_ms <= 20:  # ä¸­ç­‰ç¦»æ•£åº¦
                dtick = 5
            elif std_dev_ms <= 100:  # å¤§ç¦»æ•£åº¦
                dtick = 10
            else:  # è¶…å¤§ç¦»æ•£åº¦
                dtick = 20

            # ä»¥å¹³å‡å€¼ä¸ºä¸­å¿ƒè®¾ç½®Yè½´èŒƒå›´ï¼Œä½†ç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰æ•°æ®ç‚¹
            y_center = mean_delay_ms
            y_axis_min = min(y_min - 2, y_center - suggested_half_range)
            y_axis_max = max(y_max + 2, y_center + suggested_half_range)

        return y_axis_min, y_axis_max, dtick

    def _create_raw_delay_plot(self, times_ms: List[float], delays_ms: List[float], customdata_list: List[List[Any]]) -> Any:
        """
        åˆ›å»ºåŸå§‹å»¶æ—¶æ—¶é—´åºåˆ—å›¾

        Args:
            times_ms: æ—¶é—´æ•°æ®ï¼ˆmsï¼‰
            delays_ms: åŸå§‹å»¶æ—¶æ•°æ®ï¼ˆmsï¼‰
            customdata_list: è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨

        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        import plotly.graph_objects as go

        # åˆ›å»ºåç§»ä¹‹å‰çš„æ—¶é—´åºåˆ—å›¾
        raw_delay_fig = go.Figure()
        raw_delay_fig.add_trace(go.Scatter(
            x=times_ms,  # Xè½´ä½¿ç”¨å½•åˆ¶æ—¶é—´
            y=delays_ms,  # Yè½´ä½¿ç”¨åŸå§‹å»¶æ—¶
            mode='markers+lines',
            name='åŸå§‹å»¶æ—¶æ—¶é—´åºåˆ—',
            marker=dict(
                size=6,
                color='#FF9800',  # æ©™è‰²
                symbol='circle'  # å®å¿ƒåœ†ç‚¹
            ),
            line=dict(color='#FF9800', width=1.5),
            hovertemplate='<b>å½•åˆ¶æ—¶é—´</b>: %{x:.2f}ms<br>' +
                         '<b>åŸå§‹å»¶æ—¶</b>: %{y:.2f}ms<br>' +
                         '<b>æŒ‰é”®ID</b>: %{customdata[0]}<br>' +
                         '<extra></extra>',
            customdata=customdata_list
        ))

        # é…ç½®Yè½´
        y_axis_min, y_axis_max, dtick = self._configure_delay_plot_axes(delays_ms, is_relative=False)

        # é…ç½®åç§»å‰å›¾è¡¨çš„å¸ƒå±€
        raw_delay_fig.update_layout(
            xaxis_title='å½•åˆ¶æ—¶é—´ (ms)',
            yaxis_title='åŸå§‹å»¶æ—¶ (ms)',
            showlegend=True,
            template='plotly_white',
            height=400,
            hovermode='closest'
        )

        # æ ¹æ®æ•°æ®åŠ¨æ€è®¾ç½®Yè½´åˆ»åº¦å’ŒèŒƒå›´
        raw_delay_fig.update_yaxes(
            range=[y_axis_min, y_axis_max],
            dtick=dtick,
            tickformat='.1f'  # æ˜¾ç¤º1ä½å°æ•°
        )

        return raw_delay_fig

    def _create_relative_delay_plot(self, times_ms: List[float], relative_delays_ms: List[float],
                                   customdata_list: List[List[Any]], mean_delay: float) -> Any:
        """
        åˆ›å»ºç›¸å¯¹å»¶æ—¶æ—¶é—´åºåˆ—å›¾

        Args:
            times_ms: æ—¶é—´æ•°æ®ï¼ˆmsï¼‰
            relative_delays_ms: ç›¸å¯¹å»¶æ—¶æ•°æ®ï¼ˆmsï¼‰
            customdata_list: è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨
            mean_delay: å¹³å‡å»¶æ—¶ï¼ˆmsï¼‰

        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        import plotly.graph_objects as go

        # åˆ›å»ºåç§»ä¹‹åçš„æ—¶é—´åºåˆ—å›¾
        relative_delay_fig = go.Figure()
        relative_delay_fig.add_trace(go.Scatter(
            x=times_ms,
            y=relative_delays_ms,
            mode='markers+lines',  # åŒæ—¶æ˜¾ç¤ºç‚¹å’Œçº¿ï¼Œä¾¿äºè§‚å¯Ÿè¶‹åŠ¿
            name=f'ç›¸å¯¹å»¶æ—¶æ—¶é—´åºåˆ— (å¹³å‡å»¶æ—¶: {mean_delay:.2f}ms)',
            marker=dict(
                size=6,
                color='#2196F3',  # è“è‰²
                line=dict(width=0.5, color='#1976D2')
            ),
            line=dict(color='#2196F3', width=1.5),
            hovertemplate='<b>å½•åˆ¶æ—¶é—´</b>: %{x:.2f}ms<br>' +
                         '<b>ç›¸å¯¹å»¶æ—¶</b>: %{y:.2f}ms<br>' +
                         '<b>åŸå§‹å»¶æ—¶</b>: %{customdata[3]:.2f}ms<br>' +
                         '<b>å¹³å‡å»¶æ—¶</b>: %{customdata[4]:.2f}ms<br>' +
                         '<b>æŒ‰é”®ID</b>: %{customdata[0]}<br>' +
                         '<extra></extra>',
            customdata=customdata_list
        ))

        # é…ç½®Yè½´
        y_axis_min, y_axis_max, dtick = self._configure_delay_plot_axes(relative_delays_ms, is_relative=True)

        # é…ç½®åç§»åå›¾è¡¨çš„å¸ƒå±€
        relative_delay_fig.update_layout(
            xaxis_title='å½•åˆ¶æ—¶é—´ (ms)',
            yaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
            showlegend=True,
            template='plotly_white',
            height=400,
            hovermode='closest'
        )

        # æ ¹æ®æ•°æ®åŠ¨æ€è®¾ç½®Yè½´åˆ»åº¦å’ŒèŒƒå›´
        relative_delay_fig.update_yaxes(
            range=[y_axis_min, y_axis_max],
            dtick=dtick,
            tickformat='.1f'  # æ˜¾ç¤º1ä½å°æ•°
        )

        # ä¸ºç›¸å¯¹å»¶æ—¶å›¾æ·»åŠ å‚è€ƒçº¿
        if relative_delays_ms:
            std_0_1ms = self.get_standard_deviation()  # æ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼‰
            std_delay = std_0_1ms / 10.0  # æ ‡å‡†å·®ï¼ˆmsï¼‰

            # æ·»åŠ é›¶çº¿å‚è€ƒçº¿ï¼ˆç›¸å¯¹å»¶æ—¶çš„å‡å€¼åº”è¯¥ä¸º0ï¼‰
            relative_delay_fig.add_trace(go.Scatter(
                x=[times_ms[0], times_ms[-1]] if times_ms else [0, 1],
                y=[0, 0],
                mode='lines',
                name='å¹³å‡å»¶æ—¶ï¼ˆé›¶çº¿ï¼‰',
                line=dict(dash='dash', color='red', width=2),
                hovertemplate='<b>å¹³å‡å»¶æ—¶ï¼ˆé›¶çº¿ï¼‰</b>: 0.00ms<extra></extra>',
                showlegend=False
            ))

            # æ·»åŠ Â±3Ïƒå‚è€ƒçº¿ï¼ˆç›¸å¯¹å»¶æ—¶çš„Â±3Ïƒï¼Œä»¥0ä¸ºä¸­å¿ƒï¼‰
            if std_delay > 0:
                relative_delay_fig.add_trace(go.Scatter(
                    x=[times_ms[0], times_ms[-1]] if times_ms else [0, 1],
                    y=[3 * std_delay, 3 * std_delay],
                    mode='lines',
                    name='+3Ïƒ',
                    line=dict(dash='dot', color='orange', width=1.5),
                    hovertemplate=f'<b>+3Ïƒ</b>: {3 * std_delay:.2f}ms<extra></extra>',
                    showlegend=False
                ))
                relative_delay_fig.add_trace(go.Scatter(
                    x=[times_ms[0], times_ms[-1]] if times_ms else [0, 1],
                    y=[-3 * std_delay, -3 * std_delay],
                    mode='lines',
                    name='-3Ïƒ',
                    line=dict(dash='dot', color='orange', width=1.5),
                    hovertemplate=f'<b>-3Ïƒ</b>: {-3 * std_delay:.2f}ms<extra></extra>',
                    showlegend=False
                ))

        return relative_delay_fig

    def generate_delay_time_series_plot(self) -> Any:
        """
        ç”Ÿæˆå»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šæ—¶é—´ï¼ˆrecord_keyonï¼Œè½¬æ¢ä¸ºmsï¼‰
        yè½´ï¼šå»¶æ—¶ï¼ˆkeyon_offsetï¼Œè½¬æ¢ä¸ºmsï¼‰
        æ•°æ®æ¥æºï¼šæ‰€æœ‰å·²åŒ¹é…çš„æŒ‰é”®å¯¹ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æ—¶é—´åºåˆ—å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_delay_time_series_plot(
                active_algorithms
            )

        # å•ç®—æ³•æ¨¡å¼
        try:
            # 1. è·å–åŸå§‹æ•°æ®
            offset_data = self._get_delay_time_series_raw_data()
            if offset_data is None:
                return {
                    'raw_delay_plot': self.plot_generator._create_empty_plot("æ— æ³•è·å–æ•°æ®"),
                    'relative_delay_plot': self.plot_generator._create_empty_plot("æ— æ³•è·å–æ•°æ®")
                }

            # 2. å¤„ç†å’Œè¿‡æ»¤æ•°æ®
            data_points = self._process_delay_time_series_data(offset_data)
            if data_points is None:
                return {
                    'raw_delay_plot': self.plot_generator._create_empty_plot("æ— æœ‰æ•ˆæ—¶é—´åºåˆ—æ•°æ®"),
                    'relative_delay_plot': self.plot_generator._create_empty_plot("æ— æœ‰æ•ˆæ—¶é—´åºåˆ—æ•°æ®")
                }

            # 3. è®¡ç®—ç›¸å¯¹å»¶æ—¶
            relative_delays_ms, mean_delay = self._calculate_relative_delays(data_points)

            # 4. å‡†å¤‡å›¾è¡¨æ•°æ®
            times_ms, delays_ms, customdata_list = self._prepare_time_series_plot_data(data_points, mean_delay)

            # 5. åˆ›å»ºåŸå§‹å»¶æ—¶å›¾
            raw_delay_plot = self._create_raw_delay_plot(times_ms, delays_ms, customdata_list)

            # 6. åˆ›å»ºç›¸å¯¹å»¶æ—¶å›¾
            relative_delay_plot = self._create_relative_delay_plot(
                times_ms, relative_delays_ms, customdata_list, mean_delay
            )

            return {
                'raw_delay_plot': raw_delay_plot,
                'relative_delay_plot': relative_delay_plot
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆå»¶æ—¶æ—¶é—´åºåˆ—å›¾å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆå»¶æ—¶æ—¶é—´åºåˆ—å›¾å¤±è´¥: {str(e)}")
    
    def export_delay_histogram_data_to_csv(self, filename: str = None) -> Optional[str]:
        """
        å°†å»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾çš„æ•°æ®å¯¼å‡ºä¸ºCSVæ–‡ä»¶

        Args:
            filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            str: CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¯¼å‡ºå¤±è´¥åˆ™è¿”å›None
        """
        try:
            import csv
            import os
            from datetime import datetime

            # æ£€æŸ¥æ•°æ®
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.error("âŒ æ²¡æœ‰åˆ†æå™¨æ•°æ®ï¼Œæ— æ³•å¯¼å‡º")
                return None

            offset_data = self.analyzer.get_offset_alignment_data()
            if not offset_data:
                logger.error("âŒ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•å¯¼å‡º")
                return None

            # æå–æ•°æ®
            csv_data = []
            for item in offset_data:
                keyon_offset = item.get('keyon_offset', 0.0)
                delay_ms = keyon_offset / 10.0

                # è·å–æ›´å¤šä¿¡æ¯
                record_index = item.get('record_index', -1)
                replay_index = item.get('replay_index', -1)
                record_keyon = item.get('record_keyon', 0)
                replay_keyon = item.get('replay_keyon', 0)

                csv_data.append({
                    'record_index': record_index,
                    'replay_index': replay_index,
                    'record_keyon_raw': record_keyon,
                    'replay_keyon_raw': replay_keyon,
                    'record_keyon_ms': record_keyon / 10.0,
                    'replay_keyon_ms': replay_keyon / 10.0,
                    'keyon_offset_raw': keyon_offset,
                    'delay_ms': delay_ms
                })

            if not csv_data:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•å¯¼å‡º")
                return None

            # ç”Ÿæˆæ–‡ä»¶å
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"delay_histogram_data_{timestamp}.csv"

            # ç¡®ä¿æ–‡ä»¶åæœ‰.csvæ‰©å±•å
            if not filename.endswith('.csv'):
                filename += '.csv'

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = "exports"
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            filepath = os.path.join(output_dir, filename)

            # å†™å…¥CSV
            fieldnames = ['record_index', 'replay_index', 'record_keyon_raw', 'replay_keyon_raw',
                         'record_keyon_ms', 'replay_keyon_ms', 'keyon_offset_raw', 'delay_ms']

            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(csv_data)

            logger.info(f"âœ… å»¶æ—¶åˆ†å¸ƒæ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
            logger.info(f"ğŸ“Š å…±å¯¼å‡º {len(csv_data)} æ¡è®°å½•")
            return filepath

        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå»¶æ—¶åˆ†å¸ƒæ•°æ®å¤±è´¥: {e}")
            return None

    def generate_delay_histogram_plot(self) -> Any:
        """
        ç”Ÿæˆå»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾ï¼Œå¹¶å åŠ æ­£æ€æ‹Ÿåˆæ›²çº¿ï¼ˆåŸºäºç»å¯¹æ—¶å»¶ï¼‰ã€‚

        æ•°æ®ç­›é€‰ï¼šåªä½¿ç”¨è¯¯å·®â‰¤50msçš„æŒ‰é”®æ•°æ®
        ç»å¯¹æ—¶å»¶ = keyon_offsetï¼ˆç›´æ¥æµ‹é‡å€¼ï¼‰
        - åæ˜ ç®—æ³•çš„å®é™…å»¶æ—¶è¡¨ç°
        - åŒ…å«æ•´ä½“åç§»ä¿¡æ¯
        - å»¶æ—¶æœ‰æ­£æœ‰è´Ÿï¼Œæ­£å€¼è¡¨ç¤ºå»¶è¿Ÿï¼Œè´Ÿå€¼è¡¨ç¤ºæå‰

        xè½´ï¼šç»å¯¹å»¶æ—¶ (ms)ï¼Œyè½´ï¼šæ¦‚ç‡å¯†åº¦ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”ç›´æ–¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_delay_histogram_plot(
                active_algorithms
            )
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        try:
            if not self.analyzer or not self.analyzer.note_matcher:
                return self.plot_generator._create_empty_plot("æ²¡æœ‰åˆ†æå™¨")

            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            if not offset_data:
                return self.plot_generator._create_empty_plot("æ— ç²¾ç¡®åŒ¹é…æ•°æ®ï¼ˆâ‰¤50msï¼‰")

            # æ­¥éª¤2ï¼šæå–ç²¾ç¡®åŒ¹é…çš„ç»å¯¹å»¶æ—¶æ•°æ®ï¼ˆå¸¦ç¬¦å·çš„keyon_offsetï¼‰
            absolute_delays_ms = [item.get('keyon_offset', 0.0) / 10.0 for item in offset_data]
            if not absolute_delays_ms:
                return self.plot_generator._create_empty_plot("æ— æœ‰æ•ˆå»¶æ—¶æ•°æ®")

            # æ­¥éª¤3ï¼šè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼ˆæ¶ˆé™¤æ•´ä½“åç§»ï¼‰
            # ç›¸å¯¹å»¶æ—¶ = åŸå§‹å»¶æ—¶ - å¹³å‡å»¶æ—¶
            n = len(absolute_delays_ms)
            mean_delay_ms = sum(absolute_delays_ms) / n
            delays_ms = [delay - mean_delay_ms for delay in absolute_delays_ms]

            import plotly.graph_objects as go
            import math
            fig = go.Figure()

            # æ·»åŠ ç›´æ–¹å›¾ï¼ˆä½¿ç”¨ç›¸å¯¹å»¶æ—¶ï¼‰
            fig.add_trace(go.Histogram(
                x=delays_ms,
                histnorm='probability density',
                name='å»¶æ—¶åˆ†å¸ƒ',
                marker_color='#2196F3',  # ä½¿ç”¨æ›´é¥±å’Œçš„è“è‰²
                opacity=0.9,  # å¢åŠ ä¸é€æ˜åº¦ï¼Œä½¿é¢œè‰²æ›´æ˜æ˜¾
                marker_line_color='#1976D2',  # æ·»åŠ è¾¹æ¡†é¢œè‰²ï¼Œå¢å¼ºå¯¹æ¯”åº¦
                marker_line_width=1
            ))

            # ========== æ­¥éª¤4ï¼šè®¡ç®—ç»Ÿè®¡é‡ ==========
            # å‡å€¼åç§»ï¼šåŸºäºåŸå§‹å»¶æ—¶ï¼Œåæ˜ ç®—æ³•æ•´ä½“å»¶æ—¶å€¾å‘
            mean_offset = mean_delay_ms

            # æ–¹å·®ï¼šåŸºäºåŸå§‹å»¶æ—¶ï¼Œåæ˜ ç»å¯¹ç¨³å®šæ€§
            if n > 1:
                var_offset = sum((x - mean_delay_ms) ** 2 for x in absolute_delays_ms) / (n - 1)
                std_offset = var_offset ** 0.5
            else:
                var_offset = 0.0
                std_offset = 0.0

            # ç›¸å¯¹å»¶æ—¶çš„ç»Ÿè®¡é‡ï¼ˆå‡å€¼=0ï¼Œç”¨äºæ­£æ€æ‹Ÿåˆï¼‰
            mean_val = 0.0  # ç›¸å¯¹å»¶æ—¶å‡å€¼=0
            if n > 1:
                var_relative = sum((x - 0) ** 2 for x in delays_ms) / (n - 1)
                std_val = var_relative ** 0.5
            else:
                std_val = 0.0

            # ========== æ­¥éª¤2ï¼šç”Ÿæˆæ­£æ€æ‹Ÿåˆæ›²çº¿ ==========
            if std_val > 0:  # åªæœ‰å½“æ ‡å‡†å·®å¤§äº0æ—¶æ‰ç»˜åˆ¶æ›²çº¿ï¼ˆéœ€è¦ç¦»æ•£æ€§ï¼‰
                # ----- 2.1 ç¡®å®šæ›²çº¿ç»˜åˆ¶èŒƒå›´ -----
                # è·å–å®é™…æ•°æ®çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
                min_x = min(delays_ms)  # æ•°æ®æœ€å°å€¼ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼Œè¡¨ç¤ºæå‰ï¼‰
                max_x = max(delays_ms)  # æ•°æ®æœ€å¤§å€¼ï¼ˆå¯èƒ½ä¸ºæ­£ï¼Œè¡¨ç¤ºå»¶è¿Ÿï¼‰
                
                # è®¡ç®—3ÏƒèŒƒå›´ï¼ˆæ­£æ€åˆ†å¸ƒä¸­çº¦99.7%çš„æ•°æ®è½åœ¨[Î¼-3Ïƒ, Î¼+3Ïƒ]èŒƒå›´å†…ï¼‰
                # max(1e-6, ...) é˜²æ­¢æ ‡å‡†å·®æå°å¯¼è‡´èŒƒå›´è¿‡å°æˆ–é™¤é›¶é”™è¯¯
                span = max(1e-6, 3 * std_val)  # 3ÏƒèŒƒå›´çš„ä¸€åŠå®½åº¦
                
                # ç¡®å®šæ›²çº¿èµ·ç‚¹å’Œç»ˆç‚¹ï¼šå–æ•°æ®èŒƒå›´ä¸3ÏƒèŒƒå›´çš„å¹¶é›†
                # è¿™æ ·æ—¢è¦†ç›–å®é™…æ•°æ®ï¼Œåˆå±•ç¤ºç†è®ºåˆ†å¸ƒç‰¹å¾
                x_start = min(mean_val - span, min_x)  # èµ·ç‚¹ï¼šç†è®ºä¸‹ç•Œä¸å®é™…æœ€å°å€¼çš„è¾ƒå°è€…
                x_end = max(mean_val + span, max_x)    # ç»ˆç‚¹ï¼šç†è®ºä¸Šç•Œä¸å®é™…æœ€å¤§å€¼çš„è¾ƒå¤§è€…
                
                # ----- 2.2 ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„xåæ ‡ç‚¹ -----
                # åœ¨[x_start, x_end]èŒƒå›´å†…å‡åŒ€ç”Ÿæˆ200ä¸ªç‚¹ï¼Œç”¨äºç»˜åˆ¶å¹³æ»‘æ›²çº¿
                num_pts = 200  # å›ºå®š200ä¸ªç‚¹ï¼Œè¶³å¤Ÿå¹³æ»‘ä¸”è®¡ç®—é«˜æ•ˆ
                step = (x_end - x_start) / (num_pts - 1) if num_pts > 1 else 1.0  # ç‚¹ä¹‹é—´çš„é—´è·
                xs = [x_start + i * step for i in range(num_pts)]  # ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„xåæ ‡åºåˆ—
                
                # ----- 2.3 è®¡ç®—æ¯ä¸ªxç‚¹çš„æ¦‚ç‡å¯†åº¦å€¼ï¼ˆæ­£æ€åˆ†å¸ƒPDFï¼‰ -----
                # æ­£æ€åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼šf(x) = (1/(Ïƒâˆš(2Ï€))) * exp(-0.5 * ((x-Î¼)/Ïƒ)Â²)
                # å…¶ä¸­ï¼š
                #   - 1/(Ïƒâˆš(2Ï€))ï¼šå½’ä¸€åŒ–å¸¸æ•°ï¼Œç¡®ä¿æ›²çº¿ä¸‹é¢ç§¯ä¸º1
                #   - exp(-0.5 * ((x-Î¼)/Ïƒ)Â²)ï¼šæŒ‡æ•°é¡¹ï¼Œåœ¨å‡å€¼å¤„æœ€å¤§ï¼Œè¿œç¦»å‡å€¼æ—¶å¿«é€Ÿè¡°å‡
                #   - (x-Î¼)/Ïƒï¼šæ ‡å‡†åŒ–åå·®ï¼ˆz-scoreï¼‰ï¼Œè¡¨ç¤ºè·ç¦»å‡å€¼æœ‰å¤šå°‘ä¸ªæ ‡å‡†å·®
                ys = [(1.0 / (std_val * (2 * math.pi) ** 0.5)) * 
                      math.exp(-0.5 * ((x - mean_val) / std_val) ** 2) 
                      for x in xs]
                
                # ----- 2.4 ç»˜åˆ¶æ­£æ€æ‹Ÿåˆæ›²çº¿ -----
                # ä½¿ç”¨Scatterå›¾ç”¨çº¿æ®µè¿æ¥æ‰€æœ‰(x, y)ç‚¹ï¼Œå½¢æˆè¿ç»­å¹³æ»‘çš„æ›²çº¿
                fig.add_trace(go.Scatter(
                    x=xs,  # 200ä¸ªxåæ ‡ï¼ˆå»¶æ—¶å€¼ï¼Œå•ä½ï¼šmsï¼‰
                    y=ys,  # 200ä¸ªå¯¹åº”çš„æ¦‚ç‡å¯†åº¦å€¼
                    mode='lines',  # ç”¨çº¿æ®µè¿æ¥ç‚¹ï¼Œå½¢æˆè¿ç»­æ›²çº¿
                    name=f"æ­£æ€æ‹Ÿåˆ (å‡å€¼åç§»={mean_offset:.2f}ms, æ ‡å‡†å·®={std_offset:.2f}ms)",  # æ˜¾ç¤ºå‡å€¼åç§»å’Œæ ‡å‡†å·®
                    line=dict(color='#e53935', width=2)  # çº¢è‰²æ›²çº¿ï¼Œçº¿å®½2åƒç´ 
                ))

            fig.update_layout(
                # åˆ é™¤titleï¼Œå› ä¸ºUIåŒºåŸŸå·²æœ‰æ ‡é¢˜
                xaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
                yaxis_title='æ¦‚ç‡å¯†åº¦',
                bargap=0.05,
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(size=12),
                height=500,
                clickmode='event+select',  # å¯ç”¨ç‚¹å‡»å’Œé€‰æ‹©äº‹ä»¶
                legend=dict(
                    orientation='h',
                    yanchor='bottom',
                    y=1.05,  # å›¾æ³¨æ›´é ä¸Šï¼Œç»™æ ‡é¢˜ç•™å‡ºç©ºé—´
                    xanchor='left',
                    x=0.0,  # ä»æœ€å·¦è¾¹å¼€å§‹
                    bgcolor='rgba(255, 255, 255, 0.9)',
                    bordercolor='gray',
                    borderwidth=1
                ),
                margin=dict(t=100, b=60, l=60, r=60)  # å¢åŠ é¡¶éƒ¨è¾¹è·ï¼Œç»™å›¾æ³¨å’Œæ ‡é¢˜æ›´å¤šç©ºé—´
            )

            return fig
        except Exception as e:
            logger.error(f"ç”Ÿæˆå»¶æ—¶ç›´æ–¹å›¾å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆå»¶æ—¶ç›´æ–¹å›¾å¤±è´¥: {str(e)}")
    
    def get_delay_range_data_points(self, delay_min_ms: float, delay_max_ms: float) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹è¯¦æƒ…ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        
        æ³¨æ„ï¼šç”±äºå›¾è¡¨ç°åœ¨ä½¿ç”¨ç›¸å¯¹æ—¶å»¶ï¼Œè¿™é‡Œçš„èŒƒå›´æ˜¯ç›¸å¯¹æ—¶å»¶çš„èŒƒå›´
        
        Args:
            delay_min_ms: æœ€å°ç›¸å¯¹å»¶æ—¶å€¼ï¼ˆmsï¼‰
            delay_max_ms: æœ€å¤§ç›¸å¯¹å»¶æ—¶å€¼ï¼ˆmsï¼‰
            
        Returns:
            List[Dict[str, Any]]: è¯¥ç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªæ•°æ®ç‚¹åŒ…å«ï¼š
                - åŸå§‹æ—¶å»¶ï¼ˆabsolute_delay_msï¼‰
                - ç›¸å¯¹æ—¶å»¶ï¼ˆrelative_delay_msï¼‰
                - å…¶ä»–å®Œæ•´ä¿¡æ¯
        """
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                # å¤šç®—æ³•æ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„æ•°æ®
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if not active_algorithms:
                    return []
                
                filtered_data = []
                for algorithm in active_algorithms:
                    if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                        continue
                    
                    # ä»ç®—æ³•è·å–ç²¾ç¡®åç§»æ•°æ®ï¼ˆè¯¯å·® â‰¤ 50msï¼‰
                    offset_data = algorithm.analyzer.note_matcher.get_precision_offset_alignment_data()
                    if not offset_data:
                        continue
                    
                    algorithm_name = algorithm.metadata.algorithm_name
                    
                    # è®¡ç®—è¯¥ç®—æ³•çš„å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
                    absolute_delays_ms = [item.get('keyon_offset', 0.0) / 10.0 for item in offset_data]
                    if not absolute_delays_ms:
                        continue
                    
                    mean_delay_ms = sum(absolute_delays_ms) / len(absolute_delays_ms)
                    
                    # ç­›é€‰å‡ºæŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹
                    for item in offset_data:
                        absolute_delay_ms = item.get('keyon_offset', 0.0) / 10.0
                        relative_delay_ms = absolute_delay_ms - mean_delay_ms
                        
                        # ä½¿ç”¨ç›¸å¯¹æ—¶å»¶è¿›è¡Œç­›é€‰ï¼ˆå› ä¸ºå›¾è¡¨æ˜¾ç¤ºçš„æ˜¯ç›¸å¯¹æ—¶å»¶ï¼‰
                        if delay_min_ms <= relative_delay_ms <= delay_max_ms:
                            # æ·»åŠ åŸå§‹æ—¶å»¶ã€ç›¸å¯¹æ—¶å»¶å’Œç®—æ³•åç§°åˆ°æ•°æ®ä¸­
                            item_copy = item.copy()
                            item_copy['absolute_delay_ms'] = absolute_delay_ms  # åŸå§‹æ—¶å»¶
                            item_copy['relative_delay_ms'] = relative_delay_ms  # ç›¸å¯¹æ—¶å»¶
                            item_copy['delay_ms'] = relative_delay_ms  # ä¿æŒå…¼å®¹æ€§ï¼Œä½¿ç”¨ç›¸å¯¹æ—¶å»¶
                            item_copy['algorithm_name'] = algorithm_name
                            filtered_data.append(item_copy)
                
                return filtered_data
            
            # å•ç®—æ³•æ¨¡å¼
            if not self.analyzer or not self.analyzer.note_matcher:
                return []

            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            if not offset_data:
                return []
            
            # è®¡ç®—å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
            absolute_delays_ms = [item.get('keyon_offset', 0.0) / 10.0 for item in offset_data]
            if not absolute_delays_ms:
                return []
            
            mean_delay_ms = sum(absolute_delays_ms) / len(absolute_delays_ms)
            
            # ç­›é€‰å‡ºæŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹
            filtered_data = []
            for item in offset_data:
                absolute_delay_ms = item.get('keyon_offset', 0.0) / 10.0
                relative_delay_ms = absolute_delay_ms - mean_delay_ms
                
                # ä½¿ç”¨ç›¸å¯¹æ—¶å»¶è¿›è¡Œç­›é€‰ï¼ˆå› ä¸ºå›¾è¡¨æ˜¾ç¤ºçš„æ˜¯ç›¸å¯¹æ—¶å»¶ï¼‰
                if delay_min_ms <= relative_delay_ms <= delay_max_ms:
                    # æ·»åŠ åŸå§‹æ—¶å»¶å’Œç›¸å¯¹æ—¶å»¶åˆ°æ•°æ®ä¸­
                    item_copy = item.copy()
                    item_copy['absolute_delay_ms'] = absolute_delay_ms  # åŸå§‹æ—¶å»¶
                    item_copy['relative_delay_ms'] = relative_delay_ms  # ç›¸å¯¹æ—¶å»¶
                    item_copy['delay_ms'] = relative_delay_ms  # ä¿æŒå…¼å®¹æ€§ï¼Œä½¿ç”¨ç›¸å¯¹æ—¶å»¶
                    filtered_data.append(item_copy)
            
            return filtered_data
        except Exception as e:
            logger.error(f"è·å–å»¶æ—¶èŒƒå›´æ•°æ®ç‚¹å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return []

    
    def get_offset_alignment_data(self) -> List[Dict[str, Union[int, float, str]]]:
        """è·å–åç§»å¯¹é½æ•°æ® - è½¬æ¢ä¸ºDataTableæ ¼å¼ï¼ŒåŒ…å«æ— æ•ˆéŸ³ç¬¦åˆ†æï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰"""
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰ç®—æ³•çš„æ•°æ®ï¼Œæ·»åŠ "ç®—æ³•åç§°"åˆ—
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                return [{
                    'algorithm_name': "æ— æ•°æ®",
                    'key_id': "æ— æ•°æ®",
                    'count': 0,
                    'median': "N/A",
                    'mean': "N/A",
                    'std': "N/A",
                    'variance': "N/A",
                    'min': "N/A",
                    'max': "N/A",
                    'range': "N/A",
                    'status': 'no_data'
                }]
            
            # ä½¿ç”¨å­—å…¸æŒ‰æŒ‰é”®IDåˆ†ç»„ï¼Œæ¯ä¸ªæŒ‰é”®IDåŒ…å«å¤šä¸ªç®—æ³•çš„æ•°æ®
            from collections import defaultdict
            
            # ç¬¬ä¸€å±‚ï¼šæŒ‰é”®ID -> ç¬¬äºŒå±‚ï¼šç®—æ³•åç§° -> æ•°æ®
            key_algorithm_data = defaultdict(dict)
            
            for algorithm in active_algorithms:
                algorithm_name = algorithm.metadata.algorithm_name
                
                if not algorithm.analyzer:
                    continue
                
                try:
                    # ä»analyzerè·å–ç²¾ç¡®åç§»æ•°æ®ï¼ˆè¯¯å·® â‰¤ 50msï¼‰
                    offset_data = algorithm.analyzer.note_matcher.get_precision_offset_alignment_data()
                    invalid_offset_data = algorithm.analyzer.get_invalid_notes_offset_analysis()
                    
                    # æŒ‰æŒ‰é”®IDåˆ†ç»„æœ‰æ•ˆåŒ¹é…çš„åç§»æ•°æ®ï¼ˆä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼Œä¿ç•™æ­£è´Ÿå€¼ï¼‰
                    key_groups = defaultdict(list)
                    for item in offset_data:
                        key_id = item.get('key_id', 'N/A')
                        keyon_offset = item.get('keyon_offset', 0)  # ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼ˆæ­£æ•°=å»¶è¿Ÿï¼Œè´Ÿæ•°=æå‰ï¼‰
                        key_groups[key_id].append(keyon_offset)
                    
                    # æŒ‰æŒ‰é”®IDåˆ†ç»„æ— æ•ˆéŸ³ç¬¦æ•°æ®
                    invalid_key_groups = defaultdict(list)
                    for item in invalid_offset_data:
                        key_id = item.get('key_id', 'N/A')
                        invalid_key_groups[key_id].append(item)
                    
                    # å¤„ç†æœ‰æ•ˆåŒ¹é…çš„æŒ‰é”®
                    for key_id, offsets in key_groups.items():
                        if offsets:
                            median_val = np.median(offsets)
                            mean_val = np.mean(offsets)
                            std_val = np.std(offsets)
                            variance_val = np.var(offsets)  # æ–¹å·®ï¼ˆå•ä½ï¼š(0.1ms)Â²ï¼‰
                            min_val = np.min(offsets)
                            max_val = np.max(offsets)
                            range_val = max_val - min_val
                            
                            key_algorithm_data[key_id][algorithm_name] = {
                                'algorithm_name': algorithm_name,
                                'key_id': key_id,
                                'count': len(offsets),
                                'median': f"{median_val/10:.2f}ms",
                                'mean': f"{mean_val/10:.2f}ms",
                                'std': f"{std_val/10:.2f}ms",
                                'variance': f"{variance_val/100:.2f}msÂ²",  # è½¬æ¢ä¸ºmsÂ²
                                'min': f"{min_val/10:.2f}ms",
                                'max': f"{max_val/10:.2f}ms",
                                'range': f"{range_val/10:.2f}ms",
                                'status': 'matched'
                            }
                    
                    # å¤„ç†æ— æ•ˆéŸ³ç¬¦çš„æŒ‰é”®
                    for key_id, invalid_items in invalid_key_groups.items():
                        if key_id not in key_groups:  # åªå¤„ç†æ²¡æœ‰æœ‰æ•ˆåŒ¹é…çš„æŒ‰é”®
                            record_count = sum(1 for item in invalid_items if item.get('data_type') == 'record')
                            replay_count = sum(1 for item in invalid_items if item.get('data_type') == 'replay')
                            
                            if key_id not in key_algorithm_data:
                                key_algorithm_data[key_id] = {}
                            
                            key_algorithm_data[key_id][algorithm_name] = {
                                'algorithm_name': algorithm_name,
                                'key_id': key_id,
                                'count': len(invalid_items),
                                'median': "N/A",
                                'mean': "N/A",
                                'std': "N/A",
                                'variance': "N/A",
                                'min': "N/A",
                                'max': "N/A",
                                'range': "N/A",
                                'status': f'invalid (R:{record_count}, P:{replay_count})'
                            }
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–ç®—æ³• '{algorithm_name}' çš„åç§»å¯¹é½æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # å°†æ•°æ®æŒ‰æŒ‰é”®IDæ’åºï¼Œç„¶åæŒ‰ç®—æ³•åç§°æ’åºï¼ˆç¡®ä¿åŒä¸€æŒ‰é”®IDçš„æ•°æ®åœ¨ä¸€èµ·ï¼‰
            table_data = []
            
            # å¯¹æŒ‰é”®IDè¿›è¡Œæ’åºï¼ˆå¤„ç†æ•°å­—å’Œå­—ç¬¦ä¸²æ··åˆçš„æƒ…å†µï¼‰
            def sort_key_id(key_id):
                """æŒ‰é”®IDæ’åºå‡½æ•°ï¼šæ•°å­—æŒ‰é”®æŒ‰æ•°å€¼æ’åºï¼Œå­—ç¬¦ä¸²æŒ‰é”®æŒ‰å­—ç¬¦ä¸²æ’åº"""
                if isinstance(key_id, (int, float)):
                    return (0, key_id)  # æ•°å­—ç±»å‹ï¼Œä¼˜å…ˆçº§0
                elif isinstance(key_id, str):
                    try:
                        # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                        num_key = int(key_id)
                        return (0, num_key)
                    except (ValueError, TypeError):
                        # æ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼ŒæŒ‰å­—ç¬¦ä¸²æ’åº
                        return (1, str(key_id))
                else:
                    return (2, str(key_id))
            
            # æŒ‰æŒ‰é”®IDæ’åº
            sorted_key_ids = sorted(key_algorithm_data.keys(), key=sort_key_id)
            
            for key_id in sorted_key_ids:
                # å¯¹æ¯ä¸ªæŒ‰é”®IDä¸‹çš„ç®—æ³•æŒ‰åç§°æ’åºï¼ˆç¡®ä¿æ˜¾ç¤ºé¡ºåºä¸€è‡´ï¼‰
                algorithms_for_key = sorted(key_algorithm_data[key_id].keys())
                for algorithm_name in algorithms_for_key:
                    table_data.append(key_algorithm_data[key_id][algorithm_name])
            
            if not table_data:
                return [{
                    'algorithm_name': "æ— æ•°æ®",
                    'key_id': "æ— æ•°æ®",
                    'count': 0,
                    'median': "N/A",
                    'mean': "N/A",
                    'std': "N/A",
                    'variance': "N/A",
                    'min': "N/A",
                    'max': "N/A",
                    'range': "N/A",
                    'status': 'no_data'
                }]
            
            return table_data
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        try:
            # ä»åˆ†æå™¨è·å–ç²¾ç¡®åç§»æ•°æ®ï¼ˆè¯¯å·® â‰¤ 50msï¼‰
            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            invalid_offset_data = self.analyzer.get_invalid_notes_offset_analysis()
            
            # æŒ‰æŒ‰é”®IDåˆ†ç»„å¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            from collections import defaultdict
            
            # æŒ‰æŒ‰é”®IDåˆ†ç»„æœ‰æ•ˆåŒ¹é…çš„åç§»æ•°æ®ï¼ˆä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼Œä¿ç•™æ­£è´Ÿå€¼ï¼‰
            key_groups = defaultdict(list)
            for item in offset_data:
                key_id = item.get('key_id', 'N/A')
                keyon_offset = item.get('keyon_offset', 0)  # ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼ˆæ­£æ•°=å»¶è¿Ÿï¼Œè´Ÿæ•°=æå‰ï¼‰
                key_groups[key_id].append(keyon_offset)
            
            # æŒ‰æŒ‰é”®IDåˆ†ç»„æ— æ•ˆéŸ³ç¬¦æ•°æ®
            invalid_key_groups = defaultdict(list)
            for item in invalid_offset_data:
                key_id = item.get('key_id', 'N/A')
                invalid_key_groups[key_id].append(item)
            
            # è½¬æ¢ä¸ºDataTableæ ¼å¼
            table_data = []
            
            # å¤„ç†æœ‰æ•ˆåŒ¹é…çš„æŒ‰é”®
            for key_id, offsets in key_groups.items():
                if offsets:
                    median_val = np.median(offsets)
                    mean_val = np.mean(offsets)
                    std_val = np.std(offsets)
                    variance_val = np.var(offsets)  # æ–¹å·®ï¼ˆå•ä½ï¼š(0.1ms)Â²ï¼‰
                    min_val = np.min(offsets)
                    max_val = np.max(offsets)
                    range_val = max_val - min_val
                    
                    table_data.append({
                        'key_id': key_id,
                        'count': len(offsets),
                        'median': f"{median_val/10:.2f}ms",
                        'mean': f"{mean_val/10:.2f}ms",
                        'std': f"{std_val/10:.2f}ms",
                        'variance': f"{variance_val/100:.2f}msÂ²",  # è½¬æ¢ä¸ºmsÂ²
                        'min': f"{min_val/10:.2f}ms",
                        'max': f"{max_val/10:.2f}ms",
                        'range': f"{range_val/10:.2f}ms",
                        'status': 'matched'
                    })
            
            # å¤„ç†æ— æ•ˆéŸ³ç¬¦çš„æŒ‰é”®
            for key_id, invalid_items in invalid_key_groups.items():
                if key_id not in key_groups:  # åªå¤„ç†æ²¡æœ‰æœ‰æ•ˆåŒ¹é…çš„æŒ‰é”®
                    record_count = sum(1 for item in invalid_items if item.get('data_type') == 'record')
                    replay_count = sum(1 for item in invalid_items if item.get('data_type') == 'replay')
                    
                    table_data.append({
                        'key_id': key_id,
                        'count': len(invalid_items),
                        'median': "N/A",
                        'mean': "N/A",
                        'std': "N/A",
                        'variance': "N/A",
                        'min': "N/A",
                        'max': "N/A",
                        'range': "N/A",
                        'status': f'invalid (R:{record_count}, P:{replay_count})'
                    })
            
            # ä¸å†æ·»åŠ æ±‡æ€»è¡Œ
            
            if not table_data:
                return [{
                    'key_id': "æ— æ•°æ®",
                    'count': 0,
                    'median': "N/A",
                    'mean': "N/A",
                    'std': "N/A",
                    'variance': "N/A",
                    'min': "N/A",
                    'max': "N/A",
                    'range': "N/A",
                    'status': 'no_data'
                }]
            
            return table_data
            
        except Exception as e:
            logger.error(f"è·å–åç§»å¯¹é½æ•°æ®å¤±è´¥: {e}")
            return [{
                'key_id': "é”™è¯¯",
                'count': 0,
                'median': "N/A",
                'mean': "N/A",
                'std': "N/A",
                'variance': "N/A",
                'min': "N/A",
                'max': "N/A",
                'range': "N/A",
                'status': 'error'
            }]
    
    
    def generate_offset_alignment_plot(self) -> Any:
        """ç”Ÿæˆåç§»å¯¹é½åˆ†ææŸ±çŠ¶å›¾ - é”®ä½ä¸ºæ¨ªåæ ‡ï¼Œä¸­ä½æ•°ã€å‡å€¼ã€æ ‡å‡†å·®ä¸ºçºµåæ ‡ï¼Œåˆ†4ä¸ªå­å›¾æ˜¾ç¤ºï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰"""
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æŸ±çŠ¶å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_offset_alignment_plot(
                active_algorithms
            )
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        try:
            # è·å–åç§»å¯¹é½åˆ†ææ•°æ®
            alignment_data = self.get_offset_alignment_data()

            if not alignment_data:
                logger.warning("âš ï¸ æ²¡æœ‰åç§»å¯¹é½åˆ†ææ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰åç§»å¯¹é½åˆ†ææ•°æ®")

            # ä½¿ç”¨é¢„è®¡ç®—çš„æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆå·²ç­›é€‰â‰¤50msçš„æ•°æ®ï¼‰
            key_stats_data = self.analyzer.note_matcher.get_key_statistics_for_bar_chart()

            if not key_stats_data:
                logger.warning("âš ï¸ æ²¡æœ‰æŒ‰é”®ç»Ÿè®¡æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æŒ‰é”®ç»Ÿè®¡æ•°æ®")

            # ç›´æ¥ä»é¢„è®¡ç®—æ•°æ®æå–ç»Ÿè®¡ä¿¡æ¯
            key_ids = []
            median_values = []
            mean_values = []
            std_values = []
            variance_values = []
            status_list = []

            for item in key_stats_data:
                key_ids.append(item['key_id'])
                median_values.append(item['median'])
                mean_values.append(item['mean'])
                std_values.append(item['std'])
                variance_values.append(item['variance'])
                status_list.append(item['status'])
            
            if not key_ids:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åç§»å¯¹é½æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„åç§»å¯¹é½æ•°æ®")
            
            # ç¡®ä¿key_idsçš„æœ€å°å€¼è‡³å°‘ä¸º1ï¼ˆæŒ‰é”®IDä¸å¯èƒ½ä¸ºè´Ÿæ•°ï¼‰
            min_key_id = max(1, min(key_ids)) if key_ids else 1
            max_key_id = max(key_ids) if key_ids else 90
            
            # åˆ›å»ºPlotlyå›¾è¡¨ - 4ä¸ªå­å›¾åˆ†åˆ«æ˜¾ç¤ºæŸ±çŠ¶å›¾ï¼ˆä¸­ä½æ•°ã€å‡å€¼ã€æ ‡å‡†å·®ã€æ–¹å·®ï¼‰
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
            
            fig = make_subplots(
                rows=4, cols=1,
                subplot_titles=('ä¸­ä½æ•°åç§»', 'å‡å€¼åç§»', 'æ ‡å‡†å·®', 'æ–¹å·®'),
                vertical_spacing=0.05,
                row_heights=[0.25, 0.25, 0.25, 0.25]
            )
            
            # æ ¹æ®çŠ¶æ€è®¾ç½®ä¸åŒçš„é¢œè‰²
            matched_indices = [i for i, status in enumerate(status_list) if status == 'matched']
            unmatched_indices = [i for i, status in enumerate(status_list) if status == 'unmatched']
            
            # æ·»åŠ åŒ¹é…æ•°æ®çš„ä¸­ä½æ•°æŸ±çŠ¶å›¾
            if matched_indices:
                matched_key_ids = [key_ids[i] for i in matched_indices]
                matched_median = [median_values[i] for i in matched_indices]
                matched_mean = [mean_values[i] for i in matched_indices]
                matched_std = [std_values[i] for i in matched_indices]
                matched_variance = [variance_values[i] for i in matched_indices]
                
                fig.add_trace(
                    go.Bar(
                        x=matched_key_ids,
                        y=matched_median,
                        name='åŒ¹é…-ä¸­ä½æ•°',
                        marker_color='#1f77b4',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in matched_median],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>ä¸­ä½æ•°: %{y:.2f}ms<br>çŠ¶æ€: åŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=1, col=1
                )
                
                fig.add_trace(
                    go.Bar(
                        x=matched_key_ids,
                        y=matched_mean,
                        name='åŒ¹é…-å‡å€¼',
                        marker_color='#ff7f0e',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in matched_mean],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>å‡å€¼: %{y:.2f}ms<br>çŠ¶æ€: åŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=2, col=1
                )
                
                # è®¡ç®—æ€»ä½“å‡å€¼ï¼šä¸æ•°æ®æ¦‚è§ˆé¡µé¢çš„"å¹³å‡æ—¶å»¶"ä¸€è‡´
                # ä½¿ç”¨get_mean_absolute_error()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç»å¯¹å€¼çš„keyon_offsetè®¡ç®—å¹³å‡å»¶æ—¶
                overall_mean_0_1ms = self.analyzer.get_mean_absolute_error()  # å•ä½ï¼š0.1ms
                overall_mean = overall_mean_0_1ms / 10.0  # è½¬æ¢ä¸ºms
                
                # æ·»åŠ æ€»ä½“å‡å€¼å‚è€ƒçº¿ï¼ˆçº¢è‰²è™šçº¿ï¼‰- ä¸æ•°æ®æ¦‚è§ˆé¡µé¢çš„"å¹³å‡æ—¶å»¶"ä¸€è‡´
                fig.add_hline(
                    y=overall_mean,
                    line_dash="dash",
                    line_color="red",
                    line_width=2,
                    annotation_text=f"å¹³å‡æ—¶å»¶: {overall_mean:.2f}ms",
                    annotation_position="top right",
                    row=2, col=1
                )
                
                fig.add_trace(
                    go.Bar(
                        x=matched_key_ids,
                        y=matched_std,
                        name='åŒ¹é…-æ ‡å‡†å·®',
                        marker_color='#2ca02c',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in matched_std],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>æ ‡å‡†å·®: %{y:.2f}ms<br>çŠ¶æ€: åŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=3, col=1
                )
                
                # è®¡ç®—æ€»ä½“æ ‡å‡†å·®ï¼šä¸å»¶æ—¶è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡ä¿æŒä¸€è‡´
                # ä½¿ç”¨get_standard_deviation()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç»å¯¹å€¼çš„keyon_offsetè®¡ç®—æ€»ä½“æ ‡å‡†å·®
                # å…¬å¼ï¼šÏƒ = âˆš(ÏƒÂ²) = âˆš((1/n) * Î£(|x_i| - Î¼_abs)Â²)
                overall_std_0_1ms = self.analyzer.get_standard_deviation()  # å•ä½ï¼š0.1ms
                overall_std = overall_std_0_1ms / 10.0  # è½¬æ¢ä¸ºms
                
                # æ·»åŠ æ€»ä½“æ ‡å‡†å·®å‚è€ƒçº¿ï¼ˆçº¢è‰²è™šçº¿ï¼‰- ä¸æŸ±çŠ¶å›¾è®¡ç®—æ–¹å¼ä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨ç»å¯¹å€¼ï¼‰
                fig.add_hline(
                    y=overall_std,
                    line_dash="dash",
                    line_color="red",
                    line_width=2,
                    annotation_text=f"æ€»ä½“æ ‡å‡†å·®: {overall_std:.2f}ms",
                    annotation_position="top right",
                    row=3, col=1
                )
                
                # æ·»åŠ æ–¹å·®æŸ±çŠ¶å›¾
                fig.add_trace(
                    go.Bar(
                        x=matched_key_ids,
                        y=matched_variance,
                        name='åŒ¹é…-æ–¹å·®',
                        marker_color='#9467bd',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in matched_variance],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>æ–¹å·®: %{y:.2f}msÂ²<br>çŠ¶æ€: åŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=4, col=1
                )
                
                # è®¡ç®—æ€»ä½“æ–¹å·®ï¼šä¸å»¶æ—¶è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡ä¿æŒä¸€è‡´
                # ä½¿ç”¨get_variance()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetè®¡ç®—æ€»ä½“æ–¹å·®
                # å…¬å¼ï¼šÏƒÂ² = (1/n) * Î£(x_i - Î¼)Â²ï¼Œå…¶ä¸­ x_i æ˜¯å¸¦ç¬¦å·çš„keyon_offset
                overall_variance_0_1ms_squared = self.analyzer.get_variance()  # å•ä½ï¼š(0.1ms)Â²
                overall_variance = overall_variance_0_1ms_squared / 100.0  # è½¬æ¢ä¸ºmsÂ²
                
                # æ·»åŠ æ€»ä½“æ–¹å·®å‚è€ƒçº¿ï¼ˆçº¢è‰²è™šçº¿ï¼‰
                fig.add_hline(
                    y=overall_variance,
                    line_dash="dash",
                    line_color="red",
                    line_width=2,
                    annotation_text=f"æ€»ä½“æ–¹å·®: {overall_variance:.2f}msÂ²",
                    annotation_position="top right",
                    row=4, col=1
                )
            
            # æ·»åŠ æœªåŒ¹é…æ•°æ®çš„ä¸­ä½æ•°æŸ±çŠ¶å›¾
            if unmatched_indices:
                unmatched_key_ids = [key_ids[i] for i in unmatched_indices]
                unmatched_median = [median_values[i] for i in unmatched_indices]
                unmatched_mean = [mean_values[i] for i in unmatched_indices]
                unmatched_std = [std_values[i] for i in unmatched_indices]
                unmatched_variance = [variance_values[i] for i in unmatched_indices]
                
                fig.add_trace(
                    go.Bar(
                        x=unmatched_key_ids,
                        y=unmatched_median,
                        name='æœªåŒ¹é…-ä¸­ä½æ•°',
                        marker_color='#d62728',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in unmatched_median],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>ä¸­ä½æ•°: %{y:.2f}ms<br>çŠ¶æ€: æœªåŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=1, col=1
                )
                
                fig.add_trace(
                    go.Bar(
                        x=unmatched_key_ids,
                        y=unmatched_mean,
                        name='æœªåŒ¹é…-å‡å€¼',
                        marker_color='#9467bd',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in unmatched_mean],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>å‡å€¼: %{y:.2f}ms<br>çŠ¶æ€: æœªåŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=2, col=1
                )
                
                fig.add_trace(
                    go.Bar(
                        x=unmatched_key_ids,
                        y=unmatched_std,
                        name='æœªåŒ¹é…-æ ‡å‡†å·®',
                        marker_color='#8c564b',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in unmatched_std],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>æ ‡å‡†å·®: %{y:.2f}ms<br>çŠ¶æ€: æœªåŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=3, col=1
                )
                
                # æ·»åŠ æœªåŒ¹é…æ•°æ®çš„æ–¹å·®æŸ±çŠ¶å›¾
                fig.add_trace(
                    go.Bar(
                        x=unmatched_key_ids,
                        y=unmatched_variance,
                        name='æœªåŒ¹é…-æ–¹å·®',
                        marker_color='#bcbd22',
                        opacity=0.8,
                        width=1.0,
                        text=[f'{val:.2f}' for val in unmatched_variance],
                        textposition='outside',
                        textfont=dict(size=10),
                        hovertemplate='é”®ä½: %{x}<br>æ–¹å·®: %{y:.2f}msÂ²<br>çŠ¶æ€: æœªåŒ¹é…<extra></extra>',
                        showlegend=False
                    ),
                    row=4, col=1
                )
            
            # æ›´æ–°å¸ƒå±€ - å¢å¤§å›¾è¡¨åŒºåŸŸ
            fig.update_layout(
                # åˆ é™¤titleï¼Œå› ä¸ºUIåŒºåŸŸå·²æœ‰æ ‡é¢˜
                height=2000,  # è¿›ä¸€æ­¥å¢åŠ é«˜åº¦ï¼Œç¡®ä¿å‚è€ƒçº¿å¯è§
                showlegend=False,
                margin=dict(l=100, r=150, t=180, b=120)  # å¢åŠ é¡¶éƒ¨å’Œå³ä¾§è¾¹è·ï¼Œä¸ºå‚è€ƒçº¿æ ‡æ³¨ç•™ç©ºé—´
            )
            
            # æ›´æ–°åæ ‡è½´ - è®¾ç½®xè½´èŒƒå›´ï¼Œç¡®ä¿ä¸æ˜¾ç¤ºè´Ÿæ•°
            fig.update_xaxes(
                title_text="é”®ä½ID",
                range=[min_key_id - 1, max_key_id + 1],  # è®¾ç½®xè½´èŒƒå›´ï¼Œç¡®ä¿ä¸æ˜¾ç¤ºè´Ÿæ•°
                row=1, col=1
            )
            fig.update_xaxes(
                title_text="é”®ä½ID",
                range=[min_key_id - 1, max_key_id + 1],
                row=2, col=1
            )
            fig.update_xaxes(
                title_text="é”®ä½ID",
                range=[min_key_id - 1, max_key_id + 1],
                row=3, col=1
            )
            fig.update_xaxes(
                title_text="é”®ä½ID",
                range=[min_key_id - 1, max_key_id + 1],
                row=4, col=1
            )
            fig.update_yaxes(title_text="ä¸­ä½æ•°åç§» (ms)", row=1, col=1)
            fig.update_yaxes(title_text="å‡å€¼åç§» (ms)", row=2, col=1)
            fig.update_yaxes(title_text="æ ‡å‡†å·® (ms)", row=3, col=1)
            fig.update_yaxes(title_text="æ–¹å·® (msÂ²)", row=4, col=1)
            
            logger.info(f"âœ… åç§»å¯¹é½åˆ†ææŸ±çŠ¶å›¾ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(key_ids)} ä¸ªé”®ä½ï¼ˆåŒ¹é…: {len(matched_indices)}, æœªåŒ¹é…: {len(unmatched_indices)}ï¼‰")
            return fig
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆåç§»å¯¹é½åˆ†ææŸ±çŠ¶å›¾å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”ŸæˆæŸ±çŠ¶å›¾å¤±è´¥: {str(e)}")
    
    def generate_key_delay_scatter_plot(self, only_common_keys: bool = False, selected_algorithm_names: List[str] = None) -> Any:
        """
        ç”ŸæˆæŒ‰é”®ä¸å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šæŒ‰é”®IDï¼ˆkey_idï¼‰
        yè½´ï¼šå»¶æ—¶ï¼ˆkeyon_offsetï¼Œè½¬æ¢ä¸ºmsï¼‰
        æ•°æ®æ¥æºï¼šæ‰€æœ‰å·²åŒ¹é…çš„æŒ‰é”®å¯¹
        
        Args:
            only_common_keys: æ˜¯å¦åªæ˜¾ç¤ºå…¬å…±æŒ‰é”® (ä»…å¤šç®—æ³•æ¨¡å¼æœ‰æ•ˆ)
            selected_algorithm_names: æŒ‡å®šå‚ä¸å¯¹æ¯”çš„ç®—æ³•åç§°åˆ—è¡¨ (ä»…å¤šç®—æ³•æ¨¡å¼æœ‰æ•ˆ)
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æ•£ç‚¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_key_delay_scatter_plot(
                active_algorithms,
                only_common_keys=only_common_keys,
                selected_algorithm_names=selected_algorithm_names
            )
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        try:
            # ä»åˆ†æå™¨è·å–ç²¾ç¡®æœç´¢é˜¶æ®µçš„åç§»å¯¹é½æ•°æ®ï¼ˆè¯¯å·® â‰¤ 50msï¼‰
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")

            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()
            
            if not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # æå–æŒ‰é”®IDå’Œå»¶æ—¶æ•°æ®ï¼ˆå¸¦ç¬¦å·å€¼ï¼‰
            key_ids = []
            delays_ms = []  # å»¶æ—¶ï¼ˆmså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
            customdata_list = []  # ç”¨äºå­˜å‚¨customdataï¼ŒåŒ…å«record_indexå’Œreplay_index
            
            for item in offset_data:
                key_id = item.get('key_id')
                keyon_offset = item.get('keyon_offset', 0)  # å•ä½ï¼š0.1ms
                record_index = item.get('record_index')
                replay_index = item.get('replay_index')
                
                # è·³è¿‡æ— æ•ˆæ•°æ®
                if key_id is None or key_id == 'N/A':
                    continue
                
                try:
                    key_id_int = int(key_id)
                    # å°†å»¶æ—¶ä»0.1msè½¬æ¢ä¸ºmsï¼ˆä¿ç•™ç¬¦å·ï¼‰
                    delay_ms = keyon_offset / 10.0
                    
                    key_ids.append(key_id_int)
                    delays_ms.append(delay_ms)
                    # æ·»åŠ customdataï¼šåŒ…å«record_indexå’Œreplay_indexï¼Œç”¨äºç‚¹å‡»æ—¶æŸ¥æ‰¾åŒ¹é…å¯¹
                    customdata_list.append([record_index, replay_index, key_id_int, delay_ms])
                except (ValueError, TypeError) as e:
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆæ•°æ®ç‚¹: key_id={key_id}, error={e}")
                    continue

            # å¯¹æ•°æ®æŒ‰ç…§æŒ‰é”®IDæ’åºï¼Œç¡®ä¿æ¨ªè½´æŒ‰é”®IDæœ‰åºé€’å¢
            sorted_indices = sorted(range(len(key_ids)), key=lambda i: key_ids[i])
            key_ids[:] = [key_ids[i] for i in sorted_indices]
            delays_ms[:] = [delays_ms[i] for i in sorted_indices]
            customdata_list[:] = [customdata_list[i] for i in sorted_indices]

            if not key_ids:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
            
            # ç›´æ¥ä½¿ç”¨æ•°æ®æ¦‚è§ˆé¡µé¢çš„æ•°æ®ï¼Œä¸é‡æ–°è®¡ç®—
            # ä½¿ç”¨backendçš„æ–¹æ³•ï¼Œç¡®ä¿ä¸æ•°æ®æ¦‚è§ˆé¡µé¢å®Œå…¨ä¸€è‡´
            me_0_1ms = self.get_mean_error()  # æ€»ä½“å‡å€¼ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
            std_0_1ms = self.get_standard_deviation()  # æ€»ä½“æ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
            
            # è½¬æ¢ä¸ºmså•ä½
            mu = me_0_1ms / 10.0  # æ€»ä½“å‡å€¼ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰
            sigma = std_0_1ms / 10.0  # æ€»ä½“æ ‡å‡†å·®ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰
            
            # è®¡ç®—é˜ˆå€¼
            upper_threshold = mu + 3 * sigma  # ä¸Šé˜ˆå€¼ï¼šÎ¼ + 3Ïƒ
            lower_threshold = mu - 3 * sigma  # ä¸‹é˜ˆå€¼ï¼šÎ¼ - 3Ïƒ
            
            # åˆ›å»ºPlotlyæ•£ç‚¹å›¾
            import plotly.graph_objects as go
            
            fig = go.Figure()
            
            # æ·»åŠ æ•£ç‚¹å›¾æ•°æ®ï¼ˆå¸¦ç¬¦å·çš„å»¶æ—¶ï¼‰
            # ä¸ºè¶…è¿‡é˜ˆå€¼çš„ç‚¹ä½¿ç”¨ä¸åŒé¢œè‰²
            marker_colors = []
            marker_sizes = []
            for delay in delays_ms:
                if delay > upper_threshold or delay < lower_threshold:
                    # è¶…è¿‡é˜ˆå€¼çš„ç‚¹ä½¿ç”¨çº¢è‰²ï¼Œæ›´å¤§å°ºå¯¸
                    marker_colors.append('#d62728')  # çº¢è‰²
                    marker_sizes.append(12)
                else:
                    marker_colors.append('#2e7d32')  # ç»¿è‰²
                    marker_sizes.append(8)
            
            # å°†æŒ‰é”®IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œåªæ˜¾ç¤ºIDæ•°å­—
            key_id_strings = [str(kid) for kid in key_ids]

            fig.add_trace(go.Scatter(
                x=key_id_strings,
                y=delays_ms,
                mode='markers',
                name='åŒ¹é…å¯¹',
                marker=dict(
                    size=marker_sizes,
                    color=marker_colors,
                    opacity=0.6,
                    line=dict(width=1, color='#1b5e20')
                ),
                customdata=customdata_list,  # æ·»åŠ customdataï¼ŒåŒ…å«record_indexå’Œreplay_index
                hovertemplate=f'æŒ‰é”®: %{{customdata[2]}}<br>å»¶æ—¶: %{{y:.2f}}ms<br>å¹³å‡å»¶æ—¶: {mu:.2f}ms<extra></extra>'
            ))
            
            # æ³¨æ„ï¼šå·²åˆ é™¤å¹³å‡å»¶æ—¶çš„è¶‹åŠ¿çº¿
            
            # è·å–æ‰€æœ‰å”¯ä¸€çš„æŒ‰é”®IDï¼Œç”¨äºç¡®å®šé˜ˆå€¼çº¿çš„èŒƒå›´
            unique_key_ids = sorted(set(key_ids))
            key_labels = [str(kid) for kid in unique_key_ids]

            # æ·»åŠ æ€»ä½“å‡å€¼å‚è€ƒçº¿ï¼ˆÎ¼ï¼‰- ä½¿ç”¨Scatteråˆ›å»ºï¼Œæ”¯æŒæ‚¬åœæ˜¾ç¤º
            fig.add_trace(go.Scatter(
                x=key_labels,
                y=[mu] * len(key_labels),
                mode='lines',
                name='Î¼',
                line=dict(
                    color='blue',
                    width=1.5,
                    dash='dot'
                ),
                showlegend=True,
                hovertemplate=f"Î¼ = {mu:.2f}ms<extra></extra>"
            ))

            # æ·»åŠ ä¸Šé˜ˆå€¼çº¿ï¼ˆÎ¼ + 3Ïƒï¼‰- ä½¿ç”¨Scatteråˆ›å»ºï¼Œæ”¯æŒæ‚¬åœæ˜¾ç¤º
            fig.add_trace(go.Scatter(
                x=key_labels,
                y=[upper_threshold] * len(key_labels),
                mode='lines',
                name='Î¼+3Ïƒ',
                line=dict(
                    color='red',
                    width=2,
                    dash='dash'
                ),
                showlegend=True,
                hovertemplate=f"Î¼+3Ïƒ = {upper_threshold:.2f}ms<extra></extra>"
            ))

            # æ·»åŠ ä¸‹é˜ˆå€¼çº¿ï¼ˆÎ¼ - 3Ïƒï¼‰- ä½¿ç”¨Scatteråˆ›å»ºï¼Œæ”¯æŒæ‚¬åœæ˜¾ç¤º
            fig.add_trace(go.Scatter(
                x=key_labels,
                y=[lower_threshold] * len(key_labels),
                mode='lines',
                name='Î¼-3Ïƒ',
                line=dict(
                    color='orange',
                    width=2,
                    dash='dash'
                ),
                showlegend=True,
                hovertemplate=f"Î¼-3Ïƒ = {lower_threshold:.2f}ms<extra></extra>"
            ))
            
            # æ›´æ–°å¸ƒå±€ï¼ˆåˆ é™¤titleï¼Œå› ä¸ºUIåŒºåŸŸå·²æœ‰æ ‡é¢˜ï¼‰
            fig.update_layout(
                xaxis_title='æŒ‰é”®ID',
                yaxis_title='å»¶æ—¶ (ms)',
                xaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1,
                    type='category'  # è®¾ç½®ä¸ºç±»åˆ«è½´ï¼Œå› ä¸ºxè½´ç°åœ¨æ˜¯å­—ç¬¦ä¸²
                ),
                yaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1
                ),
                hovermode='closest',
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(size=12),
                height=500,
                showlegend=True,
                legend=dict(
                    orientation='h',  # æ°´å¹³æ’åˆ—å›¾ä¾‹
                    yanchor='bottom',
                    y=1.02,  # å›¾ä¾‹åœ¨å›¾è¡¨åŒºåŸŸä¸Šæ–¹
                    xanchor='left',
                    x=0.0,  # å›¾ä¾‹åœ¨å·¦ä¾§å¯¹é½
                    bgcolor='rgba(255, 255, 255, 0.9)',
                    bordercolor='gray',
                    borderwidth=1
                ),
                margin=dict(t=90, b=60, l=60, r=60)  # å¢åŠ é¡¶éƒ¨è¾¹è·ï¼Œä¸ºå›¾ä¾‹å’Œæ ‡æ³¨ç•™å‡ºç©ºé—´
            )
            
            logger.info(f"âœ… æŒ‰é”®-å»¶æ—¶æ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(key_ids)} ä¸ªæ•°æ®ç‚¹")
            return fig
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")
    
    def generate_key_delay_zscore_scatter_plot(self) -> Any:
        """
        ç”ŸæˆæŒ‰é”®ä¸å»¶æ—¶Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šæŒ‰é”®IDï¼ˆkey_idï¼‰
        yè½´ï¼šZ-Scoreæ ‡å‡†åŒ–å»¶æ—¶å€¼ï¼Œz = (x_i - Î¼) / Ïƒ
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”Z-Scoreæ•£ç‚¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_key_delay_zscore_scatter_plot(
                active_algorithms
            )
        
        # å‘åå…¼å®¹ï¼šæš‚ä¸æ”¯æŒï¼Œè¿”å›ç©ºå›¾è¡¨
        logger.warning("âš ï¸ Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾ç›®å‰ä»…æ”¯æŒå¤šç®—æ³•æ¨¡å¼")
        return self.plot_generator._create_empty_plot("Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾ç›®å‰ä»…æ”¯æŒå¤šç®—æ³•æ¨¡å¼")
    
    def generate_single_key_delay_comparison_plot(self, key_id: int) -> Any:
        """
        ç”Ÿæˆå•é”®å¤šæ›²å»¶æ—¶å¯¹æ¯”å›¾
        
        Args:
            key_id: æŒ‰é”®ID
            
        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¯¹æ¯”å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_single_key_delay_comparison_plot(
                active_algorithms, key_id
            )
        
        # å•ç®—æ³•æ¨¡å¼ï¼šä¸æ”¯æŒå¯¹æ¯”ï¼Œè¿”å›ç©ºå›¾è¡¨
        return self.plot_generator._create_empty_plot("å•é”®å¤šæ›²å¯¹æ¯”ä»…æ”¯æŒå¤šç®—æ³•æ¨¡å¼")

    def generate_hammer_velocity_delay_scatter_plot(self) -> Any:
        """
        ç”Ÿæˆé”¤é€Ÿä¸å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šé”¤é€Ÿï¼ˆæ’­æ”¾é”¤é€Ÿï¼‰
        yè½´ï¼šå»¶æ—¶ï¼ˆkeyon_offsetï¼Œè½¬æ¢ä¸ºmsï¼‰
        æ•°æ®æ¥æºï¼šæ‰€æœ‰å·²åŒ¹é…çš„æŒ‰é”®å¯¹
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æ•£ç‚¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_hammer_velocity_delay_scatter_plot(
                active_algorithms
            )

    def generate_hammer_velocity_relative_delay_scatter_plot(self) -> Any:
        """
        ç”Ÿæˆé”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šlogâ‚â‚€(é”¤é€Ÿ)ï¼ˆæ’­æ”¾é”¤é€Ÿçš„å¯¹æ•°å€¼ï¼‰
        yè½´ï¼šç›¸å¯¹å»¶æ—¶ï¼ˆkeyon_offset - Î¼ï¼Œè½¬æ¢ä¸ºmsï¼‰
        æ•°æ®æ¥æºï¼šç²¾ç¡®åŒ¹é…å¯¹ï¼ˆâ‰¤50msï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æ•£ç‚¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_hammer_velocity_relative_delay_scatter_plot(
                active_algorithms
            )

        # å•ç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨ç²¾ç¡®åŒ¹é…æ•°æ®
        try:
            # ä»åˆ†æå™¨è·å–ç²¾ç¡®åŒ¹é…æ•°æ®ï¼ˆâ‰¤50msï¼‰
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")

            # è·å–ç²¾ç¡®åç§»å¯¹é½æ•°æ®ï¼ˆåªåŒ…å«ç²¾ç¡®åŒ¹é…å¯¹ â‰¤50msï¼‰
            offset_data = self.analyzer.note_matcher.get_precision_offset_alignment_data()

            if not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰ç²¾ç¡®åŒ¹é…æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰ç²¾ç¡®åŒ¹é…æ•°æ®")
            
            # æå–é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ•°æ®
            hammer_velocities = []  # é”¤é€Ÿï¼ˆæ’­æ”¾éŸ³ç¬¦çš„ç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
            delays_ms = []  # å»¶æ—¶ï¼ˆmså•ä½ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
            scatter_customdata = []  # å­˜å‚¨record_idxå’Œreplay_idxï¼Œç”¨äºç‚¹å‡»äº‹ä»¶è¯†åˆ«

            # ç›´æ¥éå†ç²¾ç¡®åŒ¹é…æ•°æ®
            for item in offset_data:
                record_idx = item.get('record_index')
                replay_idx = item.get('replay_index')
                keyon_offset = item.get('keyon_offset', 0)  # å»¶æ—¶ï¼ˆ0.1mså•ä½ï¼‰

                if record_idx is None or replay_idx is None:
                    continue

                # ä»ç²¾ç¡®åŒ¹é…æ•°æ®ä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡
                record_note = None
                replay_note = None

                # æŸ¥æ‰¾precision_matched_pairsä¸­çš„Noteå¯¹è±¡
                for r_idx, p_idx, r_note, p_note in self.analyzer.note_matcher.precision_matched_pairs:
                    if r_idx == record_idx and p_idx == replay_idx:
                        record_note = r_note
                        replay_note = p_note
                        break

                if not record_note or not replay_note:
                    continue

                # è·å–æ’­æ”¾éŸ³ç¬¦çš„é”¤é€Ÿï¼ˆç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
                if len(replay_note.hammers) > 0 and len(replay_note.hammers.values) > 0:
                    hammer_velocity = replay_note.hammers.values[0]
                else:
                    continue  # è·³è¿‡æ²¡æœ‰é”¤é€Ÿæ•°æ®çš„éŸ³ç¬¦

                # å°†å»¶æ—¶ä»0.1msè½¬æ¢ä¸ºmsï¼ˆå¸¦ç¬¦å·ï¼‰
                delay_ms = keyon_offset / 10.0

                # è·³è¿‡é”¤é€Ÿä¸º0æˆ–è´Ÿæ•°çš„æ•°æ®ç‚¹ï¼ˆå¯¹æ•°æ— æ³•å¤„ç†ï¼‰
                if hammer_velocity <= 0:
                    continue

                hammer_velocities.append(hammer_velocity)
                delays_ms.append(delay_ms)
                scatter_customdata.append([record_idx, replay_idx])
            
            if not hammer_velocities:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
            
            # è®¡ç®—Z-Scoreï¼ˆä¸æŒ‰é”®ä¸å»¶æ—¶Z-Scoreæ•£ç‚¹å›¾ç›¸åŒçš„è®¡ç®—æ–¹å¼ï¼‰
            import math
            me_0_1ms = self.get_mean_error()  # æ€»ä½“å‡å€¼ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
            std_0_1ms = self.get_standard_deviation()  # æ€»ä½“æ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
            
            mu = me_0_1ms / 10.0  # æ€»ä½“å‡å€¼ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰
            sigma = std_0_1ms / 10.0  # æ€»ä½“æ ‡å‡†å·®ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰
            
            # è®¡ç®—Z-Scoreï¼šz = (x_i - Î¼) / Ïƒ
            delays_array = np.array(delays_ms)
            if sigma > 0:
                z_scores = ((delays_array - mu) / sigma).tolist()
            else:
                z_scores = [0.0] * len(delays_ms)
            
            # å°†é”¤é€Ÿè½¬æ¢ä¸ºå¯¹æ•°å½¢å¼ï¼ˆç±»ä¼¼åˆ†è´ï¼‰ï¼šlog10(velocity)
            # ä½¿ç”¨log10è€Œä¸æ˜¯20*log10ï¼Œå› ä¸ºè¿™æ˜¯ç›¸å¯¹å€¼ï¼Œä¸æ˜¯ç»å¯¹åˆ†è´
            log_velocities = [math.log10(v) for v in hammer_velocities]
            
            # åˆ›å»ºPlotlyæ•£ç‚¹å›¾
            import plotly.graph_objects as go
            
            fig = go.Figure()
            
            # æ·»åŠ æ•£ç‚¹å›¾æ•°æ®ï¼ˆxè½´ä½¿ç”¨å¯¹æ•°å½¢å¼çš„é”¤é€Ÿï¼Œyè½´ä½¿ç”¨Z-Scoreå€¼ï¼‰
            # customdataæ ¼å¼: [delay_ms, original_velocity, record_idx, replay_idx]
            # ç¬¬ä¸€ä¸ªå…ƒç´ ç”¨äºhoveræ˜¾ç¤ºå»¶æ—¶ï¼Œç¬¬äºŒä¸ªå…ƒç´ ç”¨äºhoveræ˜¾ç¤ºåŸå§‹é”¤é€Ÿï¼Œåä¸¤ä¸ªç”¨äºç‚¹å‡»äº‹ä»¶è¯†åˆ«
            combined_customdata = [[delay_ms, orig_vel, record_idx, replay_idx]
                                  for delay_ms, orig_vel, (record_idx, replay_idx)
                                  in zip(delays_ms, hammer_velocities, scatter_customdata)]
            
            fig.add_trace(go.Scatter(
                x=log_velocities,
                y=z_scores,
                mode='markers',
                name='åŒ¹é…å¯¹',
                marker=dict(
                    size=8,
                    color='#d32f2f',
                    opacity=0.6,
                    line=dict(width=1, color='#b71c1c')
                ),
                hovertemplate='é”¤é€Ÿ: %{customdata[1]:.0f} (log: %{x:.2f})<br>å»¶æ—¶: %{customdata[0]:.2f}ms<br>Z-Score: %{y:.2f}<extra></extra>',
                customdata=combined_customdata
            ))
            
            # æ·»åŠ Z-Scoreå‚è€ƒçº¿ï¼ˆä¸æŒ‰é”®ä¸å»¶æ—¶Z-Scoreæ•£ç‚¹å›¾ç›¸åŒï¼‰
            if len(log_velocities) > 0:
                # è·å–xè½´èŒƒå›´ï¼ˆå¯¹æ•°å½¢å¼ï¼‰
                x_min = min(log_velocities)
                x_max = max(log_velocities)
                
                # æ·»åŠ Z=0çš„æ°´å¹³è™šçº¿ï¼ˆå‡å€¼çº¿ï¼‰
                fig.add_trace(go.Scatter(
                    x=[x_min, x_max],
                    y=[0, 0],
                    mode='lines',
                    name='Z=0',
                    line=dict(
                        color='#1976d2',
                        width=1.5,
                        dash='dot'
                    ),
                    hovertemplate='Z-Score = 0 (å‡å€¼çº¿)<extra></extra>'
                ))
                
                # æ·»åŠ Z=+3çš„æ°´å¹³è™šçº¿ï¼ˆä¸Šé˜ˆå€¼ï¼‰
                fig.add_trace(go.Scatter(
                    x=[x_min, x_max],
                    y=[3, 3],
                    mode='lines',
                    name='Z=+3',
                    line=dict(
                        color='#1976d2',
                        width=2,
                        dash='dash'
                    ),
                    hovertemplate='Z-Score = +3 (ä¸Šé˜ˆå€¼)<extra></extra>'
                ))
                
                # æ·»åŠ Z=-3çš„æ°´å¹³è™šçº¿ï¼ˆä¸‹é˜ˆå€¼ï¼‰
                fig.add_trace(go.Scatter(
                    x=[x_min, x_max],
                    y=[-3, -3],
                    mode='lines',
                    name='Z=-3',
                    line=dict(
                        color='#1976d2',
                        width=2,
                        dash='dash'
                    ),
                    hovertemplate='Z-Score = -3 (ä¸‹é˜ˆå€¼)<extra></extra>'
                ))
            
            # ç®€å•çš„å¸ƒå±€æ›´æ–°ï¼Œä¸éœ€è¦å¤æ‚çš„åˆ»åº¦è®¡ç®—
            
            fig.update_layout(
                # åˆ é™¤titleï¼Œå› ä¸ºUIåŒºåŸŸå·²æœ‰æ ‡é¢˜
                xaxis_title='é”¤é€Ÿï¼ˆlogâ‚â‚€ï¼‰',
                yaxis_title='Z-Scoreï¼ˆæ ‡å‡†åŒ–å»¶æ—¶ï¼‰',
                xaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1,
                    # ä½¿ç”¨çº¿æ€§åˆ»åº¦ï¼Œè®©Plotlyè‡ªåŠ¨å¤„ç†ï¼Œä½†è®¾ç½®åˆé€‚çš„èŒƒå›´
                    autorange=True,
                    # è®¾ç½®åˆ»åº¦æ ¼å¼
                    tickformat='.1f',  # æ˜¾ç¤º1ä½å°æ•°
                    dtick=0.2  # æ¯0.2ä¸ªå•ä½ä¸€ä¸ªåˆ»åº¦
                ),
                yaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1
                ),
                hovermode='closest',
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(size=12),
                height=500,
                showlegend=True,
                legend=dict(
                    orientation='h',  # æ°´å¹³æ’åˆ—å›¾ä¾‹
                    yanchor='bottom',
                    y=1.02,  # å›¾ä¾‹åœ¨å›¾è¡¨åŒºåŸŸä¸Šæ–¹
                    xanchor='left',
                    x=0.0,  # å›¾ä¾‹åœ¨å·¦ä¾§å¯¹é½
                    bgcolor='rgba(255, 255, 255, 0.9)',
                    bordercolor='gray',
                    borderwidth=1
                ),
                margin=dict(t=70, b=60, l=60, r=60)  # å¢åŠ é¡¶éƒ¨è¾¹è·ï¼Œä¸ºå›¾ä¾‹ç•™å‡ºç©ºé—´
            )
            
            logger.info(f"âœ… é”¤é€Ÿ-å»¶æ—¶æ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(hammer_velocities)} ä¸ªæ•°æ®ç‚¹")
            return fig
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")
    
    def generate_key_hammer_velocity_scatter_plot(self) -> Any:
        """
        ç”ŸæˆæŒ‰é”®ä¸é”¤é€Ÿçš„æ•£ç‚¹å›¾ï¼Œé¢œè‰²è¡¨ç¤ºå»¶æ—¶ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        xè½´ï¼šæŒ‰é”®IDï¼ˆkey_idï¼‰
        yè½´ï¼šé”¤é€Ÿï¼ˆæ’­æ”¾é”¤é€Ÿï¼‰
        é¢œè‰²ï¼šå»¶æ—¶ï¼ˆkeyon_offsetï¼Œè½¬æ¢ä¸ºmsï¼Œä½¿ç”¨é¢œè‰²æ˜ å°„ï¼‰
        æ•°æ®æ¥æºï¼šæ‰€æœ‰å·²åŒ¹é…çš„æŒ‰é”®å¯¹
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šç”Ÿæˆå¤šç®—æ³•å¯¹æ¯”æ•£ç‚¹å›¾
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_multi_algorithm_key_hammer_velocity_scatter_plot(
                active_algorithms
            )
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        try:
            # ä»åˆ†æå™¨è·å–åŸå§‹åç§»å¯¹é½æ•°æ®å’ŒåŒ¹é…å¯¹
            if not self.analyzer or not self.analyzer.note_matcher:
                logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")
            
            matched_pairs = self.analyzer.note_matcher.get_matched_pairs()
            
            if not matched_pairs:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆæ•£ç‚¹å›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰åŒ¹é…æ•°æ®")
            
            # è·å–åç§»å¯¹é½æ•°æ®ï¼ˆå·²åŒ…å«å»¶æ—¶ä¿¡æ¯ï¼‰
            offset_data = self.analyzer.note_matcher.get_offset_alignment_data()
            
            # æå–æŒ‰é”®IDã€é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®
            key_ids = []  # æŒ‰é”®ID
            hammer_velocities = []  # é”¤é€Ÿï¼ˆæ’­æ”¾éŸ³ç¬¦çš„ç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
            delays_ms = []  # å»¶æ—¶ï¼ˆmså•ä½ï¼Œç”¨äºé¢œè‰²æ˜ å°„ï¼‰
            
            # åˆ›å»ºåŒ¹é…å¯¹ç´¢å¼•åˆ°åç§»æ•°æ®çš„æ˜ å°„
            offset_map = {}
            for item in offset_data:
                record_idx = item.get('record_index')
                replay_idx = item.get('replay_index')
                if record_idx is not None and replay_idx is not None:
                    offset_map[(record_idx, replay_idx)] = item
            
            for record_idx, replay_idx, record_note, replay_note in matched_pairs:
                # è·å–æŒ‰é”®ID
                key_id = record_note.id
                
                # è·å–æ’­æ”¾éŸ³ç¬¦çš„é”¤é€Ÿï¼ˆç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
                if len(replay_note.hammers) > 0 and len(replay_note.hammers.values) > 0:
                    hammer_velocity = replay_note.hammers.values[0]
                else:
                    continue  # è·³è¿‡æ²¡æœ‰é”¤é€Ÿæ•°æ®çš„éŸ³ç¬¦
                
                # ä»åç§»æ•°æ®ä¸­è·å–å»¶æ—¶ï¼ˆkeyon_offsetï¼‰ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è®¡ç®—
                keyon_offset = None
                if (record_idx, replay_idx) in offset_map:
                    keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è®¡ç®—ï¼ˆä½¿ç”¨ç§æœ‰æ–¹æ³•ï¼‰
                    try:
                        record_keyon, _ = self.analyzer.note_matcher._calculate_note_times(record_note)
                        replay_keyon, _ = self.analyzer.note_matcher._calculate_note_times(replay_note)
                        keyon_offset = replay_keyon - record_keyon
                    except:
                        continue  # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¯¥æ•°æ®ç‚¹
                
                # å°†å»¶æ—¶ä»0.1msè½¬æ¢ä¸ºmsï¼Œä½¿ç”¨ç»å¯¹å€¼ï¼ˆç”¨äºé¢œè‰²æ˜ å°„ï¼‰
                delay_ms = abs(keyon_offset) / 10.0
                
                try:
                    key_id_int = int(key_id)
                    key_ids.append(key_id_int)
                    hammer_velocities.append(hammer_velocity)
                    delays_ms.append(delay_ms)
                except (ValueError, TypeError):
                    continue  # è·³è¿‡æ— æ•ˆçš„æŒ‰é”®ID
            
            if not key_ids:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
            
            # åˆ›å»ºPlotlyæ•£ç‚¹å›¾ï¼Œä½¿ç”¨é¢œè‰²æ˜ å°„è¡¨ç¤ºå»¶æ—¶
            import plotly.graph_objects as go
            
            # è®¡ç®—å»¶æ—¶çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œç”¨äºé¢œè‰²æ˜ å°„
            min_delay = min(delays_ms)
            max_delay = max(delays_ms)
            
            fig = go.Figure()
            
            # æ·»åŠ æ•£ç‚¹å›¾æ•°æ®ï¼Œé¢œè‰²æ˜ å°„åˆ°å»¶æ—¶å€¼
            fig.add_trace(go.Scatter(
                x=key_ids,
                y=hammer_velocities,
                mode='markers',
                name='åŒ¹é…å¯¹',
                marker=dict(
                    size=8,
                    color=delays_ms,  # é¢œè‰²æ˜ å°„åˆ°å»¶æ—¶å€¼
                    colorscale='Viridis',  # ä½¿ç”¨Viridisé¢œè‰²æ–¹æ¡ˆï¼ˆä»è“è‰²åˆ°é»„è‰²ï¼‰
                    colorbar=dict(
                        title='å»¶æ—¶ (ms)',
                        thickness=20,
                        len=0.6,
                        x=1.02
                    ),
                    cmin=min_delay,
                    cmax=max_delay,
                    opacity=0.7,
                    line=dict(width=0.5, color='rgba(0,0,0,0.3)'),
                    showscale=True  # æ˜¾ç¤ºé¢œè‰²æ¡
                ),
                hovertemplate='é”®ä½: %{x}<br>é”¤é€Ÿ: %{y}<br>å»¶æ—¶: %{marker.color:.2f}ms<extra></extra>'
            ))
            
            # æ›´æ–°å¸ƒå±€
            fig.update_layout(
                # åˆ é™¤titleï¼Œå› ä¸ºUIåŒºåŸŸå·²æœ‰æ ‡é¢˜
                xaxis_title='æŒ‰é”®ID',
                yaxis_title='é”¤é€Ÿ',
                xaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1,
                    dtick=10  # æ¯10ä¸ªæŒ‰é”®æ˜¾ç¤ºä¸€ä¸ªåˆ»åº¦
                ),
                yaxis=dict(
                    showgrid=True,
                    gridcolor='lightgray',
                    gridwidth=1
                ),
                hovermode='closest',
                plot_bgcolor='white',
                paper_bgcolor='white',
                font=dict(size=12),
                height=500,
                showlegend=False,  # ä¸éœ€è¦å›¾ä¾‹ï¼Œå› ä¸ºæœ‰é¢œè‰²æ¡
                margin=dict(t=60, b=60, l=60, r=100)  # å³ä¾§è¾¹è·å¢åŠ ï¼Œä¸ºé¢œè‰²æ¡ç•™å‡ºç©ºé—´
            )
            
            logger.info(f"âœ… æŒ‰é”®-é”¤é€Ÿæ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(key_ids)} ä¸ªæ•°æ®ç‚¹")
            return fig
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    # ==================== ç»˜å›¾ç›¸å…³æ–¹æ³• ====================
    
    def generate_waterfall_plot(self) -> Any:
        """ç”Ÿæˆç€‘å¸ƒå›¾ï¼ˆç»Ÿä¸€å•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰"""

        # ç¡®å®šæ•°æ®æº
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨å¤šä¸ªç®—æ³•çš„æ•°æ®
            analyzers = []
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œè¿”å›ç©ºå›¾è¡¨")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

            analyzers = [alg.analyzer for alg in active_algorithms if alg.analyzer]
            algorithm_names = [alg.metadata.algorithm_name for alg in active_algorithms]
            is_multi_algorithm = True
        else:
            # å•ç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨å•ä¸ªåˆ†æå™¨
            if not self.analyzer:
                logger.error("åˆ†æå™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆç€‘å¸ƒå›¾")
                return self.plot_generator._create_empty_plot("åˆ†æå™¨ä¸å­˜åœ¨")

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            has_valid_data = (hasattr(self.analyzer, 'valid_record_data') and self.analyzer.valid_record_data and
                             hasattr(self.analyzer, 'valid_replay_data') and self.analyzer.valid_replay_data)

            if not has_valid_data:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ææ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç€‘å¸ƒå›¾")
                return self.plot_generator._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ææ•°æ®")

            # ç¡®ä¿æ•°æ®å·²åŒæ­¥åˆ°PlotGenerator
            if not self.plot_generator.valid_record_data or not self.plot_generator.valid_replay_data:
                logger.info("ğŸ”„ åŒæ­¥æ•°æ®åˆ°PlotGenerator")
                self._sync_analysis_results()

            analyzers = [self.analyzer]
            algorithm_names = ["single"]
            is_multi_algorithm = False

        # ä½¿ç”¨ç»Ÿä¸€çš„ç€‘å¸ƒå›¾ç”Ÿæˆå™¨
        return self.multi_algorithm_plot_generator.generate_unified_waterfall_plot(
            self,                # åç«¯å®ä¾‹
            analyzers,           # åˆ†æå™¨åˆ—è¡¨
            algorithm_names,     # ç®—æ³•åç§°åˆ—è¡¨
            is_multi_algorithm,  # æ˜¯å¦å¤šç®—æ³•æ¨¡å¼
            self.time_filter,    # æ—¶é—´è¿‡æ»¤å™¨
            self.data_filter.key_filter if self.data_filter else None  # æŒ‰é”®è¿‡æ»¤å™¨
        )
    
    def generate_watefall_conbine_plot(self, key_on: float, key_off: float, key_id: int) -> Tuple[Any, Any, Any]:
        """ç”Ÿæˆç€‘å¸ƒå›¾å¯¹æ¯”å›¾"""
        return self.plot_generator.generate_watefall_conbine_plot(key_on, key_off, key_id)
    
    def generate_watefall_conbine_plot_by_index(self, index: int, is_record: bool) -> Tuple[Any, Any, Any]:
        """æ ¹æ®ç´¢å¼•ç”Ÿæˆç€‘å¸ƒå›¾å¯¹æ¯”å›¾"""
        return self.plot_generator.generate_watefall_conbine_plot_by_index(index, is_record)
    
    def get_notes_by_delay_type(self, algorithm_name: str, delay_type: str) -> Optional[Tuple[Any, Any, int, int]]:
        """
        æ ¹æ®å»¶è¿Ÿç±»å‹ï¼ˆæœ€å¤§/æœ€å°ï¼‰è·å–å¯¹åº”çš„éŸ³ç¬¦
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            delay_type: å»¶è¿Ÿç±»å‹ï¼Œ'max' æˆ– 'min'
        
        Returns:
            Tuple[Note, Note]: (record_note, replay_note)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
                logger.warning("âš ï¸ å¤šç®—æ³•æ¨¡å¼æœªå¯ç”¨")
                return None
            
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            algorithm = None
            for alg in active_algorithms:
                if alg.metadata.algorithm_name == algorithm_name:
                    algorithm = alg
                    break
            
            if not algorithm:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç®—æ³•: {algorithm_name}")
                return None
            
            if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                logger.warning(f"âš ï¸ ç®—æ³• '{algorithm_name}' çš„åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")
                return None
            
            note_matcher = algorithm.analyzer.note_matcher
            
            # è·å–åç§»æ•°æ®
            offset_data = note_matcher.get_offset_alignment_data()
            if not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®")
                return None
            
            # æ‰¾åˆ°æœ€å¤§æˆ–æœ€å°å»¶è¿Ÿå¯¹åº”çš„æ•°æ®é¡¹
            target_item = None
            if delay_type == 'max':
                # æ‰¾åˆ°æ‰€æœ‰å…·æœ‰æœ€å¤§å»¶è¿Ÿçš„æ•°æ®é¡¹
                max_delay = max(item.get('keyon_offset', 0) for item in offset_data)
                max_delay_items = [item for item in offset_data if item.get('keyon_offset', 0) == max_delay]
                if max_delay_items:
                    # å¦‚æœæœ‰å¤šä¸ªï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–ç­–ç•¥ï¼Œæ¯”å¦‚æŒ‰æ—¶é—´æ’åºï¼‰
                    target_item = max_delay_items[0]
                    logger.info(f"ğŸ” æœ€å¤§å»¶è¿Ÿ: {max_delay/10.0:.2f}ms, æ‰¾åˆ°{len(max_delay_items)}ä¸ªåŒ¹é…é¡¹, é€‰æ‹©record_index={target_item.get('record_index')}, replay_index={target_item.get('replay_index')}")
                    if len(max_delay_items) > 1:
                        logger.warning(f"âš ï¸ æ‰¾åˆ°{len(max_delay_items)}ä¸ªå…·æœ‰æœ€å¤§å»¶è¿Ÿ({max_delay/10.0:.2f}ms)çš„æ•°æ®é¡¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª")
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ€å¤§å»¶è¿Ÿå¯¹åº”çš„æ•°æ®é¡¹")
            elif delay_type == 'min':
                # æ‰¾åˆ°æ‰€æœ‰å…·æœ‰æœ€å°å»¶è¿Ÿçš„æ•°æ®é¡¹
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼Œä¸UIæ˜¾ç¤ºä¿æŒä¸€è‡´
                all_delays = [item.get('keyon_offset', 0) for item in offset_data]
                min_delay = min(all_delays)
                min_delay_ms = min_delay / 10.0
                
                # ä½¿ç”¨ä¸UIç›¸åŒçš„é€»è¾‘ï¼šéå†æ‰€æœ‰æ•°æ®é¡¹ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªåŒ¹é…çš„ï¼ˆä¸UIä¿æŒä¸€è‡´ï¼‰
                # UIä¸­çš„é€»è¾‘ï¼šif min_delay_item is None or item_delay_ms == min_delay_ms: min_delay_item = item
                # è¿™æ„å‘³ç€å¦‚æœæœ‰å¤šä¸ªåŒ¹é…é¡¹ï¼ŒUIä¼šä¿å­˜æœ€åä¸€ä¸ª
                target_item = None
                for item in offset_data:
                    item_delay_ms = item.get('keyon_offset', 0) / 10.0
                    if target_item is None or item_delay_ms == min_delay_ms:
                        target_item = item
                
                # éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„æ‰¾åˆ°äº†æœ€å°å€¼
                logger.info(f"ğŸ” æ‰€æœ‰å»¶è¿Ÿå€¼èŒƒå›´: [{min(all_delays)/10.0:.2f}ms, {max(all_delays)/10.0:.2f}ms]")
                logger.info(f"ğŸ” æœ€å°å»¶è¿Ÿå€¼: {min_delay_ms:.2f}ms")
                
                if target_item:
                    # ç»Ÿè®¡æœ‰å¤šå°‘ä¸ªåŒ¹é…é¡¹
                    min_delay_items = [item for item in offset_data if abs(item.get('keyon_offset', 0) / 10.0 - min_delay_ms) < 0.001]
                    logger.info(f"ğŸ” æ‰¾åˆ°{len(min_delay_items)}ä¸ªå…·æœ‰æœ€å°å»¶è¿Ÿ({min_delay_ms:.2f}ms)çš„æ•°æ®é¡¹")
                    logger.info(f"ğŸ” é€‰æ‹©çš„æ•°æ®é¡¹ï¼ˆä¸UIé€»è¾‘ä¸€è‡´ï¼Œæœ€åä¸€ä¸ªåŒ¹é…é¡¹ï¼‰: record_index={target_item.get('record_index')}, replay_index={target_item.get('replay_index')}, keyon_offset={target_item.get('keyon_offset', 0)/10.0:.2f}ms")
                    if len(min_delay_items) > 1:
                        logger.warning(f"âš ï¸ æ‰¾åˆ°{len(min_delay_items)}ä¸ªå…·æœ‰æœ€å°å»¶è¿Ÿ({min_delay_ms:.2f}ms)çš„æ•°æ®é¡¹ï¼Œé€‰æ‹©æœ€åä¸€ä¸ªï¼ˆä¸UIé€»è¾‘ä¸€è‡´ï¼‰")
                        # åˆ—å‡ºæ‰€æœ‰åŒ¹é…é¡¹çš„ä¿¡æ¯
                        for idx, item in enumerate(min_delay_items):
                            logger.info(f"  åŒ¹é…é¡¹{idx+1}: record_index={item.get('record_index')}, replay_index={item.get('replay_index')}")
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ€å°å»¶è¿Ÿå¯¹åº”çš„æ•°æ®é¡¹")
            else:
                logger.warning(f"âš ï¸ æ— æ•ˆçš„å»¶è¿Ÿç±»å‹: {delay_type}")
                return None
            
            if not target_item:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{delay_type}å»¶è¿Ÿå¯¹åº”çš„æ•°æ®é¡¹")
                return None
            
            record_index = target_item.get('record_index')
            replay_index = target_item.get('replay_index')
            
            if record_index is None or replay_index is None:
                logger.warning(f"âš ï¸ æ•°æ®é¡¹ç¼ºå°‘ç´¢å¼•ä¿¡æ¯")
                return None
            
            # ä»matched_pairsä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡
            matched_pairs = note_matcher.get_matched_pairs()
            record_note = None
            replay_note = None
            
            for r_idx, p_idx, r_note, p_note in matched_pairs:
                if r_idx == record_index and p_idx == replay_index:
                    record_note = r_note
                    replay_note = p_note
                    break
            
            if record_note is None or replay_note is None:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å¯¹: record_index={record_index}, replay_index={replay_index}")
                return None
            
            delay_ms = target_item.get('keyon_offset', 0) / 10.0
            # ä»éŸ³ç¬¦å¯¹è±¡è·å–æ­£ç¡®çš„æŒ‰é”®IDï¼Œè€Œä¸æ˜¯ä»offset_dataä¸­è·å–
            key_id = getattr(record_note, 'id', 'N/A') if record_note else 'N/A'
            delay_type_name = "æœ€å¤§" if delay_type == 'max' else "æœ€å°"
            logger.info(f"âœ… æ‰¾åˆ°{delay_type_name}å»¶è¿Ÿå¯¹åº”çš„éŸ³ç¬¦: ç®—æ³•={algorithm_name}, æŒ‰é”®ID={key_id}, record_index={record_index}, replay_index={replay_index}, delay={delay_ms:.2f}ms")
            return (record_note, replay_note, record_index, replay_index)
            
        except Exception as e:
            logger.error(f"âŒ è·å–{delay_type}å»¶è¿Ÿå¯¹åº”çš„éŸ³ç¬¦å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return None
    
    def get_first_data_point_notes(self) -> Optional[Tuple[Any, Any]]:
        """
        è·å–å»¶æ—¶æ—¶é—´åºåˆ—å›¾çš„ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„éŸ³ç¬¦
        
        æ”¯æŒå•ç®—æ³•æ¨¡å¼å’Œå¤šç®—æ³•æ¨¡å¼ï¼š
        - å•ç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨ self.analyzer
        - å¤šç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿€æ´»çš„ç®—æ³•
        
        Returns:
            Tuple[Note, Note]: (record_note, replay_note)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # å¤šç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿€æ´»çš„ç®—æ³•
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if not active_algorithms:
                    logger.debug("â„¹ï¸ å¤šç®—æ³•æ¨¡å¼ä¸‹æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
                    return None
                
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿€æ´»çš„ç®—æ³•
                algorithm = active_algorithms[0]
                if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                    logger.warning(f"âš ï¸ ç®—æ³• '{algorithm.metadata.algorithm_name}' çš„åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")
                    return None
                
                analyzer = algorithm.analyzer
                note_matcher = analyzer.note_matcher
                
            else:
                # å•ç®—æ³•æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                if not self.analyzer or not self.analyzer.note_matcher:
                    logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨")
                    return None
                
                analyzer = self.analyzer
                note_matcher = analyzer.note_matcher
            
            # è·å–åç§»æ•°æ®
            offset_data = note_matcher.get_offset_alignment_data()
            if not offset_data:
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…æ•°æ®")
                return None
            
            # æå–æ•°æ®ç‚¹å¹¶æ’åº
            data_points = []
            for item in offset_data:
                record_keyon = item.get('record_keyon', 0)
                record_index = item.get('record_index')
                replay_index = item.get('replay_index')
                
                if record_keyon is None or record_index is None or replay_index is None:
                    continue
                
                data_points.append({
                    'time': record_keyon / 10.0,  # è½¬æ¢ä¸ºms
                    'record_index': record_index,
                    'replay_index': replay_index
                })
            
            if not data_points:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ç‚¹")
                return None
            
            # æŒ‰æ—¶é—´æ’åºï¼Œè·å–ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹
            data_points.sort(key=lambda x: x['time'])
            first_point = data_points[0]
            
            record_index = first_point['record_index']
            replay_index = first_point['replay_index']
            
            # ä»matched_pairsä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡
            matched_pairs = note_matcher.get_matched_pairs()
            record_note = None
            replay_note = None
            
            for r_idx, p_idx, r_note, p_note in matched_pairs:
                if r_idx == record_index and p_idx == replay_index:
                    record_note = r_note
                    replay_note = p_note
                    break
            
            if record_note is None or replay_note is None:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å¯¹: record_index={record_index}, replay_index={replay_index}")
                return None
            
            logger.info(f"âœ… æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹: record_index={record_index}, replay_index={replay_index}")
            return (record_note, replay_note)
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return None
    
    def test_curve_alignment(self) -> Optional[Dict[str, Any]]:
        """
        æµ‹è¯•æ›²çº¿å¯¹é½åŠŸèƒ½ï¼Œä½¿ç”¨å»¶æ—¶æ—¶é—´åºåˆ—å›¾çš„ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹
        
        Returns:
            Dict[str, Any]: æµ‹è¯•ç»“æœï¼ŒåŒ…å«å¯¹é½å‰åçš„å¯¹æ¯”å›¾å’Œç›¸ä¼¼åº¦
        """
        try:
            from backend.force_curve_analyzer import ForceCurveAnalyzer
            
            # è·å–ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹çš„éŸ³ç¬¦
            notes = self.get_first_data_point_notes()
            if notes is None:
                return {
                    'status': 'error',
                    'message': 'æ— æ³•è·å–ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹çš„éŸ³ç¬¦'
                }
            
            record_note, replay_note = notes
            
            # è®¡ç®—å¹³å‡å»¶æ—¶
            mean_delay = 0.0
            
            # å¤šç®—æ³•æ¨¡å¼ä¸‹ï¼Œéœ€è¦ä»å¯¹åº”çš„ç®—æ³•è·å–å¹³å‡å»¶æ—¶
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                # è·å–ç¬¬ä¸€ä¸ªæ¿€æ´»çš„ç®—æ³•ï¼ˆä¸get_first_data_point_notesé€»è¾‘ä¸€è‡´ï¼‰
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if active_algorithms:
                    algorithm = active_algorithms[0]
                    if algorithm.analyzer:
                         mean_error_0_1ms = algorithm.analyzer.get_mean_error()
                         if mean_error_0_1ms is not None:
                             mean_delay = mean_error_0_1ms / 10.0
            # å•ç®—æ³•æ¨¡å¼ï¼ˆå…¼å®¹ï¼‰
            elif self.analyzer:
                mean_error_0_1ms = self.analyzer.get_mean_error()
                if mean_error_0_1ms is not None:
                    mean_delay = mean_error_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’
            
            logger.info(f"æµ‹è¯•æ›²çº¿å¯¹é½ - è·å–å¹³å‡å»¶æ—¶: {mean_delay}ms")
            
            # åˆ›å»ºæ›²çº¿åˆ†æå™¨
            # å‡å°‘å¹³æ»‘å¼ºåº¦ï¼Œä¿æŒæ³¢å³°å’Œæ³¢è°·ç‰¹å¾
            analyzer = ForceCurveAnalyzer(
                smooth_sigma=0.3,  # å¤§å¹…å‡å°å¹³æ»‘å¼ºåº¦ï¼Œä»1.0æ”¹ä¸º0.3ï¼Œæ›´å¥½åœ°ä¿æŒæ³¢å³°å’Œæ³¢è°·
                dtw_distance_metric='manhattan',
                dtw_window_size_ratio=0.3  # é€‚ä¸­çš„çª—å£å¤§å°ï¼Œå¹³è¡¡å¯¹é½æ•ˆæœå’Œå½¢çŠ¶ä¿æŒ
            )
            
            # å¯¹æ¯”æ›²çº¿ï¼ˆæ’­æ”¾æ›²çº¿å¯¹é½åˆ°å½•åˆ¶æ›²çº¿ï¼‰
            result = analyzer.compare_curves(
                record_note, 
                replay_note,
                record_note=record_note,
                replay_note=replay_note,
                mean_delay=mean_delay  # ä¼ å…¥å¹³å‡å»¶æ—¶
            )
            
            if result is None:
                return {
                    'status': 'error',
                    'message': 'æ›²çº¿å¯¹æ¯”å¤±è´¥'
                }
            
            # ç”Ÿæˆæ‰€æœ‰å¤„ç†é˜¶æ®µçš„å¯¹æ¯”å›¾ (å¤§å›¾)
            all_stages_fig = None
            # ç”Ÿæˆç‹¬ç«‹çš„å¤„ç†é˜¶æ®µå›¾è¡¨åˆ—è¡¨
            individual_stage_figures = []
            
            if 'processing_stages' in result:
                # å…¼å®¹æ—§çš„å¤§å›¾é€»è¾‘
                all_stages_fig = analyzer.visualize_all_processing_stages(result)
                # ç”Ÿæˆæ–°çš„ç‹¬ç«‹å›¾è¡¨åˆ—è¡¨
                individual_stage_figures = analyzer.generate_processing_stages_figures(result)
            
            # ç”Ÿæˆå¯¹é½å‰åå¯¹æ¯”å›¾ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            comparison_fig = None
            if 'alignment_comparison' in result:
                comparison_fig = analyzer.visualize_alignment_comparison(result)
            
            return {
                'status': 'success',
                'result': result,
                'comparison_figure': comparison_fig,
                'all_stages_figure': all_stages_fig, # ä¿ç•™ä»¥å…¼å®¹
                'individual_stage_figures': individual_stage_figures, # æ–°å¢å­—æ®µ
                'record_index': record_note.id if hasattr(record_note, 'id') else 'N/A',
                'replay_index': replay_note.id if hasattr(replay_note, 'id') else 'N/A'
            }
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ›²çº¿å¯¹é½å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return {
                'status': 'error',
                'message': f'æµ‹è¯•å¤±è´¥: {str(e)}'
            }
    
    def generate_scatter_detail_plot_by_indices(self, record_index: int, replay_index: int) -> Tuple[Any, Any, Any]:
        """
        æ ¹æ®record_indexå’Œreplay_indexç”Ÿæˆæ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾

        Args:
            record_index: å½•åˆ¶éŸ³ç¬¦ç´¢å¼•
            replay_index: æ’­æ”¾éŸ³ç¬¦ç´¢å¼•

        Returns:
            Tuple[Any, Any, Any]: (å½•åˆ¶éŸ³ç¬¦å›¾, æ’­æ”¾éŸ³ç¬¦å›¾, å¯¹æ¯”å›¾)
        """
        if not self.analyzer or not self.analyzer.note_matcher:
            logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†æ›²çº¿å›¾")
            return None, None, None
        
        # ä»precision_matched_pairsä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡ï¼ˆç¡®ä¿åªä½¿ç”¨ç²¾ç¡®åŒ¹é…å¯¹ï¼‰
        precision_matched_pairs = self.analyzer.note_matcher.precision_matched_pairs
        record_note = None
        play_note = None

        for r_idx, p_idx, r_note, p_note in precision_matched_pairs:
            if r_idx == record_index and p_idx == replay_index:
                record_note = r_note
                play_note = p_note
                break
        
        if record_note is None or play_note is None:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å¯¹: record_index={record_index}, replay_index={replay_index}")
            return None, None, None

        # è®¡ç®—å¹³å‡å»¶æ—¶
        mean_delays = {}
        mean_delay_val = 0.0
        if self.analyzer:
            mean_error_0_1ms = self.analyzer.get_mean_error()
            mean_delay_val = mean_error_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’
            mean_delays['default'] = mean_delay_val
        else:
            logger.warning("âš ï¸ æ— æ³•è·å–å•ç®—æ³•æ¨¡å¼çš„å¹³å‡å»¶æ—¶")

        # ä½¿ç”¨spmidæ¨¡å—ç”Ÿæˆè¯¦ç»†å›¾è¡¨
        detail_figure1 = spmid.plot_note_comparison_plotly(record_note, None, mean_delays=mean_delays)
        detail_figure2 = spmid.plot_note_comparison_plotly(None, play_note, mean_delays=mean_delays)
        detail_figure_combined = spmid.plot_note_comparison_plotly(record_note, play_note, mean_delays=mean_delays)
        
        # ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾
        processing_stages_figure = None
        if self.force_curve_analyzer:
            try:
                comparison_result = self.force_curve_analyzer.compare_curves(
                    record_note, 
                    play_note,
                    mean_delay=mean_delay_val  # ä¼ å…¥å¹³å‡å»¶æ—¶
                )
                if comparison_result:
                    processing_stages_figure = self.force_curve_analyzer.visualize_all_processing_stages(comparison_result)
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾å¤±è´¥: {e}")

        logger.info(f"âœ… ç”Ÿæˆæ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾ï¼Œrecord_index={record_index}, replay_index={replay_index}")
        return detail_figure1, detail_figure2, detail_figure_combined
    
    def generate_multi_algorithm_scatter_detail_plot_by_indices(
        self,
        algorithm_name: str,
        record_index: int,
        replay_index: int
    ) -> Tuple[Any, Any, Any]:
        """
        æ ¹æ®ç®—æ³•åç§°ã€record_indexå’Œreplay_indexç”Ÿæˆå¤šç®—æ³•æ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾

        Args:
            algorithm_name: ç®—æ³•åç§°
            record_index: å½•åˆ¶éŸ³ç¬¦ç´¢å¼•
            replay_index: æ’­æ”¾éŸ³ç¬¦ç´¢å¼•

        Returns:
            Tuple[Any, Any, Any]: (å½•åˆ¶éŸ³ç¬¦å›¾, æ’­æ”¾éŸ³ç¬¦å›¾, å¯¹æ¯”å›¾)
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            logger.warning("âš ï¸ ä¸åœ¨å¤šç®—æ³•æ¨¡å¼ï¼Œæ— æ³•ç”Ÿæˆå¤šç®—æ³•è¯¦ç»†æ›²çº¿å›¾")
            return None, None, None
        
        # æ ¹æ® display_name æŸ¥æ‰¾ç®—æ³•
        logger.info(f"ğŸ” generate_multi_algorithm_scatter_detail_plot_by_indices: æŸ¥æ‰¾ç®—æ³• display_name='{algorithm_name}'")
        algorithm = None
        for alg in self.multi_algorithm_manager.get_all_algorithms():
            logger.debug(f"ğŸ” æ£€æŸ¥ç®—æ³•: display_name='{alg.metadata.display_name}', algorithm_name='{alg.metadata.algorithm_name}'")
            if alg.metadata.display_name == algorithm_name:
                algorithm = alg
                logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…ç®—æ³•: {alg.metadata.display_name}")
                break

        if not algorithm or not algorithm.analyzer or not algorithm.analyzer.note_matcher:
            logger.warning(f"âš ï¸ ç®—æ³• '{algorithm_name}' ä¸å­˜åœ¨æˆ–æ²¡æœ‰åˆ†æå™¨ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†æ›²çº¿å›¾")
            # è°ƒè¯•ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨ç®—æ³•
            all_algs = self.multi_algorithm_manager.get_all_algorithms()
            logger.warning(f"âš ï¸ å¯ç”¨ç®—æ³•åˆ—è¡¨: {[f'{alg.metadata.display_name}({alg.metadata.algorithm_name})' for alg in all_algs]}")
            return None, None, None
        
        # ä»precision_matched_pairsä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡ï¼ˆé”¤é€Ÿå¯¹æ¯”å›¾ä½¿ç”¨precisionæ•°æ®ï¼‰
        precision_matched_pairs = algorithm.analyzer.note_matcher.precision_matched_pairs
        record_note = None
        play_note = None

        for r_idx, p_idx, r_note, p_note in precision_matched_pairs:
            if r_idx == record_index and p_idx == replay_index:
                record_note = r_note
                play_note = p_note
                break
        
        if record_note is None or play_note is None:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å¯¹: ç®—æ³•={algorithm_name}, record_index={record_index}, replay_index={replay_index}")
            logger.warning(f"âš ï¸ precision_matched_pairs æ•°é‡: {len(precision_matched_pairs)}")
            for i, (r_idx, p_idx, r_note, p_note) in enumerate(precision_matched_pairs[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"âš ï¸ åŒ¹é…å¯¹ {i}: record_idx={r_idx}, replay_idx={p_idx}, has_notes={r_note is not None and p_note is not None}")
            return None, None, None

        # è®¡ç®—å¹³å‡å»¶æ—¶
        mean_delays = {}
        if algorithm.analyzer:
            mean_error_0_1ms = algorithm.analyzer.get_mean_error()
            mean_delays[algorithm_name] = mean_error_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’
        else:
            logger.warning(f"âš ï¸ æ— æ³•è·å–ç®—æ³• '{algorithm_name}' çš„å¹³å‡å»¶æ—¶")

        # ä½¿ç”¨spmidæ¨¡å—ç”Ÿæˆè¯¦ç»†å›¾è¡¨
        detail_figure1 = spmid.plot_note_comparison_plotly(record_note, None, algorithm_name=algorithm_name, mean_delays=mean_delays)
        detail_figure2 = spmid.plot_note_comparison_plotly(None, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
        detail_figure_combined = spmid.plot_note_comparison_plotly(record_note, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
        
        # ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾
        processing_stages_figure = None
        if self.force_curve_analyzer:
            try:
                comparison_result = self.force_curve_analyzer.compare_curves(record_note, play_note)
                if comparison_result:
                    processing_stages_figure = self.force_curve_analyzer.visualize_all_processing_stages(comparison_result)
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾å¤±è´¥: {e}")
        
        logger.info(f"âœ… ç”Ÿæˆå¤šç®—æ³•æ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾ï¼Œç®—æ³•={algorithm_name}, record_index={record_index}, replay_index={replay_index}")
        return detail_figure1, detail_figure2, detail_figure_combined
    
    def get_note_time_range_for_waterfall(self, algorithm_name: Optional[str], record_index: int, replay_index: int, margin_ms: float = 500.0) -> Optional[Tuple[float, float]]:
        """
        æ ¹æ®record_indexå’Œreplay_indexè·å–éŸ³ç¬¦çš„æ—¶é—´èŒƒå›´ï¼Œç”¨äºè°ƒæ•´ç€‘å¸ƒå›¾æ˜¾ç¤º
        
        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆå¤šç®—æ³•æ¨¡å¼éœ€è¦ï¼Œå•ç®—æ³•æ¨¡å¼ä¸ºNoneï¼‰
            record_index: å½•åˆ¶éŸ³ç¬¦ç´¢å¼•
            replay_index: æ’­æ”¾éŸ³ç¬¦ç´¢å¼•
            margin_ms: æ—¶é—´èŒƒå›´çš„å‰åè¾¹è·ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤500ms
        
        Returns:
            Optional[Tuple[float, float]]: (start_time_ms, end_time_ms) æˆ– None
        """
        try:
            if algorithm_name:
                # å¤šç®—æ³•æ¨¡å¼
                if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
                    logger.warning("âš ï¸ ä¸åœ¨å¤šç®—æ³•æ¨¡å¼ï¼Œæ— æ³•è·å–éŸ³ç¬¦æ—¶é—´èŒƒå›´")
                    return None
                
                # æ ¹æ® display_name æŸ¥æ‰¾ç®—æ³•
                algorithm = None
                for alg in self.multi_algorithm_manager.get_all_algorithms():
                    if alg.metadata.display_name == algorithm_name:
                        algorithm = alg
                        break

                if not algorithm or not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                    logger.warning(f"âš ï¸ ç®—æ³• '{algorithm_name}' ä¸å­˜åœ¨æˆ–æ²¡æœ‰åˆ†æå™¨")
                    return None
                
                matched_pairs = algorithm.analyzer.matched_pairs
            else:
                # å•ç®—æ³•æ¨¡å¼
                if not self.analyzer or not self.analyzer.note_matcher:
                    logger.warning("âš ï¸ åˆ†æå™¨æˆ–åŒ¹é…å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è·å–éŸ³ç¬¦æ—¶é—´èŒƒå›´")
                    return None
                
                matched_pairs = self.analyzer.matched_pairs
            
            # ä»matched_pairsä¸­æŸ¥æ‰¾å¯¹åº”çš„Noteå¯¹è±¡
            record_note = None
            replay_note = None
            
            for r_idx, p_idx, r_note, p_note in matched_pairs:
                if r_idx == record_index and p_idx == replay_index:
                    record_note = r_note
                    replay_note = p_note
                    break
            
            if record_note is None or replay_note is None:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å¯¹: record_index={record_index}, replay_index={replay_index}")
                return None
            
            # è®¡ç®—éŸ³ç¬¦çš„æ—¶é—´ï¼ˆå•ä½ï¼š0.1msï¼‰
            # ä½¿ç”¨keyonæ—¶é—´ä½œä¸ºä¸­å¿ƒç‚¹
            record_keyon = record_note.after_touch.index[0] + record_note.offset if hasattr(record_note, 'after_touch') and not record_note.after_touch.empty else record_note.offset
            replay_keyon = replay_note.after_touch.index[0] + replay_note.offset if hasattr(replay_note, 'after_touch') and not replay_note.after_touch.empty else replay_note.offset
            
            # è½¬æ¢ä¸ºæ¯«ç§’
            record_keyon_ms = record_keyon / 10.0
            replay_keyon_ms = replay_keyon / 10.0
            
            # è®¡ç®—æŒ‰é”®æŒç»­æ—¶é—´ï¼ˆkeyoff - keyonï¼‰
            record_keyoff = record_note.after_touch.index[-1] + record_note.offset if hasattr(record_note, 'after_touch') and not record_note.after_touch.empty else record_note.offset
            replay_keyoff = replay_note.after_touch.index[-1] + replay_note.offset if hasattr(replay_note, 'after_touch') and not replay_note.after_touch.empty else replay_note.offset
            
            record_keyoff_ms = record_keyoff / 10.0
            replay_keyoff_ms = replay_keyoff / 10.0
            
            # è®¡ç®—æŒ‰é”®æŒç»­æ—¶é—´ï¼ˆå–ä¸¤ä¸ªæŒ‰é”®ä¸­è¾ƒé•¿çš„ï¼‰
            record_duration = record_keyoff_ms - record_keyon_ms
            replay_duration = replay_keyoff_ms - replay_keyon_ms
            note_duration = max(record_duration, replay_duration)
            
            # è®¡ç®—ä¸­å¿ƒæ—¶é—´ï¼ˆå–ä¸¤ä¸ªéŸ³ç¬¦keyonæ—¶é—´çš„ä¸­é—´å€¼ï¼‰
            center_time_ms = (record_keyon_ms + replay_keyon_ms) / 2.0
            
            # åŠ¨æ€è°ƒæ•´æ—¶é—´èŒƒå›´ï¼šç¡®ä¿èƒ½çœ‹åˆ°æŒ‰é”®æœ¬èº«ä»¥åŠå‘¨å›´çš„æ•°æ®ç‚¹
            # åŸºç¡€è¾¹è· + æŒ‰é”®æŒç»­æ—¶é—´çš„å€æ•°ï¼Œç¡®ä¿èƒ½çœ‹åˆ°æŒ‰é”®å‰åå¤šä¸ªæŒ‰é”®
            # æœ€å°è¾¹è·ä¸ºmargin_msï¼Œå¦‚æœæŒ‰é”®æŒç»­æ—¶é—´è¾ƒé•¿ï¼Œåˆ™å¢åŠ è¾¹è·ï¼ˆæŒ‰é”®æŒç»­æ—¶é—´çš„3å€ï¼‰
            # è¿™æ ·å³ä½¿æŒ‰é”®å¾ˆé•¿ï¼Œä¹Ÿèƒ½çœ‹åˆ°å‰åè¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
            dynamic_margin = max(margin_ms, note_duration * 3.0)
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_time_ms = max(0, center_time_ms - dynamic_margin)
            end_time_ms = center_time_ms + dynamic_margin
            
            logger.info(f"âœ… è®¡ç®—éŸ³ç¬¦æ—¶é—´èŒƒå›´: center={center_time_ms:.1f}ms, range=[{start_time_ms:.1f}, {end_time_ms:.1f}]ms")
            return (start_time_ms, end_time_ms)
            
        except Exception as e:
            logger.error(f"âŒ è·å–éŸ³ç¬¦æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
            return None


    def generate_multi_algorithm_detail_plot_by_index(
        self,
        algorithm_name: str,
        index: int,
        is_record: bool
    ) -> Tuple[Figure, Figure, Figure]:
        """
        å¤šç®—æ³•æ¨¡å¼ä¸‹ï¼Œæ ¹æ®ç®—æ³•åç§°å’Œç´¢å¼•ç”Ÿæˆè¯¦ç»†å›¾è¡¨

        Args:
            algorithm_name: ç®—æ³•åç§°
            index: éŸ³ç¬¦ç´¢å¼•
            is_record: æ˜¯å¦ä¸ºå½•åˆ¶æ•°æ®

        Returns:
            Tuple[Figure, Figure, Figure]: (å½•åˆ¶éŸ³ç¬¦å›¾, æ’­æ”¾éŸ³ç¬¦å›¾, å¯¹æ¯”å›¾)
        """
        if not self.multi_algorithm_manager:
            self._ensure_multi_algorithm_manager()
        
        # è·å–å¯¹åº”çš„ç®—æ³•æ•°æ®é›†
        algorithm = self.multi_algorithm_manager.get_algorithm(algorithm_name)
        if not algorithm or not algorithm.is_ready():
            logger.error(f"ç®—æ³• '{algorithm_name}' ä¸å­˜åœ¨æˆ–æœªå°±ç»ª")
            return None, None, None
        
        # ä»ç®—æ³•çš„analyzerä¸­è·å–æ•°æ®
        if not algorithm.analyzer:
            logger.error(f"ç®—æ³• '{algorithm_name}' æ²¡æœ‰åˆ†æå™¨")
            return None, None, None
        
        # è·å–ç®—æ³•çš„æœ‰æ•ˆæ•°æ®
        valid_record_data = algorithm.analyzer.valid_record_data if hasattr(algorithm.analyzer, 'valid_record_data') else []
        valid_replay_data = algorithm.analyzer.valid_replay_data if hasattr(algorithm.analyzer, 'valid_replay_data') else []
        matched_pairs = algorithm.analyzer.matched_pairs if hasattr(algorithm.analyzer, 'matched_pairs') else []
        
        record_note = None
        play_note = None
        
        if is_record:
            if index < 0 or index >= len(valid_record_data):
                logger.error(f"å½•åˆ¶æ•°æ®ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ [0, {len(valid_record_data)-1}]")
                return None, None, None
            record_note = valid_record_data[index]
            
            # ä»matched_pairsä¸­æŸ¥æ‰¾åŒ¹é…çš„æ’­æ”¾éŸ³ç¬¦
            if matched_pairs:
                for record_index, replay_index, r_note, p_note in matched_pairs:
                    if record_index == index:
                        play_note = p_note
                        break
        else:
            if index < 0 or index >= len(valid_replay_data):
                logger.error(f"æ’­æ”¾æ•°æ®ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ [0, {len(valid_replay_data)-1}]")
                return None, None, None
            play_note = valid_replay_data[index]
            
            # ä»matched_pairsä¸­æŸ¥æ‰¾åŒ¹é…çš„å½•åˆ¶éŸ³ç¬¦
            if matched_pairs:
                for record_index, replay_index, r_note, p_note in matched_pairs:
                    if replay_index == index:
                        record_note = r_note
                        break
        
        # è®¡ç®—å¹³å‡å»¶æ—¶
        mean_delays = {}
        if algorithm.analyzer:
            mean_error_0_1ms = algorithm.analyzer.get_mean_error()
            mean_delays[algorithm_name] = mean_error_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’

        # ä½¿ç”¨spmidæ¨¡å—ç”Ÿæˆè¯¦ç»†å›¾è¡¨
        detail_figure1 = spmid.plot_note_comparison_plotly(record_note, None, algorithm_name=algorithm_name, mean_delays=mean_delays)
        detail_figure2 = spmid.plot_note_comparison_plotly(None, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
        detail_figure_combined = spmid.plot_note_comparison_plotly(record_note, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
        
        # ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾
        processing_stages_figure = None
        if self.force_curve_analyzer and record_note and play_note:
            try:
                comparison_result = self.force_curve_analyzer.compare_curves(record_note, play_note)
                if comparison_result:
                    processing_stages_figure = self.force_curve_analyzer.visualize_all_processing_stages(comparison_result)
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå…¨è¿‡ç¨‹å¤„ç†å›¾å¤±è´¥: {e}")
        
        logger.info(f"âœ… ç”Ÿæˆç®—æ³• '{algorithm_name}' çš„è¯¦ç»†å›¾è¡¨ï¼Œç´¢å¼•={index}, ç±»å‹={'record' if is_record else 'play'}")
        return detail_figure1, detail_figure2, detail_figure_combined
    
    def generate_multi_algorithm_error_detail_plot_by_index(
        self,
        algorithm_name: str,
        index: int,
        error_type: str,  # 'drop' æˆ– 'multi'
        expected_key_id=None  # æœŸæœ›çš„keyIdï¼Œç”¨äºéªŒè¯
    ) -> Tuple[Any, Any, Any]:
        """
        å¤šç®—æ³•æ¨¡å¼ä¸‹ï¼Œæ ¹æ®ç®—æ³•åç§°å’Œç´¢å¼•ç”Ÿæˆé”™è¯¯éŸ³ç¬¦ï¼ˆä¸¢é”¤/å¤šé”¤ï¼‰çš„è¯¦ç»†å›¾è¡¨
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            index: éŸ³ç¬¦ç´¢å¼•ï¼ˆåœ¨initial_valid_record_dataæˆ–initial_valid_replay_dataä¸­çš„ç´¢å¼•ï¼‰
            error_type: é”™è¯¯ç±»å‹ï¼Œ'drop'è¡¨ç¤ºä¸¢é”¤ï¼ˆå½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— ï¼‰ï¼Œ'multi'è¡¨ç¤ºå¤šé”¤ï¼ˆæ’­æ”¾æœ‰ï¼Œå½•åˆ¶æ— ï¼‰
            
        Returns:
            Tuple[Any, Any, Any]: (å½•åˆ¶éŸ³ç¬¦å›¾, æ’­æ”¾éŸ³ç¬¦å›¾, å¯¹æ¯”å›¾)
        """
        if not self.multi_algorithm_manager:
            self._ensure_multi_algorithm_manager()
        
        # è·å–å¯¹åº”çš„ç®—æ³•æ•°æ®é›†
        algorithm = self.multi_algorithm_manager.get_algorithm(algorithm_name)
        if not algorithm or not algorithm.is_ready():
            logger.error(f"ç®—æ³• '{algorithm_name}' ä¸å­˜åœ¨æˆ–æœªå°±ç»ª")
            return None, None, None
        
        # ä»ç®—æ³•çš„analyzerä¸­è·å–æ•°æ®
        if not algorithm.analyzer:
            logger.error(f"ç®—æ³• '{algorithm_name}' æ²¡æœ‰åˆ†æå™¨")
            return None, None, None
        
        # è·å–åˆå§‹æœ‰æ•ˆæ•°æ®ï¼ˆç¬¬ä¸€æ¬¡è¿‡æ»¤åçš„æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰æœ‰æ•ˆéŸ³ç¬¦ï¼‰
        initial_valid_record_data = getattr(algorithm.analyzer, 'initial_valid_record_data', None)
        initial_valid_replay_data = getattr(algorithm.analyzer, 'initial_valid_replay_data', None)
        
        if initial_valid_record_data is None or initial_valid_replay_data is None:
            logger.error(f"ç®—æ³• '{algorithm_name}' æ²¡æœ‰åˆå§‹æœ‰æ•ˆæ•°æ®")
            return None, None, None
        
        # è·å–åŒ¹é…å¯¹åˆ—è¡¨ï¼Œå°è¯•æŸ¥æ‰¾æ˜¯å¦æœ‰åŒ¹é…çš„éŸ³ç¬¦å¯¹
        # æ³¨æ„ï¼šç°åœ¨æ‰€æœ‰æˆåŠŸçš„åŒ¹é…éƒ½åœ¨matched_pairsä¸­ï¼ŒåŒ…æ‹¬æ‰©å±•å€™é€‰åŒ¹é…
        matched_pairs = getattr(algorithm.analyzer, 'matched_pairs', [])
        note_matcher = getattr(algorithm.analyzer, 'note_matcher', None)
        
        record_note = None
        play_note = None
        
        if error_type == 'drop':
            # ä¸¢é”¤ï¼šå½•åˆ¶æœ‰ï¼Œæ’­æ”¾å¯èƒ½æ— ä¹Ÿå¯èƒ½æœ‰ï¼ˆå¦‚æœè¶…è¿‡é˜ˆå€¼ä½†èƒ½åŒ¹é…åˆ°ï¼‰
            if index < 0 or index >= len(initial_valid_record_data):
                logger.error(f"å½•åˆ¶æ•°æ®ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ [0, {len(initial_valid_record_data)-1}]")
                return None, None, None
            record_note = initial_valid_record_data[index]
            
            # éªŒè¯NoteIDï¼šä»è¡¨æ ¼æ•°æ®ä¸­è·å–çš„keyIdåº”è¯¥ä¸éŸ³ç¬¦çš„idä¸€è‡´
            if expected_key_id is not None:
                if str(record_note.id) != str(expected_key_id):
                    logger.error(f"âŒ NoteIDä¸åŒ¹é…: è¡¨æ ¼ä¸­çš„keyId={expected_key_id}, éŸ³ç¬¦çš„id={record_note.id}, ç®—æ³•={algorithm_name}, index={index}")
                    return None, None, None
                logger.info(f"âœ… NoteIDéªŒè¯é€šè¿‡: keyId={expected_key_id}, éŸ³ç¬¦id={record_note.id}")
            
            # å°è¯•ä»matched_pairsä¸­æŸ¥æ‰¾åŒ¹é…çš„æ’­æ”¾éŸ³ç¬¦
            play_note = None
            # æ£€æŸ¥matched_pairsï¼ˆç°åœ¨åŒ…å«æ‰€æœ‰æˆåŠŸçš„åŒ¹é…ï¼‰
            if matched_pairs:
                for record_index, replay_index, r_note, p_note in matched_pairs:
                    if record_index == index:
                        play_note = p_note
                        logger.info(f"ğŸ” ä¸¢é”¤æ•°æ®åœ¨matched_pairsä¸­æ‰¾åˆ°åŒ¹é…çš„æ’­æ”¾éŸ³ç¬¦: replay_index={replay_index}")
                        break
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†æ£€æŸ¥è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…å¯¹
            # æ³¨æ„ï¼šç°åœ¨æ‰€æœ‰åŒ¹é…éƒ½åœ¨matched_pairsä¸­ï¼Œä¸éœ€è¦é¢å¤–æ£€æŸ¥
            
            if play_note is None:
                logger.info(f"âœ… ç”Ÿæˆä¸¢é”¤è¯¦ç»†æ›²çº¿å›¾ï¼ˆæ— åŒ¹é…æ’­æ”¾æ•°æ®ï¼‰ï¼Œç®—æ³•={algorithm_name}, index={index}")
            else:
                logger.info(f"âœ… ç”Ÿæˆä¸¢é”¤è¯¦ç»†æ›²çº¿å›¾ï¼ˆæœ‰åŒ¹é…æ’­æ”¾æ•°æ®ï¼Œæ˜¾ç¤ºå¯¹æ¯”å›¾ï¼‰ï¼Œç®—æ³•={algorithm_name}, index={index}")
                
        elif error_type == 'multi':
            # å¤šé”¤ï¼šæ’­æ”¾æœ‰ï¼Œå½•åˆ¶å¯èƒ½æ— ä¹Ÿå¯èƒ½æœ‰ï¼ˆå¦‚æœè¶…è¿‡é˜ˆå€¼ä½†èƒ½åŒ¹é…åˆ°ï¼‰
            if index < 0 or index >= len(initial_valid_replay_data):
                logger.error(f"æ’­æ”¾æ•°æ®ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ [0, {len(initial_valid_replay_data)-1}]")
                return None, None, None
            play_note = initial_valid_replay_data[index]
            
            # éªŒè¯NoteIDï¼šä»è¡¨æ ¼æ•°æ®ä¸­è·å–çš„keyIdåº”è¯¥ä¸éŸ³ç¬¦çš„idä¸€è‡´
            if expected_key_id is not None:
                if str(play_note.id) != str(expected_key_id):
                    logger.error(f"âŒ NoteIDä¸åŒ¹é…: è¡¨æ ¼ä¸­çš„keyId={expected_key_id}, éŸ³ç¬¦çš„id={play_note.id}, ç®—æ³•={algorithm_name}, index={index}")
                    return None, None, None
                logger.info(f"âœ… NoteIDéªŒè¯é€šè¿‡: keyId={expected_key_id}, éŸ³ç¬¦id={play_note.id}")
            
            # å°è¯•ä»matched_pairsä¸­æŸ¥æ‰¾åŒ¹é…çš„å½•åˆ¶éŸ³ç¬¦
            record_note = None
            # æ£€æŸ¥matched_pairsï¼ˆç°åœ¨åŒ…å«æ‰€æœ‰æˆåŠŸçš„åŒ¹é…ï¼‰
            if matched_pairs:
                for record_index, replay_index, r_note, p_note in matched_pairs:
                    if replay_index == index:
                        record_note = r_note
                        logger.info(f"ğŸ” å¤šé”¤æ•°æ®åœ¨matched_pairsä¸­æ‰¾åˆ°åŒ¹é…çš„å½•åˆ¶éŸ³ç¬¦: record_index={record_index}")
                        break
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†æ£€æŸ¥è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…å¯¹
            # æ³¨æ„ï¼šç°åœ¨æ‰€æœ‰åŒ¹é…éƒ½åœ¨matched_pairsä¸­ï¼Œä¸éœ€è¦é¢å¤–æ£€æŸ¥
            
            if record_note is None:
                logger.info(f"âœ… ç”Ÿæˆå¤šé”¤è¯¦ç»†æ›²çº¿å›¾ï¼ˆæ— åŒ¹é…å½•åˆ¶æ•°æ®ï¼‰ï¼Œç®—æ³•={algorithm_name}, index={index}")
            else:
                logger.info(f"âœ… ç”Ÿæˆå¤šé”¤è¯¦ç»†æ›²çº¿å›¾ï¼ˆæœ‰åŒ¹é…å½•åˆ¶æ•°æ®ï¼Œæ˜¾ç¤ºå¯¹æ¯”å›¾ï¼‰ï¼Œç®—æ³•={algorithm_name}, index={index}")
        else:
            logger.error(f"æœªçŸ¥çš„é”™è¯¯ç±»å‹: {error_type}")
            return None, None, None
        
        # ç”Ÿæˆå›¾è¡¨

        try:
            # éªŒè¯ Note å¯¹è±¡æ˜¯å¦æœ‰æ•ˆ
            if error_type == 'drop' and record_note:
                logger.info(f"ğŸ” ä¸¢é”¤ - record_note ID={record_note.id}, after_touché•¿åº¦={len(record_note.after_touch) if hasattr(record_note, 'after_touch') and record_note.after_touch is not None else 0}, hammersé•¿åº¦={len(record_note.hammers) if hasattr(record_note, 'hammers') and record_note.hammers is not None else 0}")
            elif error_type == 'multi' and play_note:
                logger.info(f"ğŸ” å¤šé”¤ - play_note ID={play_note.id}, after_touché•¿åº¦={len(play_note.after_touch) if hasattr(play_note, 'after_touch') and play_note.after_touch is not None else 0}, hammersé•¿åº¦={len(play_note.hammers) if hasattr(play_note, 'hammers') and play_note.hammers is not None else 0}")
            
            # è®¡ç®—å¹³å‡å»¶æ—¶
            mean_delays = {}
            if algorithm and algorithm.analyzer:
                mean_error_0_1ms = algorithm.analyzer.get_mean_error()
                mean_delays[algorithm_name] = mean_error_0_1ms / 10.0  # è½¬æ¢ä¸ºæ¯«ç§’

            detail_figure1 = spmid.plot_note_comparison_plotly(record_note, None, algorithm_name=algorithm_name, mean_delays=mean_delays)
            detail_figure2 = spmid.plot_note_comparison_plotly(None, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
            detail_figure_combined = spmid.plot_note_comparison_plotly(record_note, play_note, algorithm_name=algorithm_name, mean_delays=mean_delays)
            
            # éªŒè¯å›¾è¡¨æ˜¯å¦æœ‰æ•ˆ
            if detail_figure1 is None or detail_figure2 is None or detail_figure_combined is None:
                logger.error(f"âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œéƒ¨åˆ†å›¾è¡¨ä¸ºNone")
                return None, None, None
            
            logger.info(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: figure1æ•°æ®ç‚¹={len(detail_figure1.data)}, figure2æ•°æ®ç‚¹={len(detail_figure2.data)}, figure_combinedæ•°æ®ç‚¹={len(detail_figure_combined.data)}")
            
            return detail_figure1, detail_figure2, detail_figure_combined
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
            
            logger.error(traceback.format_exc())
            return None, None, None
    
    def get_note_image_base64(self, global_index: int) -> str:
        """è·å–éŸ³ç¬¦å›¾åƒBase64ç¼–ç """
        return self.plot_generator.get_note_image_base64(global_index)
    
    # ==================== æ•°æ®è¿‡æ»¤ç›¸å…³æ–¹æ³• ====================
    
    def get_available_keys(self) -> List[int]:
        """è·å–å¯ç”¨æŒ‰é”®åˆ—è¡¨"""
        return self.data_filter.get_available_keys()
    
    def set_key_filter(self, key_ids: List[int]) -> None:
        """è®¾ç½®æŒ‰é”®è¿‡æ»¤"""
        self.data_filter.set_key_filter(key_ids)
    
    def get_key_filter_status(self) -> Dict[str, Any]:
        """è·å–æŒ‰é”®è¿‡æ»¤çŠ¶æ€"""
        return self.data_filter.get_key_filter_status()
    
    def set_time_filter(self, time_range: Optional[Tuple[float, float]]) -> None:
        """è®¾ç½®æ—¶é—´èŒƒå›´è¿‡æ»¤"""
        self.time_filter.set_time_filter(time_range)
    
    def get_time_filter_status(self) -> Dict[str, Any]:
        """è·å–æ—¶é—´è¿‡æ»¤çŠ¶æ€"""
        return self.time_filter.get_time_filter_status()
    
    def get_time_range(self) -> Tuple[float, float]:
        """è·å–æ—¶é—´èŒƒå›´ä¿¡æ¯"""
        return self.time_filter.get_time_range()
    
    def get_display_time_range(self) -> Tuple[float, float]:
        """è·å–æ˜¾ç¤ºæ—¶é—´èŒƒå›´"""
        return self.time_filter.get_display_time_range()
    
    def update_time_range_from_input(self, start_time: float, end_time: float) -> Tuple[bool, str]:
        """ä»è¾“å…¥æ›´æ–°æ—¶é—´èŒƒå›´"""
        success = self.time_filter.update_time_range_from_input(start_time, end_time)
        if success:
            return True, "æ—¶é—´èŒƒå›´æ›´æ–°æˆåŠŸ"
        else:
            return False, "æ—¶é—´èŒƒå›´æ›´æ–°å¤±è´¥"
    
    def get_time_range_info(self) -> Dict[str, Any]:
        """è·å–æ—¶é—´èŒƒå›´è¯¦ç»†ä¿¡æ¯"""
        return self.time_filter.get_time_range_info()
    
    def reset_display_time_range(self) -> None:
        """é‡ç½®æ˜¾ç¤ºæ—¶é—´èŒƒå›´"""
        self.time_filter.reset_display_time_range()
    
    def get_filtered_data(self) -> Dict[str, Any]:
        """è·å–è¿‡æ»¤åçš„æ•°æ®"""
        return self.data_filter.get_filtered_data()
    
    # ==================== è¡¨æ ¼æ•°æ®ç›¸å…³æ–¹æ³• ====================
    
    def get_summary_info(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦ä¿¡æ¯"""
        return self.table_generator.get_summary_info()

    def calculate_accuracy_for_algorithm(self, algorithm) -> float:
        """
        ä¸ºæŒ‡å®šçš„ç®—æ³•è®¡ç®—å‡†ç¡®ç‡ - é‡æ„ç‰ˆæœ¬

        ä½¿ç”¨ç»Ÿä¸€çš„AlgorithmStatisticsæœåŠ¡è¿›è¡Œè®¡ç®—ï¼Œç¡®ä¿å‰åç«¯æ•°æ®ä¸€è‡´æ€§

        Args:
            algorithm: ç®—æ³•å¯¹è±¡ï¼ˆå…·æœ‰analyzerå±æ€§ï¼‰

        Returns:
            float: å‡†ç¡®ç‡ç™¾åˆ†æ¯”
        """
        try:
            if not hasattr(algorithm, 'analyzer') or not algorithm.analyzer:
                return 0.0

            # ä½¿ç”¨ç»Ÿä¸€çš„ç»Ÿè®¡æœåŠ¡
            stats_service = AlgorithmStatistics(algorithm)
            accuracy_info = stats_service.get_accuracy_info()

            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            algorithm_name = getattr(getattr(algorithm, 'metadata', None), 'algorithm_name', 'unknown')
            print(f"[DEBUG] å‡†ç¡®ç‡è®¡ç®— - ç®—æ³•: {algorithm_name}")
            print(f"[DEBUG]   ç²¾ç¡®åŒ¹é…: {accuracy_info['precision_matches']} å¯¹")
            print(f"[DEBUG]   è¿‘ä¼¼åŒ¹é…: {accuracy_info['approximate_matches']} å¯¹")
            print(f"[DEBUG]   æ€»åŒ¹é…å¯¹: {accuracy_info['matched_count']} å¯¹")
            print(f"[DEBUG]   åˆ†å­ï¼ˆåŒ¹é…æŒ‰é”®æ•°ï¼‰: {accuracy_info['matched_count'] * 2}")
            print(f"[DEBUG]   åˆ†æ¯ï¼ˆæ€»æœ‰æ•ˆæŒ‰é”®æ•°ï¼‰: {accuracy_info['total_effective_keys']}")
            print(f"[DEBUG]   å‡†ç¡®ç‡: {accuracy_info['accuracy']:.2f}%")

            return accuracy_info['accuracy']

        except Exception as e:
            logger.error(f"è®¡ç®—ç®—æ³•å‡†ç¡®ç‡å¤±è´¥: {e}")
            return 0.0

    def get_algorithm_statistics(self, algorithm) -> dict:
        """
        è·å–ç®—æ³•çš„å®Œæ•´ç»Ÿè®¡ä¿¡æ¯ - ç»Ÿä¸€æ¥å£

        Args:
            algorithm: ç®—æ³•å¯¹è±¡

        Returns:
            dict: åŒ…å«å‡†ç¡®ç‡å’Œé”™è¯¯ç»Ÿè®¡çš„å®Œæ•´ä¿¡æ¯
        """
        try:
            if not hasattr(algorithm, 'analyzer') or not algorithm.analyzer:
                return {
                    'accuracy': 0.0,
                    'matched_count': 0,
                    'total_effective_keys': 0,
                    'precision_matches': 0,
                    'approximate_matches': 0,
                    'drop_count': 0,
                    'multi_count': 0,
                    'total_errors': 0
                }

            # ä½¿ç”¨ç»Ÿä¸€çš„ç»Ÿè®¡æœåŠ¡
            stats_service = AlgorithmStatistics(algorithm)
            return stats_service.get_full_statistics()

        except Exception as e:
            logger.error(f"è·å–ç®—æ³•ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'accuracy': 0.0,
                'matched_count': 0,
                'total_effective_keys': 0,
                'precision_matches': 0,
                'approximate_matches': 0,
                'drop_count': 0,
                'multi_count': 0,
                'total_errors': 0
            }
    
    def get_invalid_notes_table_data(self) -> List[Dict[str, Any]]:
        """
        è·å–æ— æ•ˆéŸ³ç¬¦è¡¨æ ¼æ•°æ®ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰
        
        Returns:
            List[Dict[str, Any]]: æ— æ•ˆéŸ³ç¬¦ç»Ÿè®¡è¡¨æ ¼æ•°æ®
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šä¸ºæ¯ä¸ªæ¿€æ´»çš„ç®—æ³•ç”Ÿæˆæ•°æ®
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            
            if not active_algorithms:
                return []
            
            table_data = []
            
            for algorithm in active_algorithms:
                algorithm_name = algorithm.metadata.algorithm_name
                
                if not algorithm.analyzer:
                    continue
                
                try:
                    # è·å–ç®—æ³•çš„æ— æ•ˆéŸ³ç¬¦ç»Ÿè®¡æ•°æ®
                    invalid_notes_table_data = getattr(algorithm.analyzer, 'invalid_notes_table_data', {})
                    
                    if not invalid_notes_table_data:
                        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ·»åŠ é»˜è®¤çš„ç©ºæ•°æ®
                        table_data.append({
                            'algorithm_name': algorithm_name,
                            'data_type': 'å½•åˆ¶æ•°æ®',
                            'total_notes': 0,
                            'valid_notes': 0,
                            'invalid_notes': 0,
                            'duration_too_short': 0,
                            'empty_data': 0,
                            'silent_notes': 0,
                            'other_errors': 0
                        })
                        table_data.append({
                            'algorithm_name': algorithm_name,
                            'data_type': 'å›æ”¾æ•°æ®',
                            'total_notes': 0,
                            'valid_notes': 0,
                            'invalid_notes': 0,
                            'duration_too_short': 0,
                            'empty_data': 0,
                            'silent_notes': 0,
                            'other_errors': 0
                        })
                        continue
                    
                    # å¤„ç†å½•åˆ¶æ•°æ®
                    record_data = invalid_notes_table_data.get('record_data', {})
                    invalid_reasons = record_data.get('invalid_reasons', {})
                    table_data.append({
                        'algorithm_name': algorithm_name,
                        'data_type': 'å½•åˆ¶æ•°æ®',
                        'total_notes': record_data.get('total_notes', 0),
                        'valid_notes': record_data.get('valid_notes', 0),
                        'invalid_notes': record_data.get('invalid_notes', 0),
                        'duration_too_short': invalid_reasons.get('duration_too_short', 0),
                        'empty_data': invalid_reasons.get('empty_data', 0),
                        'silent_notes': invalid_reasons.get('silent_notes', 0),
                        'other_errors': invalid_reasons.get('other_errors', 0)
                    })
                    
                    # å¤„ç†å›æ”¾æ•°æ®
                    replay_data = invalid_notes_table_data.get('replay_data', {})
                    replay_invalid_reasons = replay_data.get('invalid_reasons', {})
                    table_data.append({
                        'algorithm_name': algorithm_name,
                        'data_type': 'å›æ”¾æ•°æ®',
                        'total_notes': replay_data.get('total_notes', 0),
                        'valid_notes': replay_data.get('valid_notes', 0),
                        'invalid_notes': replay_data.get('invalid_notes', 0),
                        'duration_too_short': replay_invalid_reasons.get('duration_too_short', 0),
                        'empty_data': replay_invalid_reasons.get('empty_data', 0),
                        'silent_notes': replay_invalid_reasons.get('silent_notes', 0),
                        'other_errors': replay_invalid_reasons.get('other_errors', 0)
                    })
                    
                except Exception as e:
                    logger.error(f"âŒ è·å–ç®—æ³• '{algorithm_name}' çš„æ— æ•ˆéŸ³ç¬¦ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
                    
                    logger.error(traceback.format_exc())
                    continue
            
            return table_data
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼‰
        return self.table_generator.get_invalid_notes_table_data()
    
    def get_error_table_data(self, error_type: str) -> List[Dict[str, Any]]:
        """è·å–é”™è¯¯è¡¨æ ¼æ•°æ®ï¼ˆæ”¯æŒå•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šç®—æ³•æ¨¡å¼
        if self.multi_algorithm_mode and self.multi_algorithm_manager:
            # å¤šç®—æ³•æ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„é”™è¯¯æ•°æ®ï¼Œæ·»åŠ "ç®—æ³•åç§°"åˆ—
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            if not active_algorithms:
                return []
            
            table_data = []
            for algorithm in active_algorithms:
                algorithm_name = algorithm.metadata.algorithm_name
                
                if not algorithm.analyzer:
                    continue
                
                # è·å–è¯¥ç®—æ³•çš„é”™è¯¯æ•°æ®
                if error_type == 'ä¸¢é”¤':
                    error_notes = algorithm.analyzer.drop_hammers if hasattr(algorithm.analyzer, 'drop_hammers') else []
                elif error_type == 'å¤šé”¤':
                    error_notes = algorithm.analyzer.multi_hammers if hasattr(algorithm.analyzer, 'multi_hammers') else []
                else:
                    continue

                # è®°å½•è¯¥ç®—æ³•çš„é”™è¯¯æ•°æ®è¯¦æƒ…
                if error_notes:
                    logger.info(f"ğŸ“Š ç®—æ³• '{algorithm_name}' {error_type}æ•°æ®è¯¦æƒ… ({len(error_notes)}ä¸ª):")
                    for i, note in enumerate(error_notes[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        if len(note.infos) > 0:
                            info = note.infos[0]
                            logger.info(f"  {error_type}{i+1}: æŒ‰é”®ID={info.keyId}, ç´¢å¼•={info.index}, keyOn={info.keyOn/10:.2f}ms")
                    if len(error_notes) > 3:
                        logger.info(f"  ...è¿˜æœ‰{len(error_notes)-3}ä¸ª{error_type}æŒ‰é”®")

                # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®æ ¼å¼ï¼Œæ·»åŠ ç®—æ³•åç§°
                for note in error_notes:
                    row = {
                        'algorithm_name': algorithm_name,
                        'data_type': 'record' if error_type == 'ä¸¢é”¤' else 'play',
                        'keyId': note.keyId if hasattr(note, 'keyId') else 'N/A',
                    }

                    # æ·»åŠ æ—¶é—´å’Œç´¢å¼•ä¿¡æ¯
                    if error_type == 'ä¸¢é”¤':
                        row.update({
                            'keyOn': f"{note.keyOn/10:.2f}" if hasattr(note, 'keyOn') else 'N/A',
                            'keyOff': f"{note.keyOff/10:.2f}" if hasattr(note, 'keyOff') else 'N/A',
                            'index': note.index if hasattr(note, 'index') else 'N/A',
                            'analysis_reason': 'ä¸¢é”¤ï¼ˆå½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— ï¼‰'
                        })
                    else:  # å¤šé”¤
                        row.update({
                            'keyOn': f"{note.keyOn/10:.2f}" if hasattr(note, 'keyOn') else 'N/A',
                            'keyOff': f"{note.keyOff/10:.2f}" if hasattr(note, 'keyOff') else 'N/A',
                            'index': note.index if hasattr(note, 'index') else 'N/A',
                            'analysis_reason': 'å¤šé”¤ï¼ˆæ’­æ”¾æœ‰ï¼Œå½•åˆ¶æ— ï¼‰'
                        })

                    table_data.append(row)

            return table_data

        else:
            # å•ç®—æ³•æ¨¡å¼ï¼šä»self.analyzerè·å–æ•°æ®
            logger.info(f"ğŸ” å•ç®—æ³•æ¨¡å¼get_error_table_data: error_type={error_type}, self.analyzerå­˜åœ¨={self.analyzer is not None}")

            if not self.analyzer:
                logger.warning("âš ï¸ å•ç®—æ³•æ¨¡å¼ä¸‹analyzerä¸å­˜åœ¨")
                return []

            # è·å–è¯¥ç®—æ³•çš„é”™è¯¯æ•°æ®
            if error_type == 'ä¸¢é”¤':
                error_notes = self.analyzer.drop_hammers if hasattr(self.analyzer, 'drop_hammers') else []
                logger.info(f"ğŸ“Š å•ç®—æ³•æ¨¡å¼ä¸¢é”¤æ•°æ®: len={len(error_notes)}, hasattr={hasattr(self.analyzer, 'drop_hammers')}")
                if hasattr(self.analyzer, 'drop_hammers'):
                    logger.info(f"ğŸ“Š drop_hammersç±»å‹: {type(self.analyzer.drop_hammers)}")
                    logger.info(f"ğŸ“Š drop_hammerså†…å®¹: {self.analyzer.drop_hammers}")
            elif error_type == 'å¤šé”¤':
                error_notes = self.analyzer.multi_hammers if hasattr(self.analyzer, 'multi_hammers') else []
                logger.info(f"ğŸ“Š å•ç®—æ³•æ¨¡å¼å¤šé”¤æ•°æ®: len={len(error_notes)}, hasattr={hasattr(self.analyzer, 'multi_hammers')}")
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥é”™è¯¯ç±»å‹: {error_type}")
                return []

            # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®æ ¼å¼ï¼ˆå•ç®—æ³•æ¨¡å¼ä¸æ·»åŠ ç®—æ³•åç§°åˆ—ï¼‰
            table_data = []
            logger.info(f"ğŸ“Š å•ç®—æ³•æ¨¡å¼è½¬æ¢æ•°æ®: error_notesé•¿åº¦={len(error_notes)}, error_type={error_type}")

            for i, note in enumerate(error_notes):
                if len(note.infos) > 0:
                    info = note.infos[0]
                    logger.info(f"ğŸ“Š å¤„ç†ç¬¬{i+1}ä¸ª{error_type}: keyId={info.keyId}, index={info.index}, keyOn={info.keyOn/10:.2f}ms, keyOff={info.keyOff/10:.2f}ms")

                row = {
                    'data_type': 'record' if error_type == 'ä¸¢é”¤' else 'play',
                    'keyId': note.keyId if hasattr(note, 'keyId') else 'N/A',
                }

                # æ·»åŠ æ—¶é—´å’Œç´¢å¼•ä¿¡æ¯
                if error_type == 'ä¸¢é”¤':
                    row.update({
                        'keyOn': f"{note.keyOn/10:.2f}" if hasattr(note, 'keyOn') else 'N/A',
                        'keyOff': f"{note.keyOff/10:.2f}" if hasattr(note, 'keyOff') else 'N/A',
                        'index': note.index if hasattr(note, 'index') else 'N/A',
                        'analysis_reason': 'ä¸¢é”¤ï¼ˆå½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— ï¼‰'
                    })
                else:  # å¤šé”¤
                    row.update({
                        'keyOn': f"{note.keyOn/10:.2f}" if hasattr(note, 'keyOn') else 'N/A',
                        'keyOff': f"{note.keyOff/10:.2f}" if hasattr(note, 'keyOff') else 'N/A',
                        'index': note.index if hasattr(note, 'index') else 'N/A',
                        'analysis_reason': 'å¤šé”¤ï¼ˆæ’­æ”¾æœ‰ï¼Œå½•åˆ¶æ— ï¼‰'
                    })

                table_data.append(row)
                logger.info(f"ğŸ“Š æ·»åŠ è¡Œåˆ°table_data: {row}")

            logger.info(f"ğŸ“Š å•ç®—æ³•æ¨¡å¼æœ€ç»ˆè¿”å›: table_dataé•¿åº¦={len(table_data)}")
            return table_data
        
        # å•ç®—æ³•æ¨¡å¼ï¼šç›´æ¥ä»analyzerè·å–æ•°æ®
        if not self.analyzer:
            return []

        # è·å–é”™è¯¯æ•°æ®
        if error_type == 'ä¸¢é”¤':
            error_notes = self.analyzer.drop_hammers if hasattr(self.analyzer, 'drop_hammers') else []
        elif error_type == 'å¤šé”¤':
            error_notes = self.analyzer.multi_hammers if hasattr(self.analyzer, 'multi_hammers') else []
        else:
            return []

        # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®æ ¼å¼ï¼ˆå•ç®—æ³•æ¨¡å¼ä¸éœ€è¦æ·»åŠ ç®—æ³•åç§°åˆ—ï¼‰
        table_data = []
        for note in error_notes:
            row = {
                'data_type': 'record' if error_type == 'ä¸¢é”¤' else 'play',
                'keyId': note.keyId if hasattr(note, 'keyId') else 'N/A',
                'keyOn': f"{note.keyOn:.2f}" if hasattr(note, 'keyOn') and note.keyOn is not None else 'N/A',
                'keyOff': f"{note.keyOff:.2f}" if hasattr(note, 'keyOff') and note.keyOff is not None else 'N/A',
                'index': note.index if hasattr(note, 'index') else 'N/A',
                'analysis_reason': getattr(note, 'analysis_reason', 'æœªåŒ¹é…')
            }
            table_data.append(row)

        return table_data
    
    
    # ==================== å†…éƒ¨æ–¹æ³• ====================
    
    def _perform_error_analysis(self) -> None:
        """æ‰§è¡Œé”™è¯¯åˆ†æ"""
        try:
            # ç¡®ä¿analyzerå­˜åœ¨ï¼ˆå‘åå…¼å®¹ï¼Œå·²åºŸå¼ƒï¼‰
            if self.analyzer is None:
                self.analyzer = SPMIDAnalyzer()
                logger.info("âœ… é‡æ–°åˆå§‹åŒ–analyzerï¼ˆå‘åå…¼å®¹ï¼‰")

            # æ¸…é™¤ä¹‹å‰çš„ä¸€è‡´æ€§éªŒè¯çŠ¶æ€ï¼Œç¡®ä¿æ¯æ¬¡åˆ†æéƒ½ä¼šé‡æ–°éªŒè¯
            self._last_analysis_hash = None
            self._last_overview_metrics = None

            # æ‰§è¡Œåˆ†æ
            record_data = self.data_manager.get_record_data()
            replay_data = self.data_manager.get_replay_data()
            
            if not record_data or not replay_data:
                logger.error("æ•°æ®ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œé”™è¯¯åˆ†æ")
                return
            
            analysis_result = self.analyzer.analyze(record_data, replay_data)
        
            # è§£åŒ…åˆ†æç»“æœ
            self.analyzer.multi_hammers, self.analyzer.drop_hammers, self.analyzer.silent_hammers, \
            self.analyzer.valid_record_data, self.analyzer.valid_replay_data, \
            self.analyzer.invalid_notes_table_data, self.analyzer.matched_pairs = analysis_result
            
            # åŒæ­¥æ•°æ®åˆ°æ•°æ®ç®¡ç†å™¨
            self.data_manager.set_analysis_results(
                self.analyzer.valid_record_data, 
                self.analyzer.valid_replay_data
            )
            
            # åŒæ­¥åˆ†æç»“æœåˆ°å„ä¸ªæ¨¡å—
            self._sync_analysis_results()

            # æ•°æ®ä¸€è‡´æ€§éªŒè¯
            self._verify_data_consistency()

            logger.info("âœ… é”™è¯¯åˆ†æå®Œæˆ")

            # åˆå§‹åŒ–å»¶æ—¶åˆ†æå™¨ï¼ˆåœ¨åˆ†æå®Œæˆåï¼‰
            if self.analyzer:
                self.delay_analysis = DelayAnalysis(self.analyzer)

        except Exception as e:
            logger.error(f"é”™è¯¯åˆ†æå¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
    
    # ==================== å»¶æ—¶å…³ç³»åˆ†æç›¸å…³æ–¹æ³• ====================
    
    
    
    
    
    def get_force_delay_by_key_analysis(self) -> Dict[str, Any]:
        """
        è·å–æ¯ä¸ªæŒ‰é”®çš„åŠ›åº¦ä¸å»¶æ—¶å…³ç³»åˆ†æç»“æœ
        
        æŒ‰æŒ‰é”®IDåˆ†ç»„ï¼Œå¯¹æ¯ä¸ªæŒ‰é”®åˆ†æå…¶å†…éƒ¨çš„åŠ›åº¦ï¼ˆé”¤é€Ÿï¼‰ä¸å»¶æ—¶çš„ç»Ÿè®¡å…³ç³»ã€‚
        åŒ…æ‹¬ï¼šç›¸å…³æ€§åˆ†æã€å›å½’åˆ†æã€æè¿°æ€§ç»Ÿè®¡ç­‰ã€‚
        
        æ”¯æŒå¤šç®—æ³•æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„æ•°æ®ã€‚
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - key_analysis: æ¯ä¸ªæŒ‰é”®çš„åˆ†æç»“æœåˆ—è¡¨ï¼ˆå•ç®—æ³•æ¨¡å¼ï¼‰
                - overall_summary: æ•´ä½“æ‘˜è¦ç»Ÿè®¡
                - scatter_data: æ•£ç‚¹å›¾æ•°æ®ï¼ˆæŒ‰æŒ‰é”®åˆ†ç»„ï¼‰
                - algorithm_results: å¤šç®—æ³•æ¨¡å¼ä¸‹çš„å„ç®—æ³•ç»“æœï¼ˆå¤šç®—æ³•æ¨¡å¼ï¼‰
                - status: çŠ¶æ€æ ‡è¯†
        """
        try:
            # å¤šç®—æ³•æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„æ•°æ®
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if not active_algorithms:
                    logger.warning("âš ï¸ æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æ")
                    return {
                        'status': 'error',
                        'message': 'æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•'
                    }
                
                # ä¸ºæ¯ä¸ªç®—æ³•ç”Ÿæˆåˆ†æç»“æœ
                algorithm_results = {}
                for algorithm in active_algorithms:
                    if not algorithm.analyzer:
                        logger.warning(f"âš ï¸ ç®—æ³• '{algorithm.metadata.algorithm_name}' æ²¡æœ‰åˆ†æå™¨ï¼Œè·³è¿‡")
                        continue
                    
                    algorithm_name = algorithm.metadata.algorithm_name
                    delay_analysis = DelayAnalysis(algorithm.analyzer)
                    result = delay_analysis.analyze_force_delay_by_key()
                    
                    if result.get('status') == 'success':
                        algorithm_results[algorithm_name] = result
                        logger.info(f"âœ… ç®—æ³• '{algorithm_name}' çš„æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æå®Œæˆ")
                
                if not algorithm_results:
                    logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•")
                    return {
                        'status': 'error',
                        'message': 'æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•'
                    }
                
                # è¿”å›å¤šç®—æ³•ç»“æœ
                return {
                    'status': 'success',
                    'multi_algorithm_mode': True,
                    'algorithm_results': algorithm_results,
                    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹ŸåŒ…å«ç¬¬ä¸€ä¸ªç®—æ³•çš„ç»“æœ
                    'key_analysis': list(algorithm_results.values())[0].get('key_analysis', []),
                    'overall_summary': list(algorithm_results.values())[0].get('overall_summary', {}),
                    'scatter_data': list(algorithm_results.values())[0].get('scatter_data', {}),
                }
            
            # å•ç®—æ³•æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            if not self.delay_analysis:
                if self.analyzer:
                    self.delay_analysis = DelayAnalysis(self.analyzer)
                else:
                    logger.warning("âš ï¸ åˆ†æå™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æ")
                    return {
                        'status': 'error',
                        'message': 'åˆ†æå™¨ä¸å­˜åœ¨'
                    }
            
            result = self.delay_analysis.analyze_force_delay_by_key()
            logger.info("âœ… æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰é”®åŠ›åº¦-å»¶æ—¶åˆ†æå¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return {
                'status': 'error',
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def get_key_force_interaction_analysis(self) -> Dict[str, Any]:
        """
        è·å–æŒ‰é”®ä¸åŠ›åº¦çš„è”åˆç»Ÿè®¡å…³ç³»åˆ†æç»“æœï¼ˆå¤šå› ç´ åˆ†æï¼‰
        
        ä½¿ç”¨å¤šå› ç´ ANOVAå’Œäº¤äº’æ•ˆåº”åˆ†æï¼Œè¯„ä¼°ï¼š
        1. æŒ‰é”®å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        2. åŠ›åº¦å¯¹å»¶æ—¶çš„ä¸»æ•ˆåº”
        3. æŒ‰é”®Ã—åŠ›åº¦çš„äº¤äº’æ•ˆåº”
        
        æ”¯æŒå¤šç®—æ³•æ¨¡å¼ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿€æ´»ç®—æ³•çš„æ•°æ®è¿›è¡Œåˆ†æã€‚
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - two_way_anova: åŒå› ç´ ANOVAç»“æœ
                - interaction_effect: äº¤äº’æ•ˆåº”åˆ†æç»“æœ
                - stratified_regression: åˆ†å±‚å›å½’åˆ†æç»“æœ
                - interaction_plot_data: äº¤äº’æ•ˆåº”å›¾æ•°æ®
                - status: çŠ¶æ€æ ‡è¯†
        """
        try:
            # å¤šç®—æ³•æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„æ•°æ®
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if not active_algorithms:
                    logger.warning("âš ï¸ æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æ")
                    return {
                        'status': 'error',
                        'message': 'æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•'
                    }
                
                # ä¸ºæ¯ä¸ªç®—æ³•ç”Ÿæˆåˆ†æç»“æœ
                # ä½¿ç”¨å†…éƒ¨çš„algorithm_nameä½œä¸ºkeyï¼ˆå”¯ä¸€æ ‡è¯†ï¼ŒåŒ…å«æ–‡ä»¶åï¼‰ï¼Œé¿å…åŒç§ç®—æ³•ä¸åŒæ›²å­è¢«è¦†ç›–
                algorithm_results = {}
                for algorithm in active_algorithms:
                    if not algorithm.analyzer:
                        logger.warning(f"âš ï¸ ç®—æ³• '{algorithm.metadata.algorithm_name}' æ²¡æœ‰åˆ†æå™¨ï¼Œè·³è¿‡")
                        continue
                    
                    # ä½¿ç”¨å†…éƒ¨çš„algorithm_nameä½œä¸ºkeyï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
                    algorithm_name = algorithm.metadata.algorithm_name
                    display_name = algorithm.metadata.display_name
                    delay_analysis = DelayAnalysis(algorithm.analyzer)
                    result = delay_analysis.analyze_key_force_interaction()
                    
                    if result.get('status') == 'success':
                        # åœ¨resultä¸­æ·»åŠ display_nameï¼Œç”¨äºUIæ˜¾ç¤º
                        result['display_name'] = display_name
                        algorithm_results[algorithm_name] = result
                        logger.info(f"âœ… ç®—æ³• '{display_name}' (å†…éƒ¨: {algorithm_name}) çš„æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆ")
                
                if not algorithm_results:
                    logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•")
                    return {
                        'status': 'error',
                        'message': 'æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•'
                    }
                
                # è¿”å›å¤šç®—æ³•ç»“æœ
                return {
                    'status': 'success',
                    'multi_algorithm_mode': True,
                    'algorithm_results': algorithm_results,
                    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹ŸåŒ…å«ç¬¬ä¸€ä¸ªç®—æ³•çš„ç»“æœï¼ˆç”¨äºäº¤äº’æ•ˆåº”å¼ºåº¦ç­‰ç»Ÿè®¡ä¿¡æ¯ï¼‰
                    'two_way_anova': list(algorithm_results.values())[0].get('two_way_anova', {}),
                    'interaction_effect': list(algorithm_results.values())[0].get('interaction_effect', {}),
                    'stratified_regression': list(algorithm_results.values())[0].get('stratified_regression', {}),
                }
            
            # å•ç®—æ³•æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            if not self.delay_analysis:
                if self.analyzer:
                    self.delay_analysis = DelayAnalysis(self.analyzer)
                else:
                    logger.warning("âš ï¸ åˆ†æå™¨ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æ")
                    return {
                        'status': 'error',
                        'message': 'åˆ†æå™¨ä¸å­˜åœ¨'
                    }
            
            result = self.delay_analysis.analyze_key_force_interaction()
            logger.info("âœ… æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return {
                'status': 'error',
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    
    def generate_key_force_interaction_plot(self) -> Any:
        """
        ç”ŸæˆæŒ‰é”®-åŠ›åº¦äº¤äº’æ•ˆåº”å›¾
        
        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            analysis_result = self.get_key_force_interaction_analysis()
            
            if analysis_result.get('status') != 'success':
                return self.plot_generator._create_empty_plot("åˆ†æå¤±è´¥")
            
            return self.plot_generator.generate_key_force_interaction_plot(analysis_result)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆäº¤äº’æ•ˆåº”å›¾å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    # ==================== å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼ç›¸å…³æ–¹æ³• ====================
    
    def _ensure_multi_algorithm_manager(self, max_algorithms: Optional[int] = None) -> None:
        """
        ç¡®ä¿multi_algorithm_managerå·²åˆå§‹åŒ–ï¼ˆå¤šç®—æ³•æ¨¡å¼å§‹ç»ˆå¯ç”¨ï¼‰
        
        Args:
            max_algorithms: æœ€å¤§ç®—æ³•æ•°é‡ï¼ˆNoneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
        """
        if self.multi_algorithm_manager is None:
            self.multi_algorithm_manager = MultiAlgorithmManager(max_algorithms=max_algorithms)
            limit_text = "æ— é™åˆ¶" if max_algorithms is None else str(max_algorithms)
            logger.info(f"âœ… åˆå§‹åŒ–å¤šç®—æ³•ç®¡ç†å™¨ (æœ€å¤§ç®—æ³•æ•°: {limit_text})")
    
    def enable_multi_algorithm_mode(self, max_algorithms: Optional[int] = None) -> Tuple[bool, bool, Optional[str]]:
        """
        å¯ç”¨å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼Œç°åœ¨åªæ˜¯ç¡®ä¿ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼‰
        
        Args:
            max_algorithms: æœ€å¤§ç®—æ³•æ•°é‡ï¼ˆNoneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
            
        Returns:
            Tuple[bool, bool, Optional[str]]: (æ˜¯å¦æˆåŠŸ, æ˜¯å¦æœ‰ç°æœ‰æ•°æ®éœ€è¦è¿ç§», æ–‡ä»¶å)
        """
        try:
            self._ensure_multi_algorithm_manager(max_algorithms)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„åˆ†ææ•°æ®éœ€è¦è¿ç§»
            has_existing_data = False
            existing_filename = None
            
            if self.analyzer and self.analyzer.note_matcher and hasattr(self.analyzer, 'matched_pairs') and len(self.analyzer.matched_pairs) > 0:
                # æœ‰å·²åˆ†æçš„æ•°æ®
                has_existing_data = True
                # è·å–æ–‡ä»¶å
                data_source_info = self.get_data_source_info()
                existing_filename = data_source_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                logger.info(f"æ£€æµ‹åˆ°ç°æœ‰åˆ†ææ•°æ®ï¼Œæ–‡ä»¶å: {existing_filename}")
            
            return True, has_existing_data, existing_filename
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤šç®—æ³•ç®¡ç†å™¨å¤±è´¥: {e}")
            return False, False, None
    
    def migrate_existing_data_to_algorithm(self, algorithm_name: str) -> Tuple[bool, str]:
        """
        å°†ç°æœ‰çš„å•ç®—æ³•åˆ†ææ•°æ®è¿ç§»åˆ°å¤šç®—æ³•æ¨¡å¼
        
        Args:
            algorithm_name: ç”¨æˆ·æŒ‡å®šçš„ç®—æ³•åç§°
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        if not self.multi_algorithm_manager:
            self._ensure_multi_algorithm_manager()
        
        if not self.analyzer or not self.analyzer.note_matcher:
            return False, "æ²¡æœ‰å¯è¿ç§»çš„åˆ†ææ•°æ®"
        
        try:
            # è·å–åŸå§‹æ•°æ®
            record_data = self.data_manager.get_record_data()
            replay_data = self.data_manager.get_replay_data()
            
            if not record_data or not replay_data:
                return False, "åŸå§‹æ•°æ®ä¸å­˜åœ¨"
            
            # è·å–æ–‡ä»¶å
            data_source_info = self.get_data_source_info()
            filename = data_source_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            
            # ç”Ÿæˆå”¯ä¸€çš„ç®—æ³•åç§°ï¼ˆç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
            unique_algorithm_name = self.multi_algorithm_manager._generate_unique_algorithm_name(algorithm_name, filename)
            
            # åˆ›å»ºç®—æ³•æ•°æ®é›†ï¼ˆä½¿ç”¨å”¯ä¸€åç§°ä½œä¸ºå†…éƒ¨æ ‡è¯†ï¼ŒåŸå§‹åç§°ä½œä¸ºæ˜¾ç¤ºåç§°ï¼‰
            color_index = 0
            algorithm = AlgorithmDataset(unique_algorithm_name, algorithm_name, filename, color_index)
            
            # ç›´æ¥ä½¿ç”¨ç°æœ‰çš„ analyzerï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†æ
            algorithm.analyzer = self.analyzer
            algorithm.record_data = record_data
            algorithm.replay_data = replay_data

            # ç¡®ä¿é”™è¯¯æ•°æ®ä¹ŸåŒæ­¥åˆ°analyzerå¯¹è±¡
            # ä»table_generatorè·å–å½“å‰çš„é”™è¯¯æ•°æ®å¹¶åŒæ­¥
            if hasattr(self, 'table_generator') and self.table_generator:
                analyzer = self.table_generator.analyzer
                if analyzer and hasattr(analyzer, 'drop_hammers') and hasattr(analyzer, 'multi_hammers'):
                    algorithm.analyzer.drop_hammers = analyzer.drop_hammers
                    algorithm.analyzer.multi_hammers = analyzer.multi_hammers
                    algorithm.analyzer.silent_hammers = getattr(analyzer, 'silent_hammers', [])
                    logger.info(f"âœ… è¿ç§»æ—¶åŒæ­¥é”™è¯¯æ•°æ®: ä¸¢é”¤={len(algorithm.analyzer.drop_hammers)}, å¤šé”¤={len(algorithm.analyzer.multi_hammers)}")

            algorithm.metadata.status = AlgorithmStatus.READY

            # æ·»åŠ åˆ°ç®¡ç†å™¨
            self.multi_algorithm_manager.algorithms[unique_algorithm_name] = algorithm
            
            logger.info(f"âœ… ç°æœ‰æ•°æ®å·²è¿ç§»ä¸ºç®—æ³•: {algorithm_name}")
            return True, ""
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return False, str(e)
    
    # disable_multi_algorithm_mode æ–¹æ³•å·²ç§»é™¤ - ä¸å†æ”¯æŒå•ç®—æ³•æ¨¡å¼
    
    def is_multi_algorithm_mode(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºå¤šç®—æ³•å¯¹æ¯”æ¨¡å¼ï¼ˆå§‹ç»ˆè¿”å›Trueï¼‰"""
        return True

    def get_current_analysis_mode(self) -> Tuple[str, int]:
        """
        è·å–å½“å‰çš„åˆ†ææ¨¡å¼å’Œæ´»è·ƒç®—æ³•æ•°é‡

        Returns:
            Tuple[str, int]: (æ¨¡å¼åç§°, æ´»è·ƒç®—æ³•æ•°é‡)
                           æ¨¡å¼: "multi" (å¤šç®—æ³•), "single" (å•ç®—æ³•), "none" (æ— æ•°æ®)
        """
        # æ£€æŸ¥æ´»è·ƒçš„å¤šç®—æ³•
        active_algorithms = []
        if self.multi_algorithm_manager:
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()

        if active_algorithms:
            # æœ‰æ´»è·ƒçš„å¤šç®—æ³•
            return "multi", len(active_algorithms)
        elif self.analyzer:
            # æ²¡æœ‰æ´»è·ƒçš„å¤šç®—æ³•ï¼Œä½†æœ‰å•ç®—æ³•åˆ†æå™¨
            return "single", 1
        else:
            # ä¸¤è€…éƒ½æ²¡æœ‰
            return "none", 0

    def has_active_multi_algorithm_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„å¤šç®—æ³•æ•°æ®"""
        if self.multi_algorithm_manager:
            active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
            return len(active_algorithms) > 0
        return False

    def has_single_algorithm_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å•ç®—æ³•æ•°æ®"""
        return self.analyzer is not None
    
    async def add_algorithm(self, algorithm_name: str, filename: str, 
                           contents: str) -> Tuple[bool, str]:
        """
        æ·»åŠ ç®—æ³•åˆ°å¤šç®—æ³•ç®¡ç†å™¨ï¼ˆå¼‚æ­¥ï¼‰
        
        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆç”¨æˆ·æŒ‡å®šï¼‰
            filename: æ–‡ä»¶å
            contents: æ–‡ä»¶å†…å®¹ï¼ˆbase64ç¼–ç ï¼‰
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        if not self.multi_algorithm_manager:
            self._ensure_multi_algorithm_manager()
        
        try:
            # è§£ç æ–‡ä»¶å†…å®¹
            import base64
            decoded_bytes = base64.b64decode(contents.split(',')[1] if ',' in contents else contents)
            
            # åŠ è½½SPMIDæ•°æ®
            from .spmid_loader import SPMIDLoader
            loader = SPMIDLoader()
            success = loader.load_spmid_data(decoded_bytes)
            
            if not success:
                return False, "SPMIDæ–‡ä»¶è§£æå¤±è´¥"
            
            # è·å–æ•°æ®
            record_data = loader.get_record_data()
            replay_data = loader.get_replay_data()
            
            if not record_data or not replay_data:
                return False, "æ•°æ®ä¸ºç©º"
            
            # å¼‚æ­¥æ·»åŠ ç®—æ³•
            success, error_msg = await self.multi_algorithm_manager.add_algorithm_async(
                algorithm_name, filename, record_data, replay_data
            )
            
            return success, error_msg
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç®—æ³•å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return False, str(e)
    
    def remove_algorithm(self, algorithm_name: str) -> bool:
        """
        ä»å¤šç®—æ³•ç®¡ç†å™¨ä¸­ç§»é™¤ç®—æ³•
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return False
        
        return self.multi_algorithm_manager.remove_algorithm(algorithm_name)
    
    def get_all_algorithms(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰ç®—æ³•çš„ä¿¡æ¯åˆ—è¡¨
        
        Returns:
            List[Dict[str, Any]]: ç®—æ³•ä¿¡æ¯åˆ—è¡¨
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return []
        
        algorithms = []
        for alg in self.multi_algorithm_manager.get_all_algorithms():
            # ç¡®ä¿is_activeæœ‰å€¼ï¼Œå¦‚æœä¸ºNoneåˆ™é»˜è®¤ä¸ºTrueï¼ˆæ–°ä¸Šä¼ çš„æ–‡ä»¶åº”è¯¥é»˜è®¤æ˜¾ç¤ºï¼‰
            is_active = alg.is_active if alg.is_active is not None else True
            if alg.is_active is None:
                alg.is_active = True
                logger.info(f"âœ… ç¡®ä¿ç®—æ³• '{alg.metadata.algorithm_name}' é»˜è®¤æ˜¾ç¤º: is_active={is_active}")
            
            algorithms.append({
                'algorithm_name': alg.metadata.algorithm_name,  # å†…éƒ¨å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºæŸ¥æ‰¾ï¼‰
                'display_name': alg.metadata.display_name,  # æ˜¾ç¤ºåç§°ï¼ˆç”¨äºUIæ˜¾ç¤ºï¼‰
                'filename': alg.metadata.filename,
                'status': alg.metadata.status.value,
                'is_active': is_active,
                'color': alg.color,
                'is_ready': alg.is_ready()
            })
        
        return algorithms
    
    def get_key_matched_pairs_by_algorithm(self, algorithm_name: str, key_id: int) -> List[Tuple[int, int, Any, Any, float]]:
        """
        è·å–æŒ‡å®šç®—æ³•å’ŒæŒ‰é”®IDçš„æ‰€æœ‰åŒ¹é…å¯¹ï¼ŒæŒ‰æ—¶é—´æˆ³æ’åº
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            key_id: æŒ‰é”®ID
            
        Returns:
            List[Tuple[int, int, Note, Note, float]]: åŒ¹é…å¯¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(record_index, replay_index, record_note, replay_note, record_keyon)
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return []
        
        # æ ¹æ® display_name æŸ¥æ‰¾ç®—æ³•
        algorithm = None
        for alg in self.multi_algorithm_manager.get_all_algorithms():
            if alg.metadata.display_name == algorithm_name:
                algorithm = alg
                break

        if not algorithm or not algorithm.is_ready() or not algorithm.analyzer:
            return []
        
        matched_pairs = algorithm.analyzer.matched_pairs if hasattr(algorithm.analyzer, 'matched_pairs') else []
        if not matched_pairs:
            return []
        
        # è·å–åç§»å¯¹é½æ•°æ®ä»¥è·å–æ—¶é—´æˆ³
        offset_data = algorithm.analyzer.get_offset_alignment_data()
        offset_map = {}
        for item in offset_data:
            record_idx = item.get('record_index')
            replay_idx = item.get('replay_index')
            if record_idx is not None and replay_idx is not None:
                offset_map[(record_idx, replay_idx)] = item.get('record_keyon', 0)
        
        # ç­›é€‰æŒ‡å®šæŒ‰é”®IDçš„åŒ¹é…å¯¹
        key_pairs = []
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            if record_note.id == key_id:
                record_keyon = offset_map.get((record_idx, replay_idx), 0)
                key_pairs.append((record_idx, replay_idx, record_note, replay_note, record_keyon))
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        key_pairs.sort(key=lambda x: x[4])
        
        return key_pairs
    
    def get_active_algorithms(self) -> List[AlgorithmDataset]:
        """
        è·å–æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯”æ˜¾ç¤ºï¼‰
        
        Returns:
            List[AlgorithmDataset]: æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return []
        
        return self.multi_algorithm_manager.get_active_algorithms()
    
    def toggle_algorithm(self, algorithm_name: str) -> bool:
        """
        åˆ‡æ¢ç®—æ³•çš„æ˜¾ç¤º/éšè—çŠ¶æ€
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return False
        
        return self.multi_algorithm_manager.toggle_algorithm(algorithm_name)
    
    def rename_algorithm(self, old_name: str, new_name: str) -> bool:
        """
        é‡å‘½åç®—æ³•
        
        Args:
            old_name: æ—§åç§°
            new_name: æ–°åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return False
        
        return self.multi_algorithm_manager.rename_algorithm(old_name, new_name)
    
    def get_multi_algorithm_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤šç®—æ³•å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
            return {}
        
        return self.multi_algorithm_manager.get_comparison_statistics()
    
    def get_same_algorithm_relative_delay_analysis(self) -> Dict[str, Any]:
        """
        åˆ†æåŒç§ç®—æ³•ä¸åŒæ›²å­çš„ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒ
        
        è¯†åˆ«é€»è¾‘ï¼š
        - display_name ç›¸åŒï¼šè¡¨ç¤ºåŒç§ç®—æ³•
        - algorithm_name ä¸åŒï¼šè¡¨ç¤ºä¸åŒæ›²å­ï¼ˆå› ä¸ºæ–‡ä»¶åä¸åŒï¼‰
        
        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - status: çŠ¶æ€æ ‡è¯†
                - algorithm_groups: æŒ‰ç®—æ³•åˆ†ç»„çš„ç›¸å¯¹å»¶æ—¶æ•°æ®
                - overall_relative_delays: æ‰€æœ‰æ›²å­åˆå¹¶åçš„ç›¸å¯¹å»¶æ—¶åˆ—è¡¨
                - statistics: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
                return {
                    'status': 'error',
                    'message': 'æœªå¯ç”¨å¤šç®—æ³•æ¨¡å¼'
                }
            
            all_algorithms = self.multi_algorithm_manager.get_all_algorithms()
            if not all_algorithms:
                return {
                    'status': 'error',
                    'message': 'æ²¡æœ‰ç®—æ³•æ•°æ®'
                }
            
            # æŒ‰display_nameåˆ†ç»„ï¼Œè¯†åˆ«åŒç§ç®—æ³•
            from collections import defaultdict
            algorithm_groups = defaultdict(list)
            
            for algorithm in all_algorithms:
                if not algorithm.is_ready():
                    continue
                
                display_name = algorithm.metadata.display_name
                algorithm_groups[display_name].append(algorithm)
            
            # æ‰¾å‡ºæœ‰å¤šä¸ªæ›²å­çš„ç®—æ³•ï¼ˆåŒç§ç®—æ³•çš„ä¸åŒæ›²å­ï¼‰
            same_algorithm_groups = {}
            for display_name, algorithms in algorithm_groups.items():
                if len(algorithms) > 1:
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä¸åŒæ›²å­ï¼ˆalgorithm_nameä¸åŒï¼‰
                    algorithm_names = set(alg.metadata.algorithm_name for alg in algorithms)
                    if len(algorithm_names) > 1:
                        same_algorithm_groups[display_name] = algorithms
            
            if not same_algorithm_groups:
                return {
                    'status': 'error',
                    'message': 'æœªæ‰¾åˆ°åŒç§ç®—æ³•çš„ä¸åŒæ›²å­'
                }
            
            # åˆ†ææ¯ä¸ªç®—æ³•çš„ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒ
            result_groups = {}
            all_relative_delays = []  # åˆå¹¶æ‰€æœ‰æ›²å­çš„ç›¸å¯¹å»¶æ—¶
            
            for display_name, algorithms in same_algorithm_groups.items():
                group_relative_delays = []  # è¯¥ç®—æ³•ç»„æ‰€æœ‰æ›²å­åˆå¹¶åçš„ç›¸å¯¹å»¶æ—¶
                group_info = []
                song_data = []  # æŒ‰æ›²å­åˆ†ç»„çš„æ•°æ®
                
                for algorithm in algorithms:
                    if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                        continue
                    
                    # è·å–è¯¥æ›²å­çš„ç²¾ç¡®åç§»æ•°æ®ï¼ˆè¯¯å·® â‰¤ 50msï¼‰
                    offset_data = algorithm.analyzer.note_matcher.get_precision_offset_alignment_data()
                    if not offset_data:
                        continue
                    
                    # è®¡ç®—è¯¥æ›²å­çš„å¹³å‡å»¶æ—¶ï¼ˆå¸¦ç¬¦å·ï¼‰
                    me_0_1ms = algorithm.analyzer.note_matcher.get_mean_error()
                    mean_delay_ms = me_0_1ms / 10.0  # è½¬æ¢ä¸ºms
                    
                    # è®¡ç®—ç›¸å¯¹å»¶æ—¶å’Œé”¤é€Ÿå·®å€¼
                    relative_delays = []
                    hammer_velocity_diffs = []  # é”¤é€Ÿå·®å€¼åˆ—è¡¨
                    for item in offset_data:
                        keyon_offset_0_1ms = item.get('keyon_offset', 0.0)
                        absolute_delay_ms = keyon_offset_0_1ms / 10.0
                        relative_delay_ms = absolute_delay_ms - mean_delay_ms
                        relative_delays.append(relative_delay_ms)

                        # è·å–é”¤é€Ÿå·®å€¼ï¼ˆæ’­æ”¾é”¤é€Ÿ - å½•åˆ¶é”¤é€Ÿï¼‰
                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»åŒ¹é…å¯¹ä¸­è·å–é”¤é€Ÿä¿¡æ¯
                        record_idx = item.get('record_index')
                        replay_idx = item.get('replay_index')
                        if record_idx is not None and replay_idx is not None:
                            # æŸ¥æ‰¾åŒ¹é…å¯¹
                            matched_pairs = algorithm.analyzer.note_matcher.get_matched_pairs()
                            for r_idx, p_idx, record_note, replay_note in matched_pairs:
                                if r_idx == record_idx and p_idx == replay_idx:
                                    # ä»åŒ¹é…çš„éŸ³ç¬¦å¯¹ä¸­æå–çœŸå®çš„é”¤é€Ÿå€¼
                                    record_velocity = None
                                    replay_velocity = None

                                    # æå–å½•åˆ¶é”¤é€Ÿ
                                    if hasattr(record_note, 'hammers') and record_note.hammers is not None:
                                        if hasattr(record_note.hammers, 'empty'):
                                            if not record_note.hammers.empty:
                                                record_velocity = record_note.hammers.values[0] if hasattr(record_note.hammers, 'values') else record_note.hammers[0]
                                        elif len(record_note.hammers) > 0:
                                            record_velocity = record_note.hammers.values[0] if hasattr(record_note.hammers, 'values') else record_note.hammers[0]

                                    # æå–æ’­æ”¾é”¤é€Ÿ
                                    if hasattr(replay_note, 'hammers') and replay_note.hammers is not None:
                                        if hasattr(replay_note.hammers, 'empty'):
                                            if not replay_note.hammers.empty:
                                                replay_velocity = replay_note.hammers.values[0] if hasattr(replay_note.hammers, 'values') else replay_note.hammers[0]
                                        elif len(replay_note.hammers) > 0:
                                            replay_velocity = replay_note.hammers.values[0] if hasattr(replay_note.hammers, 'values') else replay_note.hammers[0]

                                    # åªæœ‰å½“ä¸¤ä¸ªé”¤é€Ÿéƒ½æœ‰æ•ˆæ—¶æ‰æ·»åŠ æ•°æ®
                                    if record_velocity is not None and replay_velocity is not None:
                                        velocity_diff = replay_velocity - record_velocity
                                        hammer_velocity_diffs.append({
                                            'key_id': record_note.id,
                                            'record_velocity': record_velocity,
                                            'replay_velocity': replay_velocity,
                                            'velocity_diff': velocity_diff
                                        })
                                    break
                    
                    if relative_delays:
                        group_relative_delays.extend(relative_delays)
                        all_relative_delays.extend(relative_delays)
                        
                        # ä»algorithm_nameä¸­æå–æ–‡ä»¶åéƒ¨åˆ†
                        filename_display = algorithm.metadata.filename
                        if '_' in algorithm.metadata.algorithm_name:
                            parts = algorithm.metadata.algorithm_name.rsplit('_', 1)
                            if len(parts) == 2:
                                filename_display = parts[1]
                        
                        group_info.append({
                            'algorithm_name': algorithm.metadata.algorithm_name,
                            'filename': algorithm.metadata.filename,
                            'filename_display': filename_display,
                            'mean_delay_ms': mean_delay_ms,
                            'relative_delay_count': len(relative_delays)
                        })
                        
                        # ä¿å­˜è¯¥æ›²å­çš„å•ç‹¬æ•°æ®
                        song_data.append({
                            'algorithm_name': algorithm.metadata.algorithm_name,
                            'filename': algorithm.metadata.filename,
                            'filename_display': filename_display,
                            'mean_delay_ms': mean_delay_ms,
                            'relative_delays': relative_delays,  # è¯¥æ›²å­çš„ç›¸å¯¹å»¶æ—¶åˆ—è¡¨
                            'relative_delay_count': len(relative_delays),
                            'hammer_velocity_diffs': hammer_velocity_diffs  # è¯¥æ›²å­çš„é”¤é€Ÿå·®å€¼åˆ—è¡¨
                        })
                
                if group_relative_delays:
                    result_groups[display_name] = {
                        'relative_delays': group_relative_delays,  # åˆå¹¶åçš„ç›¸å¯¹å»¶æ—¶
                        'algorithms': group_info,  # ç®—æ³•ä¿¡æ¯
                        'song_data': song_data  # æŒ‰æ›²å­åˆ†ç»„çš„æ•°æ®
                    }
            
            if not all_relative_delays:
                return {
                    'status': 'error',
                    'message': 'æ²¡æœ‰æœ‰æ•ˆçš„ç›¸å¯¹å»¶æ—¶æ•°æ®'
                }
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            relative_delays_array = np.array(all_relative_delays)
            
            statistics = {
                'mean': float(np.mean(relative_delays_array)),
                'std': float(np.std(relative_delays_array)),
                'median': float(np.median(relative_delays_array)),
                'min': float(np.min(relative_delays_array)),
                'max': float(np.max(relative_delays_array)),
                'q25': float(np.percentile(relative_delays_array, 25)),
                'q75': float(np.percentile(relative_delays_array, 75)),
                'iqr': float(np.percentile(relative_delays_array, 75) - np.percentile(relative_delays_array, 25)),
                'count': len(all_relative_delays),
                'cv': float(np.std(relative_delays_array) / abs(np.mean(relative_delays_array))) if np.mean(relative_delays_array) != 0 else float('inf')
            }
            
            # è®¡ç®—Â±1Ïƒã€Â±2Ïƒã€Â±3ÏƒèŒƒå›´å†…çš„æ•°æ®å æ¯”
            std = statistics['std']
            mean = statistics['mean']
            within_1sigma = np.sum(np.abs(relative_delays_array - mean) <= std) / len(relative_delays_array) * 100
            within_2sigma = np.sum(np.abs(relative_delays_array - mean) <= 2 * std) / len(relative_delays_array) * 100
            within_3sigma = np.sum(np.abs(relative_delays_array - mean) <= 3 * std) / len(relative_delays_array) * 100
            
            statistics['within_1sigma_percent'] = float(within_1sigma)
            statistics['within_2sigma_percent'] = float(within_2sigma)
            statistics['within_3sigma_percent'] = float(within_3sigma)
            
            logger.info(f"âœ… åŒç§ç®—æ³•ç›¸å¯¹å»¶æ—¶åˆ†æå®Œæˆï¼Œå…± {len(same_algorithm_groups)} ä¸ªç®—æ³•ç»„ï¼Œ{len(all_relative_delays)} ä¸ªæ•°æ®ç‚¹")
            
            return {
                'status': 'success',
                'algorithm_groups': result_groups,
                'overall_relative_delays': all_relative_delays,
                'statistics': statistics
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æåŒç§ç®—æ³•ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒå¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return {
                'status': 'error',
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def get_relative_delay_range_data_points_by_subplot(
        self, 
        display_name: str, 
        filename_display: str, 
        relative_delay_min_ms: float, 
        relative_delay_max_ms: float
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å­å›¾ä¿¡æ¯è·å–æŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹è¯¦æƒ…
        
        Args:
            display_name: ç®—æ³•æ˜¾ç¤ºåç§°ï¼ˆç”¨äºè¯†åˆ«ç®—æ³•ç»„ï¼‰
            filename_display: æ–‡ä»¶åæ˜¾ç¤ºï¼ˆ'æ±‡æ€»' æˆ–å…·ä½“æ–‡ä»¶åï¼‰
            relative_delay_min_ms: æœ€å°ç›¸å¯¹å»¶æ—¶å€¼ï¼ˆmsï¼‰
            relative_delay_max_ms: æœ€å¤§ç›¸å¯¹å»¶æ—¶å€¼ï¼ˆmsï¼‰
            
        Returns:
            List[Dict[str, Any]]: è¯¥ç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹åˆ—è¡¨
        """
        try:
            if not self.multi_algorithm_mode or not self.multi_algorithm_manager:
                return []
            
            all_algorithms = self.multi_algorithm_manager.get_all_algorithms()
            filtered_data = []
            
            for algorithm in all_algorithms:
                if not algorithm.is_ready():
                    continue
                
                # æ£€æŸ¥ç®—æ³•æ˜¯å¦å±äºæŒ‡å®šçš„display_nameç»„
                if algorithm.metadata.display_name != display_name:
                    continue
                
                # å¦‚æœæ˜¯æ±‡æ€»å›¾ï¼ŒåŒ…å«è¯¥ç»„æ‰€æœ‰ç®—æ³•ï¼›å¦åˆ™åªåŒ…å«æŒ‡å®šæ–‡ä»¶åçš„ç®—æ³•
                if filename_display != 'æ±‡æ€»':
                    # ä»algorithm_nameä¸­æå–æ–‡ä»¶åéƒ¨åˆ†
                    algorithm_filename = algorithm.metadata.filename
                    if '_' in algorithm.metadata.algorithm_name:
                        parts = algorithm.metadata.algorithm_name.rsplit('_', 1)
                        if len(parts) == 2:
                            algorithm_filename = parts[1]
                    
                    if algorithm_filename != filename_display:
                        continue
                
                if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                    continue
                
                # è·å–åç§»æ•°æ®
                offset_data = algorithm.analyzer.get_offset_alignment_data()
                if not offset_data:
                    continue
                
                algorithm_name = algorithm.metadata.algorithm_name
                
                # è®¡ç®—è¯¥ç®—æ³•çš„å¹³å‡å»¶æ—¶ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼‰
                absolute_delays_ms = [item.get('keyon_offset', 0.0) / 10.0 for item in offset_data]
                if not absolute_delays_ms:
                    continue
                
                mean_delay_ms = sum(absolute_delays_ms) / len(absolute_delays_ms)
                
                # ç­›é€‰å‡ºæŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹
                for item in offset_data:
                    absolute_delay_ms = item.get('keyon_offset', 0.0) / 10.0
                    relative_delay_ms = absolute_delay_ms - mean_delay_ms
                    
                    if relative_delay_min_ms <= relative_delay_ms <= relative_delay_max_ms:
                        item_copy = item.copy()
                        item_copy['absolute_delay_ms'] = absolute_delay_ms
                        item_copy['relative_delay_ms'] = relative_delay_ms
                        item_copy['delay_ms'] = relative_delay_ms  # ä¿æŒå…¼å®¹æ€§
                        item_copy['algorithm_name'] = algorithm_name
                        filtered_data.append(item_copy)
            
            return filtered_data
            
        except Exception as e:
            logger.error(f"è·å–ç›¸å¯¹å»¶æ—¶èŒƒå›´æ•°æ®ç‚¹å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return []
    
    def generate_relative_delay_distribution_plot(self) -> Any:
        """
        ç”ŸæˆåŒç§ç®—æ³•ä¸åŒæ›²å­çš„ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒå›¾
        
        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            analysis_result = self.get_same_algorithm_relative_delay_analysis()
            
            if analysis_result.get('status') != 'success':
                return self.plot_generator._create_empty_plot(
                    analysis_result.get('message', 'åˆ†æå¤±è´¥')
                )
            
            # ä½¿ç”¨å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
            return self.multi_algorithm_plot_generator.generate_relative_delay_distribution_plot(analysis_result)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç›¸å¯¹å»¶æ—¶åˆ†å¸ƒå›¾å¤±è´¥: {e}")

            logger.error(traceback.format_exc())
            return self.plot_generator._create_empty_plot(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    # ==================== æ•°æ®ä¸€è‡´æ€§éªŒè¯ç›¸å…³æ–¹æ³• ====================

    def _verify_data_consistency(self) -> None:
        """
        éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡ºï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆæŒ‡æ ‡çš„å…·ä½“å¯¹æ¯”

        è¿™ä¸ªæ–¹æ³•ä¼šè®¡ç®—å…³é”®æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œå¹¶åœ¨é‡å¤åˆ†ææ—¶è¿›è¡Œæ¯”è¾ƒï¼Œ
        ä»¥ç¡®ä¿æ•°æ®å¤„ç†è¿‡ç¨‹çš„ç¡®å®šæ€§ã€‚
        """
        try:
            # è®¡ç®—å½“å‰åˆ†æç»“æœçš„å“ˆå¸Œå€¼å’ŒæŒ‡æ ‡
            current_hash = self._calculate_analysis_hash()
            current_metrics = self._calculate_overview_metrics()

            # è·å–ä¹‹å‰ä¿å­˜çš„å“ˆå¸Œå€¼å’ŒæŒ‡æ ‡
            previous_hash = getattr(self, '_last_analysis_hash', None)
            previous_metrics = getattr(self, '_last_overview_metrics', None)

            if previous_hash is not None and previous_metrics is not None:
                if current_hash == previous_hash:
                    logger.info("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡ï¼šç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º")
                    logger.info(f"ğŸ“Š æ•°æ®æ¦‚è§ˆæŒ‡æ ‡éªŒè¯: å‡†ç¡®ç‡={current_metrics.get('accuracy_percent', 'N/A')}%, "
                              f"ä¸¢é”¤æ•°={current_metrics.get('drop_hammers_count', 'N/A')}, "
                              f"å¤šé”¤æ•°={current_metrics.get('multi_hammers_count', 'N/A')}, "
                              f"å·²é…å¯¹æ•°={current_metrics.get('matched_pairs_count', 'N/A')}")
                else:
                    logger.warning("âš ï¸ æ•°æ®ä¸€è‡´æ€§è­¦å‘Šï¼šç›¸åŒè¾“å…¥äº§ç”Ÿäº†ä¸åŒè¾“å‡ºï¼")
                    logger.warning(f"  ä¹‹å‰çš„å“ˆå¸Œå€¼: {previous_hash}")
                    logger.warning(f"  å½“å‰çš„å“ˆå¸Œå€¼: {current_hash}")

                    # å¯¹æ¯”å…·ä½“æŒ‡æ ‡
                    self._log_metrics_comparison(previous_metrics, current_metrics)
            else:
                logger.info(f"ğŸ“ é¦–æ¬¡åˆ†æï¼Œè®°å½•æ•°æ®å“ˆå¸Œå€¼: {current_hash}")
                logger.info(f"ğŸ“Š è®°å½•æ•°æ®æ¦‚è§ˆæŒ‡æ ‡: å‡†ç¡®ç‡={current_metrics.get('accuracy_percent', 'N/A')}%, "
                          f"ä¸¢é”¤æ•°={current_metrics.get('drop_hammers_count', 'N/A')}, "
                          f"å¤šé”¤æ•°={current_metrics.get('multi_hammers_count', 'N/A')}, "
                          f"å·²é…å¯¹æ•°={current_metrics.get('matched_pairs_count', 'N/A')}")

            # ä¿å­˜å½“å‰å“ˆå¸Œå€¼å’ŒæŒ‡æ ‡ä¾›ä¸‹æ¬¡æ¯”è¾ƒ
            self._last_analysis_hash = current_hash
            self._last_overview_metrics = current_metrics

        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")
            # ä¸å½±å“æ­£å¸¸åŠŸèƒ½ï¼Œåªæ˜¯è­¦å‘Š

    def _log_metrics_comparison(self, previous_metrics: Dict[str, Any], current_metrics: Dict[str, Any]) -> None:
        """
        è®°å½•æŒ‡æ ‡å¯¹æ¯”ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•ä¸ä¸€è‡´é—®é¢˜

        Args:
            previous_metrics: ä¹‹å‰çš„æŒ‡æ ‡æ•°æ®
            current_metrics: å½“å‰çš„æŒ‡æ ‡æ•°æ®
        """
        try:
            logger.warning("ğŸ” æ•°æ®æ¦‚è§ˆæŒ‡æ ‡å¯¹æ¯”:")

            metrics_to_compare = [
                ('accuracy_percent', 'å‡†ç¡®ç‡(%)'),
                ('drop_hammers_count', 'ä¸¢é”¤æ•°'),
                ('multi_hammers_count', 'å¤šé”¤æ•°'),
                ('matched_pairs_count', 'å·²é…å¯¹éŸ³ç¬¦æ•°'),
                ('total_valid_record', 'æœ‰æ•ˆå½•åˆ¶éŸ³ç¬¦æ•°'),
                ('total_valid_replay', 'æœ‰æ•ˆæ’­æ”¾éŸ³ç¬¦æ•°'),
                ('total_valid_combined', 'æ€»æœ‰æ•ˆéŸ³ç¬¦æ•°')
            ]

            for key, name in metrics_to_compare:
                prev_val = previous_metrics.get(key, 'N/A')
                curr_val = current_metrics.get(key, 'N/A')
                if prev_val != curr_val:
                    logger.warning(f"  âŒ {name}: {prev_val} â†’ {curr_val} (ä¸ä¸€è‡´ï¼)")
                else:
                    logger.info(f"  âœ… {name}: {curr_val} (ä¸€è‡´)")

        except Exception as e:
            logger.warning(f"è®°å½•æŒ‡æ ‡å¯¹æ¯”å¤±è´¥: {e}")

    def _calculate_analysis_hash(self) -> str:
        """
        è®¡ç®—åˆ†æç»“æœçš„å“ˆå¸Œå€¼ï¼Œç”¨äºä¸€è‡´æ€§éªŒè¯ï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆæŒ‡æ ‡

        Returns:
            str: åˆ†æç»“æœçš„SHA256å“ˆå¸Œå€¼
        """
        try:
            # è®¡ç®—æ•°æ®æ¦‚è§ˆæŒ‡æ ‡
            overview_metrics = self._calculate_overview_metrics()

            # æ”¶é›†å…³é”®æ•°æ®ç”¨äºå“ˆå¸Œè®¡ç®—
            hash_data = {
                'overview_metrics': overview_metrics,
                'matched_pairs_count': len(getattr(self.analyzer, 'matched_pairs', [])),
                'valid_record_count': len(getattr(self.analyzer, 'valid_record_data', [])),
                'valid_replay_count': len(getattr(self.analyzer, 'valid_replay_data', [])),
                'multi_hammers_count': len(getattr(self.analyzer, 'multi_hammers', [])),
                'drop_hammers_count': len(getattr(self.analyzer, 'drop_hammers', [])),
                'silent_hammers_count': len(getattr(self.analyzer, 'silent_hammers', [])),
            }

            # æ·»åŠ matched_pairsçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self.analyzer, 'matched_pairs') and self.analyzer.matched_pairs:
                # åªå–å‰å‡ ä¸ªåŒ¹é…å¯¹çš„å…³é”®ä¿¡æ¯ï¼Œé¿å…å“ˆå¸Œè¿‡å¤§
                pairs_info = []
                for i, (r_idx, p_idx, r_note, p_note) in enumerate(self.analyzer.matched_pairs[:10]):
                    pairs_info.append({
                        'record_index': r_idx,
                        'replay_index': p_idx,
                        'record_note_id': getattr(r_note, 'id', None),
                        'replay_note_id': getattr(p_note, 'id', None)
                    })
                hash_data['matched_pairs_sample'] = pairs_info

            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶è®¡ç®—å“ˆå¸Œ
            hash_string = json.dumps(hash_data, sort_keys=True, default=str)
            return hashlib.sha256(hash_string.encode('utf-8')).hexdigest()

        except Exception as e:
            logger.warning(f"è®¡ç®—åˆ†æå“ˆå¸Œå¤±è´¥: {e}")
            return "hash_calculation_failed"

    def _calculate_overview_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—æ•°æ®æ¦‚è§ˆä¸­çš„å…³é”®æŒ‡æ ‡ï¼Œç”¨äºä¸€è‡´æ€§éªŒè¯

        Returns:
            Dict[str, Any]: åŒ…å«æ•°æ®æ¦‚è§ˆæŒ‡æ ‡çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨ä¸UIç›¸åŒçš„è®¡ç®—é€»è¾‘
            initial_valid_record = getattr(self.analyzer, 'initial_valid_record_data', None)
            initial_valid_replay = getattr(self.analyzer, 'initial_valid_replay_data', None)

            total_valid_record = len(initial_valid_record) if initial_valid_record else 0
            total_valid_replay = len(initial_valid_replay) if initial_valid_replay else 0

            matched_pairs = getattr(self.analyzer, 'matched_pairs', [])
            drop_hammers = getattr(self.analyzer, 'drop_hammers', [])
            multi_hammers = getattr(self.analyzer, 'multi_hammers', [])

            matched_count = len(matched_pairs)
            total_valid = total_valid_record + total_valid_replay
            accuracy = (matched_count * 2 / total_valid * 100) if total_valid > 0 else 0.0

            return {
                'accuracy_percent': round(accuracy, 1),
                'drop_hammers_count': len(drop_hammers),
                'multi_hammers_count': len(multi_hammers),
                'matched_pairs_count': matched_count,
                'total_valid_record': total_valid_record,
                'total_valid_replay': total_valid_replay,
                'total_valid_combined': total_valid
            }

        except Exception as e:
            logger.warning(f"è®¡ç®—æ¦‚è§ˆæŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}

    def _log_consistency_details(self) -> None:
        """
        è®°å½•æ•°æ®ä¸ä¸€è‡´çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•
        """
        try:
            logger.warning("ğŸ” æ•°æ®ä¸ä¸€è‡´è¯¦ç»†ä¿¡æ¯:")

            # è®°å½•å…³é”®ç»Ÿè®¡ä¿¡æ¯
            analyzer = self.analyzer
            if analyzer:
                logger.warning(f"  åŒ¹é…å¯¹æ•°é‡: {len(getattr(analyzer, 'matched_pairs', []))}")
                logger.warning(f"  æœ‰æ•ˆå½•åˆ¶éŸ³ç¬¦: {len(getattr(analyzer, 'valid_record_data', []))}")
                logger.warning(f"  æœ‰æ•ˆæ’­æ”¾éŸ³ç¬¦: {len(getattr(analyzer, 'valid_replay_data', []))}")
                logger.warning(f"  å¤šé”¤é”™è¯¯: {len(getattr(analyzer, 'multi_hammers', []))}")
                logger.warning(f"  ä¸¢é”¤é”™è¯¯: {len(getattr(analyzer, 'drop_hammers', []))}")
                logger.warning(f"  é™éŸ³é”™è¯¯: {len(getattr(analyzer, 'silent_hammers', []))}")

        except Exception as e:
            logger.warning(f"è®°å½•ä¸€è‡´æ€§è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")

    def get_data_consistency_status(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®ä¸€è‡´æ€§çŠ¶æ€ä¿¡æ¯

        Returns:
            Dict[str, Any]: åŒ…å«ä¸€è‡´æ€§éªŒè¯çŠ¶æ€çš„ä¿¡æ¯
        """
        return {
            'last_analysis_hash': getattr(self, '_last_analysis_hash', None),
            'consistency_verified': hasattr(self, '_last_analysis_hash'),
            'data_source': self.data_manager.get_upload_data_source_info() if hasattr(self.data_manager, 'get_upload_data_source_info') else None
        }

    def get_graded_error_stats(self, algorithm=None) -> Dict[str, Any]:
        """
        è·å–åˆ†çº§è¯¯å·®ç»Ÿè®¡æ•°æ®

        Args:
            algorithm: æŒ‡å®šç®—æ³•ï¼ˆç”¨äºå¤šç®—æ³•æ¨¡å¼ä¸‹çš„å•ç®—æ³•æŸ¥è¯¢ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«å„çº§åˆ«è¯¯å·®ç»Ÿè®¡çš„æ•°æ®
        """
        try:
            # å¦‚æœæŒ‡å®šäº†ç®—æ³•ï¼Œä½¿ç”¨è¯¥ç®—æ³•çš„æ•°æ®
            if algorithm:
                if algorithm.analyzer and algorithm.analyzer.note_matcher:
                    return algorithm.analyzer.note_matcher.get_graded_error_stats()
                else:
                    return {'error': f'ç®—æ³• {algorithm} æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨'}

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šç®—æ³•æ¨¡å¼
            if self.multi_algorithm_mode and self.multi_algorithm_manager:
                active_algorithms = self.multi_algorithm_manager.get_active_algorithms()
                if not active_algorithms:
                    return {'error': 'æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•'}

                # æ±‡æ€»æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„è¯„çº§ç»Ÿè®¡
                total_stats = {
                    'correct': {'count': 0, 'percent': 0.0},
                    'minor': {'count': 0, 'percent': 0.0},
                    'moderate': {'count': 0, 'percent': 0.0},
                    'large': {'count': 0, 'percent': 0.0},
                    'severe': {'count': 0, 'percent': 0.0}
                }

                total_count = 0
                for algorithm in active_algorithms:
                    if algorithm.analyzer and algorithm.analyzer.note_matcher:
                        alg_stats = algorithm.analyzer.note_matcher.get_graded_error_stats()
                        if alg_stats and 'error' not in alg_stats:
                            for level in ['correct', 'minor', 'moderate', 'large', 'severe']:
                                if level in alg_stats:
                                    total_stats[level]['count'] += alg_stats[level].get('count', 0)
                                    total_count += alg_stats[level].get('count', 0)

                # è®¡ç®—ç™¾åˆ†æ¯”ï¼Œç¡®ä¿æ€»å’Œä¸º100%ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
                if total_count > 0:
                    # å…ˆè®¡ç®—åŸå§‹ç™¾åˆ†æ¯”
                    raw_percentages = {}
                    for level in ['correct', 'minor', 'moderate', 'large', 'severe']:
                        raw_percentages[level] = (total_stats[level]['count'] / total_count) * 100.0

                    # ä¿ç•™4ä½å°æ•°å¹¶å››èˆäº”å…¥
                    rounded_percentages = {}
                    for level in ['correct', 'minor', 'moderate', 'large', 'severe']:
                        rounded_percentages[level] = round(raw_percentages[level], 4)

                    # è°ƒæ•´æœ€åä¸€ä¸ªç™¾åˆ†æ¯”ä»¥ç¡®ä¿æ€»å’Œä¸º100%
                    total_rounded = sum(rounded_percentages.values())
                    if total_rounded != 100.0:
                        # æ‰¾å‡ºæœ€å¤§çš„ç™¾åˆ†æ¯”è¿›è¡Œè°ƒæ•´
                        max_level = max(rounded_percentages.keys(), key=lambda x: rounded_percentages[x])
                        rounded_percentages[max_level] += (100.0 - total_rounded)

                    # è®¾ç½®æœ€ç»ˆç™¾åˆ†æ¯”
                    for level in ['correct', 'minor', 'moderate', 'large', 'severe']:
                        total_stats[level]['percent'] = rounded_percentages[level]

                return total_stats

            # å•ç®—æ³•æ¨¡å¼
            if not self.analyzer or not self.analyzer.note_matcher:
                return {'error': 'æ²¡æœ‰åˆ†æå™¨æˆ–éŸ³ç¬¦åŒ¹é…å™¨'}

            stats = self.analyzer.note_matcher.get_graded_error_stats()
            return stats

        except Exception as e:
            logger.error(f"è·å–åˆ†çº§è¯¯å·®ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

