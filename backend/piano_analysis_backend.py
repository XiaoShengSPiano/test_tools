#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import traceback
import pandas as pd
import numpy as np
from typing import Optional, Tuple, Dict, Any, List, Union
from collections import defaultdict
from plotly.graph_objects import Figure
from utils.logger import Logger
from utils.constants import GRADE_LEVELS


import dash_bootstrap_components as dbc
from dash import html

# SPMIDç›¸å…³å¯¼å…¥
from spmid.spmid_analyzer import SPMIDAnalyzer
from backend.file_upload_service import FileUploadService

# å¯¼å…¥å„ä¸ªæ¨¡å—
from .data_manager import DataManager
from .plot_generator import PlotGenerator
from .plot_service import PlotService
from .key_filter import KeyFilter
from .table_data_generator import TableDataGenerator
from .delay_analysis import DelayAnalysis
from .multi_algorithm_manager import MultiAlgorithmManager, AlgorithmDataset, AlgorithmStatus
from .force_curve_analyzer import ForceCurveAnalyzer
from .relative_delay_analyzer import RelativeDelayAnalyzer

logger = Logger.get_logger()


class PianoAnalysisBackend:
    """é’¢ç´åˆ†æåç«¯ä¸»ç±» - ç»Ÿä¸€ç®¡ç†å•ç®—æ³•å’Œå¤šç®—æ³•åˆ†ææµç¨‹"""

    def __init__(self, session_id=None, history_manager=None):
        """
        åˆå§‹åŒ–é’¢ç´åˆ†æåç«¯
        
        Args:
            session_id: ä¼šè¯IDï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„åˆ†æä¼šè¯
            history_manager: å…¨å±€å†å²ç®¡ç†å™¨å®ä¾‹
        """
        self.session_id = session_id
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.data_manager = DataManager()
        self.key_filter = KeyFilter()
        self.plot_generator = PlotGenerator(self.key_filter)

        # åˆå§‹åŒ–å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨ï¼ˆå¤ç”¨å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
        from backend.multi_algorithm_plot_generator import MultiAlgorithmPlotGenerator
        self.multi_algorithm_plot_generator = MultiAlgorithmPlotGenerator(self.key_filter)

        # åˆå§‹åŒ–åŠ›åº¦æ›²çº¿åˆ†æå™¨
        self.force_curve_analyzer = ForceCurveAnalyzer()
        
        # ä½¿ç”¨å…¨å±€çš„å†å²ç®¡ç†å™¨å®ä¾‹
        self.history_manager = history_manager
        
        # åˆå§‹åŒ–å»¶æ—¶åˆ†æå™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œå› ä¸ºéœ€è¦analyzerï¼‰
        self.delay_analysis = None
        
        # ==================== å¤šç®—æ³•ç®¡ç†å™¨ ====================
        self.multi_algorithm_manager = MultiAlgorithmManager(max_algorithms=None)
        
        # åˆå§‹åŒ–æ–‡ä»¶ä¸Šä¼ æœåŠ¡ï¼ˆç»Ÿä¸€çš„æ–‡ä»¶ä¸Šä¼ å¤„ç†ï¼‰
        self.file_upload_service = FileUploadService(self.multi_algorithm_manager, self.history_manager)
        
        # åˆå§‹åŒ–ç»˜å›¾æœåŠ¡ï¼ˆç»Ÿä¸€ç®¡ç†æ‰€æœ‰å›¾è¡¨ç”Ÿæˆï¼‰
        self.plot_service = PlotService(self)
        
        # åˆå§‹åŒ–è¡¨æ ¼æ•°æ®ç”Ÿæˆå™¨ï¼ˆç»Ÿä¸€ç®¡ç†æ‰€æœ‰è¡¨æ ¼æ•°æ®ç”Ÿæˆï¼‰
        self.table_data_generator = TableDataGenerator(self)
        
        # åˆå§‹åŒ–ç›¸å¯¹å»¶æ—¶åˆ†æå™¨ï¼ˆå¤„ç†ç›¸å¯¹å»¶æ—¶åˆ†æï¼‰
        self.relative_delay_analyzer = RelativeDelayAnalyzer(self)
        
        # ==================== ä¸´æ—¶æ–‡ä»¶ç¼“å­˜ ====================
        # ç”¨äºå­˜å‚¨ä¸Šä¼ çš„æ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®ï¼Œå‡å°‘ dcc.Store çš„è´Ÿè½½
        self.temp_file_cache: Dict[str, bytes] = {}
        
        logger.debug(f"[DEBUG]PianoAnalysisBackendåˆå§‹åŒ–å®Œæˆ (Session: {session_id})")


    # ==================== æ•°æ®ç®¡ç†ç›¸å…³æ–¹æ³• ====================
    def clear_data_state(self) -> None:
        """æ¸…ç†æ‰€æœ‰æ•°æ®çŠ¶æ€"""
        self.data_manager.clear_data_state()
        self.plot_generator.set_data()
        self.key_filter.set_data(None, None)
        
        # æ¸…ç†å¤šç®—æ³•ç®¡ç†å™¨
        if self.multi_algorithm_manager:
            self.multi_algorithm_manager.clear_all()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç¼“å­˜
        self.clear_temp_cache()

        logger.debug("[DEBUG] æ‰€æœ‰æ•°æ®çŠ¶æ€å·²æ¸…ç†")
    
    def _get_algorithms_to_analyze(self) -> List[Any]:
        """
        è·å–éœ€è¦åˆ†æçš„ç®—æ³•åˆ—è¡¨

        Returns:
            List[AlgorithmDataset]: éœ€è¦åˆ†æçš„ç®—æ³•åˆ—è¡¨
        """
        # è·å–æ¿€æ´»çš„ç®—æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºå•ç®—æ³•æ•°æ®é›†
        if self.multi_algorithm_manager:
            active_algorithms = self.get_active_algorithms()
            if active_algorithms:
                return active_algorithms
            # å¦‚æœç®¡ç†å™¨å­˜åœ¨ä½†æ²¡æœ‰æ¿€æ´»ç®—æ³•ï¼Œåˆ™åˆ›å»ºå•ç®—æ³•
            return [self._get_or_create_single_algorithm()]
        else:
            # å¤šç®—æ³•ç®¡ç†å™¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºå•ç®—æ³•æ•°æ®é›†
            return [self._get_or_create_single_algorithm()]

    def _get_or_create_single_algorithm(self) -> Any:
        """
        è·å–æˆ–åˆ›å»ºå•ç®—æ³•æ•°æ®é›†

        Returns:
            AlgorithmDataset: å•ç®—æ³•æ•°æ®é›†
        """
        # æŸ¥æ‰¾ç°æœ‰çš„å•ç®—æ³•
        active_algorithms = self.get_active_algorithms()
        for algorithm in active_algorithms:
            if algorithm.metadata.algorithm_name == "single_algorithm":
                return algorithm

        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„å•ç®—æ³•æ•°æ®é›†
        from backend.multi_algorithm_manager import AlgorithmMetadata
        algorithm = self.multi_algorithm_manager.create_algorithm(
            AlgorithmMetadata(
                algorithm_name="single_algorithm",
                display_name="å•ç®—æ³•åˆ†æ",
                filename=self.data_manager.get_upload_filename()
            )
        )
        return algorithm

    # ==================== ä¸´æ—¶æ–‡ä»¶ç¼“å­˜æ–¹æ³• ====================
    def cache_temp_file(self, file_id: str, content_bytes: bytes) -> None:
        """ç¼“å­˜ä¸´æ—¶ä¸Šä¼ çš„æ–‡ä»¶"""
        if file_id and content_bytes:
            self.temp_file_cache[file_id] = content_bytes
            logger.debug(f"å·²ç¼“å­˜ä¸´æ—¶æ–‡ä»¶: {file_id}, å¤§å°: {len(content_bytes)} å­—èŠ‚")
    
    def get_cached_temp_file(self, file_id: str) -> Optional[bytes]:
        """è·å–ç¼“å­˜çš„ä¸´æ—¶æ–‡ä»¶"""
        return self.temp_file_cache.get(file_id)
    
    def clear_temp_cache(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç¼“å­˜"""
        self.temp_file_cache.clear()
        logger.debug("[DEBUG] ä¸´æ—¶æ–‡ä»¶ç¼“å­˜å·²æ¸…ç†")

    def _analyze_single_algorithm(self, algorithm) -> bool:
        """
        åˆ†æå•ä¸ªç®—æ³•

        Args:
            algorithm: ç®—æ³•æ•°æ®é›†

        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–è¯¥ç®—æ³•çš„æ•°æ®
            record_data = self.data_manager.get_record_data()
            replay_data = self.data_manager.get_replay_data()

            if not record_data or not replay_data:
                logger.error(f"ç®—æ³• '{algorithm.metadata.algorithm_name}' æ²¡æœ‰æœ‰æ•ˆçš„å½•åˆ¶æˆ–æ’­æ”¾æ•°æ®")
                return False

            analyzer = SPMIDAnalyzer()

            # è·å–è¿‡æ»¤ä¿¡æ¯æ”¶é›†å™¨ï¼ˆåŒ…å«åŠ è½½é˜¶æ®µè¢«è¿‡æ»¤çš„éŸ³ç¬¦ä¿¡æ¯ï¼‰
            filter_collector = self.data_manager.get_filter_collector()

            # æ‰§è¡Œåˆ†æï¼ˆä¼ é€’è¿‡æ»¤ä¿¡æ¯ï¼‰
            success = analyzer.analyze(record_data, replay_data, filter_collector)

            if success:
                # å°†åˆ†æå™¨å­˜å‚¨åˆ°ç®—æ³•å®ä¾‹ä¸­
                algorithm.analyzer = analyzer
                algorithm.is_analyzed = True
                return True
            else:
                return False

        except Exception as e:
            logger.error(f"åˆ†æç®—æ³• '{algorithm.metadata.algorithm_name}' æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False

    def _get_current_analyzer(self, algorithm_name: Optional[str] = None) -> Optional[Any]:
        """
        è·å–å½“å‰ä¸Šä¸‹æ–‡çš„åˆ†æå™¨ï¼ˆç»Ÿä¸€çš„æ•°æ®è®¿é—®å±‚ï¼‰

        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆå¤šç®—æ³•æ¨¡å¼æ—¶éœ€è¦ï¼‰

        Returns:
            åˆ†æå™¨å®ä¾‹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        # è·å–æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨
        if self.multi_algorithm_manager:
            active_algorithms = self.get_active_algorithms()
            if active_algorithms:
                # å¤šç®—æ³•æ¨¡å¼
                if algorithm_name:
                    algorithm = self._find_algorithm_by_name(algorithm_name)
                    return algorithm.analyzer if algorithm else None
                else:
                    # å¤šç®—æ³•ä½†æœªæŒ‡å®šç®—æ³•åï¼Œè¿”å›ç¬¬ä¸€ä¸ªç®—æ³•çš„åˆ†æå™¨
                    return active_algorithms[0].analyzer
            else:
                # å¤šç®—æ³•ç®¡ç†å™¨å­˜åœ¨ä½†æ²¡æœ‰æ¿€æ´»ç®—æ³•ï¼Œè¿”å›None
                return None
        else:
            # å¤šç®—æ³•ç®¡ç†å™¨ä¸å­˜åœ¨ï¼Œè¿”å›None
            return None

    def get_data_source_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æºä¿¡æ¯"""
        return self.data_manager.get_data_source_info()

    def analyze_data(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹ï¼ˆç»Ÿä¸€å•ç®—æ³•å’Œå¤šç®—æ³•æ¨¡å¼ï¼‰

        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹æ•°æ®åˆ†ææµç¨‹")

            # è·å–éœ€è¦åˆ†æçš„ç®—æ³•åˆ—è¡¨
            algorithms_to_analyze = self._get_algorithms_to_analyze()

            if not algorithms_to_analyze:
                logger.error("æ²¡æœ‰éœ€è¦åˆ†æçš„ç®—æ³•")
                return False

            # å¯¹æ¯ä¸ªç®—æ³•æ‰§è¡Œåˆ†æ
            success_count = 0
            for algorithm in algorithms_to_analyze:
                if self._analyze_single_algorithm(algorithm):
                    success_count += 1
                    logger.info(f"ç®—æ³• '{algorithm.metadata.algorithm_name}' åˆ†ææˆåŠŸ")
                else:
                    logger.error(f"ç®—æ³• '{algorithm.metadata.algorithm_name}' åˆ†æå¤±è´¥")

            if success_count == len(algorithms_to_analyze):
                logger.info(f"æ•°æ®åˆ†æå®Œæˆï¼Œå…±åˆ†æ {success_count} ä¸ªç®—æ³•")
                return True
            else:
                logger.error(f"æ•°æ®åˆ†æéƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ {success_count}/{len(algorithms_to_analyze)} ä¸ªç®—æ³•")
                return False

        except Exception as e:
            logger.error(f"æ•°æ®åˆ†æå¼‚å¸¸: {e}")
            logger.error(traceback.format_exc())
            return False


    def process_history_selection(self, history_id):
        """
        å¤„ç†å†å²è®°å½•é€‰æ‹© - ç»Ÿä¸€çš„å†å²è®°å½•å…¥å£
        
        Args:
            history_id: å†å²è®°å½•ID
            
        Returns:
            tuple: (success, result_data, error_msg)
        """
        return self.history_manager.process_history_selection(history_id, self)
    
    async def load_algorithm_from_history(self, record_id: int) -> Tuple[bool, str]:
        """
        ä»å†å²è®°å½•åŠ è½½ç®—æ³•åˆ°å½“å‰ä¼šè¯
        
        Args:
            record_id: æ•°æ®åº“è®°å½• ID
            
        Returns:
            Tuple[bool, str]: (æˆåŠŸä¸å¦, ç®—æ³•åæˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            # 1. è·å–è®°å½•
            record = self.history_manager.get_record_by_id(record_id)
            if not record:
                return False, f"æœªæ‰¾åˆ°è®°å½• ID: {record_id}"
            
            # 2. ä» Parquet åŠ è½½æ•°æ®
            from database.history_manager import ParquetDataLoader
            tracks = ParquetDataLoader.load_from_record(record)
            
            if len(tracks) < 2:
                return False, "å†å²æ•°æ®éŸ³è½¨ä¸è¶³"
            
            # 3. è½¬æ¢ä¸º Note åˆ—è¡¨å¹¶è¿›è¡ŒäºŒæ¬¡è¿‡æ»¤ (Ensuring quality criteria for historical data)
            # æ³¨æ„ï¼šå†å²æ•°æ®å­˜å‚¨çš„æ˜¯ OptimizedNote
            raw_record_notes = [note.to_standard_note() for note in tracks[0]]
            raw_replay_notes = [note.to_standard_note() for note in tracks[1]]
            
            # [æ–°å¢] é‡æ–°åº”ç”¨æœ€æ–°çš„è¿‡æ»¤è§„åˆ™ (User Requirement)
            from spmid.data_filter import DataFilter
            data_filter = DataFilter()
            record_notes, replay_notes, _ = data_filter.filter_notes(raw_record_notes, raw_replay_notes)
            
            logger.info(f"ğŸ’¾ ä»å†å²åŠ è½½å¹¶é‡æ–°è¿‡æ»¤: å½•åˆ¶({len(raw_record_notes)}->{len(record_notes)}), æ’­æ”¾({len(raw_replay_notes)}->{len(replay_notes)})")

            # 4. ç”Ÿæˆç®—æ³•åç§° (å¦‚æœç”¨æˆ·æ²¡ç»™ï¼Œç”¨æ–‡ä»¶å+ç”µæœº/ç®—æ³•æ ‡è®°)
            display_name = f"{record['filename']}_{record['motor_type']}_{record['algorithm']}"
            
            # 5. æ·»åŠ åˆ°ç®¡ç†å™¨
            success, result = await self.multi_algorithm_manager.add_algorithm_async(
                display_name,
                record['filename'],
                record_notes,
                replay_notes,
                filter_collector=None # å†å²åŠ è½½é€šå¸¸ä¸é‡å¤å±•ç¤ºè¿‡æ»¤è¯¦ç»†æ—¥å¿—
            )
            
            if success:
                # è‡ªåŠ¨æ¿€æ´»
                unique_name = result
                alg = self.multi_algorithm_manager.get_algorithm(unique_name)
                if alg:
                    alg.is_active = True
                return True, unique_name
            else:
                return False, result
                
        except Exception as e:
            logger.error(f"ä»å†å²åŠ è½½å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False, str(e)

            
    def _sync_data_to_modules(self) -> None:
        """åŒæ­¥æ•°æ®åˆ°å„ä¸ªæ¨¡å—"""
        # è·å–æœ‰æ•ˆæ•°æ®
        valid_record_data = self.data_manager.get_valid_record_data()
        valid_replay_data = self.data_manager.get_valid_replay_data()
        
        # åŒæ­¥åˆ°å„ä¸ªæ¨¡å—
        current_analyzer = self._get_current_analyzer()
        self.plot_generator.set_data(valid_record_data, valid_replay_data, analyzer=current_analyzer)
        self.key_filter.set_data(valid_record_data, valid_replay_data)
        
        # åŒæ­¥åˆ†æç»“æœåˆ°å„ä¸ªæ¨¡å—
        self._sync_analysis_results()
    
    def _extract_analysis_results(self, analyzer) -> Dict[str, Any]:
        """æå–åˆ†æç»“æœæ•°æ®"""
        # è·å–åˆ†æç»“æœ
        multi_hammers = analyzer.multi_hammers
        drop_hammers = analyzer.drop_hammers

        # ç»Ÿè®¡æ•°æ®ç°åœ¨ç”± invalid_statistics å¯¹è±¡ç®¡ç†
        matched_pairs = analyzer.matched_pairs

        # åˆå¹¶æ‰€æœ‰é”™è¯¯éŸ³ç¬¦
        all_error_notes = multi_hammers + drop_hammers

        return {
            'multi_hammers': multi_hammers,
            'drop_hammers': drop_hammers,
            'matched_pairs': matched_pairs,
            'all_error_notes': all_error_notes
        }

    def _prepare_error_data(self, analyzer, analysis_data: Dict[str, Any]) -> None:
        """å‡†å¤‡é”™è¯¯æ•°æ®ï¼Œç¡®ä¿analyzerå¯¹è±¡æœ‰å¿…è¦çš„å±æ€§"""
        multi_hammers = analysis_data['multi_hammers']
        drop_hammers = analysis_data['drop_hammers']
        all_error_notes = analysis_data['all_error_notes']

        # ç¡®ä¿analyzerå¯¹è±¡æœ‰è¿™äº›å±æ€§ï¼ˆä¾›ç€‘å¸ƒå›¾ç”Ÿæˆå™¨ä½¿ç”¨ï¼‰
        analyzer.drop_hammers = drop_hammers
        analyzer.multi_hammers = multi_hammers
        logger.info(f"è®¾ç½®analyzeré”™è¯¯æ•°æ®: ä¸¢é”¤={len(drop_hammers)}, å¤šé”¤={len(multi_hammers)}")

        # è®¾ç½®all_error_noteså±æ€§ä¾›UIå±‚ä½¿ç”¨
        self.all_error_notes = all_error_notes

    def _sync_analysis_data_to_modules(self, analysis_data: Dict[str, Any]) -> None:
        """åŒæ­¥åˆ†ææ•°æ®åˆ°å„ä¸ªæ¨¡å—"""
        multi_hammers = analysis_data['multi_hammers']
        drop_hammers = analysis_data['drop_hammers']
        # ä¸å†ä» analysis_data è·å– invalid_notes_table_dataï¼ˆå·²ç§»é™¤ï¼‰
        matched_pairs = analysis_data['matched_pairs']
        all_error_notes = analysis_data['all_error_notes']

        # è·å–æœ‰æ•ˆæ•°æ®
        valid_record_data = self.data_manager.get_valid_record_data()
        valid_replay_data = self.data_manager.get_valid_replay_data()

        # åŒæ­¥åˆ°å„ä¸ªæ¨¡å—
        self.key_filter.set_data(valid_record_data, valid_replay_data)
        self.plot_generator.set_data(valid_record_data, valid_replay_data, matched_pairs, analyzer=self._get_current_analyzer())

        logger.info(f"é”™è¯¯æ•°æ®ç»Ÿè®¡: ä¸¢é”¤={len(drop_hammers)}, å¤šé”¤={len(multi_hammers)}")

    def _validate_sync(self, analyzer) -> bool:
        """éªŒè¯æ•°æ®åŒæ­¥æ˜¯å¦æˆåŠŸ"""
        drop_count = len(analyzer.drop_hammers)
        multi_count = len(analyzer.multi_hammers)
        total_errors = drop_count + multi_count
        
        logger.info(f"analyzeré”™è¯¯æ•°æ®åŒæ­¥éªŒè¯: ä¸¢é”¤={drop_count}, å¤šé”¤={multi_count}")
        
        if total_errors > 0:
            logger.info(f"é”™è¯¯ç»Ÿè®¡: ä¸¢é”¤={drop_count}, å¤šé”¤={multi_count}, æ€»è®¡={total_errors}")
        
        return True

    def _sync_analysis_results(self) -> None:
        """åŒæ­¥åˆ†æç»“æœåˆ°å„ä¸ªæ¨¡å—"""
        analyzer = self._get_current_analyzer()
        if not analyzer:
            return

        try:
            # æå–åˆ†æç»“æœ
            analysis_data = self._extract_analysis_results(analyzer)

            # å‡†å¤‡é”™è¯¯æ•°æ®
            self._prepare_error_data(analyzer, analysis_data)

            # åŒæ­¥æ•°æ®åˆ°å„ä¸ªæ¨¡å—
            self._sync_analysis_data_to_modules(analysis_data)

            # éªŒè¯åŒæ­¥ç»“æœ
            self._validate_sync(analyzer)

            logger.info("åˆ†æç»“æœåŒæ­¥å®Œæˆ")

        except Exception as e:
            logger.error(f"åŒæ­¥åˆ†æç»“æœå¤±è´¥: {e}")
    
    def get_global_average_delay(self) -> float:
        """
        è·å–æ•´é¦–æ›²å­çš„å¹³å‡æ—¶å»¶ï¼ˆåŸºäºå·²é…å¯¹æ•°æ®ï¼‰

        Returns:
            float: å¹³å‡æ—¶å»¶ï¼ˆ0.1mså•ä½ï¼‰
        """
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_global_average_delay()
    
    def get_variance(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„æ€»ä½“æ–¹å·®"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_variance()
    
    def get_standard_deviation(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„æ€»ä½“æ ‡å‡†å·®"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_standard_deviation()
    
    def get_mean_absolute_error(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_mean_absolute_error()
    
    def get_mean_squared_error(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_mean_squared_error()

    def get_root_mean_squared_error(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_root_mean_squared_error()

    def get_mean_error(self) -> float:
        """è·å–å·²åŒ¹é…æŒ‰é”®å¯¹çš„å¹³å‡è¯¯å·®ï¼ˆMEï¼‰"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_mean_error()

    def get_coefficient_of_variation(self) -> float:
        """è·å–å·²é…å¯¹æŒ‰é”®çš„å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰"""
        active_algorithms = self.get_active_algorithms()
        if not active_algorithms or not active_algorithms[0].analyzer:
            return 0.0
        return active_algorithms[0].analyzer.get_coefficient_of_variation()

    def generate_delay_time_series_plot(self) -> Any:
        """ç”Ÿæˆå»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_delay_time_series_plot()
    

    def generate_delay_histogram_plot(self) -> Any:
        """ç”Ÿæˆå»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_delay_histogram_plot()

    def generate_offset_alignment_plot(self) -> Any:
        """ç”Ÿæˆåç§»å¯¹é½åˆ†ææŸ±çŠ¶å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_offset_alignment_plot()

    def generate_key_delay_zscore_scatter_plot(self) -> Any:
        """ç”ŸæˆæŒ‰é”®ä¸å»¶æ—¶Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_key_delay_zscore_scatter_plot()
    
    def generate_single_key_delay_comparison_plot(self, key_id: int) -> Any:
        """ç”Ÿæˆå•é”®å¤šæ›²å»¶æ—¶å¯¹æ¯”å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_single_key_delay_comparison_plot(key_id)
    
    def generate_key_delay_scatter_plot(self, only_common_keys: bool = False, selected_algorithm_names: List[str] = None) -> Any:
        """ç”ŸæˆæŒ‰é”®ä¸å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_key_delay_scatter_plot(only_common_keys, selected_algorithm_names)

    def generate_hammer_velocity_delay_scatter_plot(self) -> Any:
        """ç”Ÿæˆé”¤é€Ÿä¸å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_hammer_velocity_delay_scatter_plot()
        
    def generate_hammer_velocity_relative_delay_scatter_plot(self) -> Any:
        """ç”Ÿæˆé”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶çš„æ•£ç‚¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_hammer_velocity_relative_delay_scatter_plot()
    
    def generate_key_hammer_velocity_scatter_plot(self) -> Any:
        """ç”ŸæˆæŒ‰é”®ä¸é”¤é€Ÿçš„æ•£ç‚¹å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_key_hammer_velocity_scatter_plot()
    
    def generate_waterfall_plot(self, data_types: List[str] = None, key_ids: List[int] = None, key_filter=None) -> Any:
        """ç”Ÿæˆç€‘å¸ƒå›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰

        Args:
            data_types: è¦æ˜¾ç¤ºçš„æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤æ˜¾ç¤ºæ‰€æœ‰ç±»å‹
            key_ids: è¦æ˜¾ç¤ºçš„æŒ‰é”®IDåˆ—è¡¨ï¼Œé»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æŒ‰é”®
            key_filter: æŒ‰é”®ç­›é€‰æ¡ä»¶
        """
        return self.plot_service.generate_waterfall_plot(data_types, key_ids, key_filter)

    def get_waterfall_key_statistics(self, data_types: List[str] = None) -> Dict[str, Any]:
        """è·å–ç€‘å¸ƒå›¾æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯

        Args:
            data_types: æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ç»Ÿè®¡æ‰€æœ‰ç±»å‹

        Returns:
            Dict[str, Any]: åŒ…å«æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
                - available_keys: List[Dict] å¯ç”¨æŒ‰é”®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«key_id, total_count, exception_countç­‰
                - summary: Dict æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        """
        return self.plot_service.get_waterfall_key_statistics(data_types)
    
    def generate_watefall_conbine_plot(self, key_on: float, key_off: float, key_id: int) -> Any:
        """ç”Ÿæˆç€‘å¸ƒå›¾å¯¹æ¯”å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_watefall_conbine_plot(key_on, key_off, key_id)
    
    def generate_watefall_conbine_plot_by_index(self, index: int, is_record: bool) -> Any:
        """æ ¹æ®ç´¢å¼•ç”Ÿæˆç€‘å¸ƒå›¾å¯¹æ¯”å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_watefall_conbine_plot_by_index(index, is_record)
    
    def _find_algorithm_by_name(self, algorithm_name: str):
        """æ ¹æ®ç®—æ³•åç§°æŸ¥æ‰¾ç®—æ³•"""
        for alg in self.get_active_algorithms():
            if alg.metadata.algorithm_name == algorithm_name:
                return alg
        return None
    
    def _find_extreme_delay_item(self, offset_data: List[Dict], delay_type: str) -> Optional[Dict]:
        """æŸ¥æ‰¾æå€¼å»¶è¿Ÿé¡¹"""
        if not offset_data:
            return None
        
        if delay_type == 'max':
            max_delay = max(item.get('keyon_offset', 0) for item in offset_data)
            items = [item for item in offset_data if item.get('keyon_offset', 0) == max_delay]
            return items[0] if items else None
        elif delay_type == 'min':
            # ä¸UIé€»è¾‘ä¸€è‡´ï¼šæ‰¾æœ€åä¸€ä¸ªåŒ¹é…çš„æœ€å°å»¶è¿Ÿé¡¹
            min_delay = min(item.get('keyon_offset', 0) for item in offset_data)
            target = None
            for item in offset_data:
                if item.get('keyon_offset', 0) == min_delay:
                    target = item
            return target
        return None
    
    def get_notes_by_delay_type(self, algorithm_name: str, delay_type: str) -> Optional[Tuple[Any, Any, int, int]]:
        """
        æ ¹æ®å»¶è¿Ÿç±»å‹ï¼ˆæœ€å¤§/æœ€å°ï¼‰è·å–å¯¹åº”çš„éŸ³ç¬¦
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            delay_type: å»¶è¿Ÿç±»å‹ï¼Œ'max' æˆ– 'min'
        
        Returns:
            Tuple[Note, Note, int, int]: (record_note, replay_note, record_index, replay_index) æˆ– None
        """
        try:
            # æŸ¥æ‰¾ç®—æ³•
            algorithm = self._find_algorithm_by_name(algorithm_name)
            if not algorithm or not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                logger.warning(f"ç®—æ³• '{algorithm_name}' ä¸å¯ç”¨")
                return None
            
            note_matcher = algorithm.analyzer.note_matcher
            
            # è·å–å¯¹é½æ•°æ®ï¼ˆæ³¨æ„ï¼šrecord_index/replay_index åœ¨æ­¤å¤„å®é™…å­˜å‚¨çš„æ˜¯ UUIDï¼‰
            offset_data = note_matcher.get_offset_alignment_data()
            if not offset_data:
                logger.warning("æ²¡æœ‰åŒ¹é…æ•°æ®")
                return None
            
            # æŸ¥æ‰¾æå€¼é¡¹
            target_item = self._find_extreme_delay_item(offset_data, delay_type)
            if not target_item:
                logger.warning(f"æœªæ‰¾åˆ°{delay_type}å»¶è¿Ÿé¡¹")
                return None

            # ä»å¯¹é½æ•°æ®ä¸­æå– UUID
            record_uuid = target_item.get('record_uuid')
            replay_uuid = target_item.get('replay_uuid')
            
            # å…¼å®¹æ€§ï¼šå¦‚æœæ–°å­—æ®µä¸å­˜åœ¨ï¼Œå°è¯•å›é€€åˆ° record_index
            if not record_uuid: record_uuid = target_item.get('record_index')
            if not replay_uuid: replay_uuid = target_item.get('replay_index')
            
            # é€šè¿‡ UUID åœ¨åŒ¹é…å™¨ä¸­æŸ¥æ‰¾å®Œæ•´çš„åŒ¹é…å¯¹å¯¹è±¡
            matched = note_matcher.find_matched_pair_by_uuid(str(record_uuid), str(replay_uuid))
            if not matched:
                logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…å¯¹: record_uuid={record_uuid}, replay_uuid={replay_uuid}")
                return None
            
            record_note, rep_note, match_type, error_ms = matched
            
            # è¿”å› Note å¯¹è±¡åŠå…¶å†…éƒ¨åç§»é‡ï¼ˆæ•´æ•°ç´¢å¼•ï¼‰
            return (record_note, rep_note, record_note.offset, rep_note.offset)
            
        except Exception as e:
            logger.error(f"è·å–{delay_type}å»¶è¿ŸéŸ³ç¬¦å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def generate_scatter_detail_plot_by_indices(self, record_index: int, replay_index: int) -> Any:
        """ç”Ÿæˆæ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_scatter_detail_plot_by_indices(record_index, replay_index)
    
    def generate_multi_algorithm_scatter_detail_plot_by_indices(self, algorithm_name: str, record_index: int, replay_index: int) -> Any:
        """å¤šç®—æ³•æ¨¡å¼ä¸‹ç”Ÿæˆæ•£ç‚¹å›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_multi_algorithm_scatter_detail_plot_by_indices(algorithm_name, record_index, replay_index)


    def generate_multi_algorithm_detail_plot_by_index(self, algorithm_name: str, index: int, is_record: bool) -> Any:
        """å¤šç®—æ³•æ¨¡å¼ä¸‹æ ¹æ®ç´¢å¼•ç”Ÿæˆç€‘å¸ƒå›¾ç‚¹å‡»çš„è¯¦ç»†æ›²çº¿å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_multi_algorithm_detail_plot_by_index(algorithm_name, index, is_record)
    def generate_multi_algorithm_error_detail_plot_by_index(self, algorithm_name: str, index: int, is_record: bool) -> Any:
        """å¤šç®—æ³•æ¨¡å¼ä¸‹ç”Ÿæˆé”™è¯¯è¯¦æƒ…çš„è¯¦ç»†æ›²çº¿å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_multi_algorithm_error_detail_plot_by_index(algorithm_name, index, is_record)
    def apply_key_filter(self, key_ids: Optional[List[int]]) -> None:
        """åº”ç”¨æŒ‰é”®è¿‡æ»¤

        Args:
            key_ids: è¦è¿‡æ»¤çš„æŒ‰é”®IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ¸…é™¤è¿‡æ»¤
        """
        if key_ids is None:
            # æ¸…é™¤æŒ‰é”®è¿‡æ»¤
            self.key_filter.set_key_filter([])
        else:
            self.key_filter.set_key_filter(key_ids)

    def get_filter_info(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰è¿‡æ»¤å™¨çš„çŠ¶æ€ä¿¡æ¯

        Returns:
            DictåŒ…å«ï¼š
            - key_filter: æŒ‰é”®è¿‡æ»¤çŠ¶æ€
            - available_keys: å¯ç”¨æŒ‰é”®åˆ—è¡¨
        """
        return {
            'key_filter': self.key_filter.get_key_filter_status(),
            'available_keys': self.key_filter.get_available_keys(),
        }
    
    # ==================== è¡¨æ ¼æ•°æ®ç›¸å…³æ–¹æ³• ====================
    
    def get_summary_info(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦ä¿¡æ¯ï¼ˆå§”æ‰˜ç»™TableDataGeneratorï¼‰"""
        return self.table_data_generator.get_summary_info()
    
    def get_algorithm_statistics(self, algorithm) -> dict:
        """
        è·å–ç®—æ³•çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«æ‰€æœ‰é”™è¯¯ç±»å‹ï¼‰

        Args:
            algorithm: ç®—æ³•å¯¹è±¡

        Returns:
            dict: åŒ…å«å®Œæ•´é”™è¯¯ç»Ÿè®¡çš„ä¿¡æ¯
                - drop_hammers: ä¸¢é”¤æ•°
                - multi_hammers: å¤šé”¤æ•°
                - invalid_record_notes: å½•åˆ¶æ— æ•ˆéŸ³ç¬¦æ•°
                - invalid_replay_notes: æ’­æ”¾æ— æ•ˆéŸ³ç¬¦æ•°
                - total_errors: æ€»é”™è¯¯æ•°ï¼ˆä¸¢é”¤+å¤šé”¤+æ— æ•ˆéŸ³ç¬¦ï¼‰
        """
        try:
            if not hasattr(algorithm, 'analyzer') or not algorithm.analyzer:
                return {
                    'drop_hammers': 0,
                    'multi_hammers': 0,
                    'invalid_record_notes': 0,
                    'invalid_replay_notes': 0,
                    'total_errors': 0
                }

            # ç›´æ¥ä»analyzerè·å–é”™è¯¯ç»Ÿè®¡
            analyzer = algorithm.analyzer

            # é”¤å‡»é”™è¯¯ç»Ÿè®¡
            drop_hammers = len(analyzer.drop_hammers) 
            multi_hammers = len(analyzer.multi_hammers)

            # æ— æ•ˆéŸ³ç¬¦ç»Ÿè®¡
            analysis_stats = analyzer.get_analysis_stats()
            invalid_record_notes = analysis_stats.get('record_invalid_notes', 0)
            invalid_replay_notes = analysis_stats.get('replay_invalid_notes', 0)

            # è®¡ç®—æ€»é”™è¯¯æ•°
            total_errors = drop_hammers + multi_hammers + invalid_record_notes + invalid_replay_notes

            result = {
                'drop_hammers': drop_hammers,
                'multi_hammers': multi_hammers,
                'invalid_record_notes': invalid_record_notes,
                'invalid_replay_notes': invalid_replay_notes,
                'total_errors': total_errors
            }
            return result

        except Exception as e:
            logger.error(f"è·å–ç®—æ³•ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'drop_hammers': 0,
                'multi_hammers': 0,
                'invalid_record_notes': 0,
                'invalid_replay_notes': 0,
                'total_errors': 0
            }
    
    def get_data_overview_statistics(self, algorithm) -> dict:
        """
        è·å–æ•°æ®æ¦‚è§ˆç»Ÿè®¡ä¿¡æ¯
        
        Args:
            algorithm: ç®—æ³•å¯¹è±¡
        
        Returns:
            dict: åŒ…å«éŸ³ç¬¦æ€»æ•°ã€æœ‰æ•ˆéŸ³ç¬¦æ•°ã€åŒ¹é…å¯¹æ•°ç­‰ä¿¡æ¯
        """
        try:
            if not hasattr(algorithm, 'analyzer') or not algorithm.analyzer:
                return {
                    'total_notes': 0,
                    'valid_record_notes': 0,
                    'valid_replay_notes': 0,
                    'matched_pairs': 0,
                    'invalid_record_notes': 0,
                    'invalid_replay_notes': 0
                }
            
            # ä»analyzerè·å–ç»Ÿè®¡æ•°æ®
            analyzer = algorithm.analyzer
            stats = analyzer.get_analysis_stats()
            
            # è®¡ç®—æœ‰æ•ˆéŸ³ç¬¦æ•°ï¼ˆtotal_record_notes å°±æ˜¯æœ‰æ•ˆçš„å½•åˆ¶éŸ³ç¬¦æ•°ï¼‰
            valid_record = stats.get('total_record_notes', 0)
            valid_replay = stats.get('total_replay_notes', 0)
            invalid_record = stats.get('record_invalid_notes', 0)
            invalid_replay = stats.get('replay_invalid_notes', 0)
            
            # éŸ³ç¬¦æ€»æ•° = æœ‰æ•ˆéŸ³ç¬¦ + æ— æ•ˆéŸ³ç¬¦
            total_notes = valid_record + valid_replay + invalid_record + invalid_replay
            
            result = {
                'total_notes': total_notes,
                'valid_record_notes': valid_record,
                'valid_replay_notes': valid_replay,
                'matched_pairs': stats.get('matched_pairs', 0),
                'invalid_record_notes': invalid_record,
                'invalid_replay_notes': invalid_replay
            }
            return result

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æ¦‚è§ˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                        'total_notes': 0,
                'valid_record_notes': 0,
                'valid_replay_notes': 0,
                'matched_pairs': 0,
                'invalid_record_notes': 0,
                'invalid_replay_notes': 0
            }
    
    def get_invalid_notes_table_data(self) -> List[Dict[str, Any]]:
        """è·å–æ— æ•ˆéŸ³ç¬¦è¡¨æ ¼æ•°æ®ï¼ˆå§”æ‰˜ç»™TableDataGeneratorï¼‰"""
        return self.table_data_generator.get_invalid_notes_table_data()

    def get_invalid_notes_detail_table_data(self, data_type: str) -> List[Dict[str, Any]]:
        """è·å–æ— æ•ˆéŸ³ç¬¦è¯¦ç»†åˆ—è¡¨æ•°æ®ï¼ˆå§”æ‰˜ç»™TableDataGeneratorï¼‰"""
        return self.table_data_generator.get_invalid_notes_detail_table_data(data_type)

    def _filter_error_data_by_algorithm(self, error_type: str, algorithm_name: str) -> List[Dict[str, Any]]:
        """æ ¹æ®ç®—æ³•åç§°è¿‡æ»¤é”™è¯¯æ•°æ®ï¼ˆå†…éƒ¨è¾…åŠ©å‡½æ•°ï¼‰
        
        Args:
            error_type: é”™è¯¯ç±»å‹ ('ä¸¢é”¤' æˆ– 'å¤šé”¤')
            algorithm_name: ç®—æ³•åç§°
        
        Returns:
            List[Dict[str, Any]]: è¿‡æ»¤åçš„é”™è¯¯æ•°æ®
        """
        all_error_data = self.table_data_generator.get_error_table_data(error_type)

        # è¿‡æ»¤å‡ºæŒ‡å®šç®—æ³•çš„æ•°æ®
        filtered_data = []
        for item in all_error_data:
            if item.get('algorithm_name') == algorithm_name:
                filtered_data.append(item)

        return filtered_data

    def get_drop_hammers_detail_table_data(self, algorithm_name: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šç®—æ³•çš„ä¸¢é”¤è¯¦ç»†åˆ—è¡¨æ•°æ®"""
        return self._filter_error_data_by_algorithm('ä¸¢é”¤', algorithm_name)

    def get_multi_hammers_detail_table_data(self, algorithm_name: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šç®—æ³•çš„å¤šé”¤è¯¦ç»†åˆ—è¡¨æ•°æ®"""
        return self._filter_error_data_by_algorithm('å¤šé”¤', algorithm_name)

    # TODO
    def get_error_table_data(self, error_type: str) -> List[Dict[str, Any]]:
        """è·å–é”™è¯¯è¡¨æ ¼æ•°æ®ï¼ˆå§”æ‰˜ç»™TableDataGeneratorï¼‰"""
        return self.table_data_generator.get_error_table_data(error_type)
    
    
    # ==================== å»¶æ—¶å…³ç³»åˆ†æç›¸å…³æ–¹æ³• ====================
    def get_key_force_interaction_analysis(self) -> Dict[str, Any]:
        """
        è·å–æŒ‰é”®ä¸åŠ›åº¦çš„äº¤äº’æ•ˆåº”å›¾æ•°æ®

        ç”ŸæˆæŒ‰é”®-åŠ›åº¦äº¤äº’æ•ˆåº”å›¾æ‰€éœ€çš„æ•°æ®ï¼Œç”¨äºå¯è§†åŒ–åˆ†ææŒ‰é”®å’ŒåŠ›åº¦å¯¹å»¶æ—¶çš„è”åˆå½±å“ã€‚

        ç»Ÿä¸€å¤„ç†æ‰€æœ‰ç®—æ³•ï¼šé€šè¿‡multi_algorithm_managerè·å–æ¿€æ´»ç®—æ³•å¹¶ç”Ÿæˆåˆ†æç»“æœã€‚

        Returns:
            Dict[str, Any]: åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - status: çŠ¶æ€æ ‡è¯†
                - multi_algorithm_mode: Trueï¼ˆå§‹ç»ˆä¸ºç»Ÿä¸€æ ¼å¼ï¼‰
                - algorithm_results: å„ç®—æ³•çš„å®Œæ•´ç»“æœå­—å…¸
        """
        try:
            # è·å–æ¿€æ´»çš„ç®—æ³•å¹¶ç”Ÿæˆæ•°æ®
            active_algorithms = self.get_active_algorithms()

            if not active_algorithms:
                logger.warning("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•ï¼Œæ— æ³•è¿›è¡ŒæŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æ")
                return {
                    'status': 'error',
                    'message': 'æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•'
                }

            # ä¸ºæ¯ä¸ªç®—æ³•ç”Ÿæˆåˆ†æç»“æœ
            # ä½¿ç”¨å†…éƒ¨çš„algorithm_nameä½œä¸ºkeyï¼ˆå”¯ä¸€æ ‡è¯†ï¼ŒåŒ…å«æ–‡ä»¶åï¼‰ï¼Œé¿å…åŒç§ç®—æ³•ä¸åŒæ›²å­è¢«è¦†ç›–
            algorithm_results = {}
            for algorithm in active_algorithms:
                if not algorithm.analyzer:
                    logger.warning(f"ç®—æ³• '{algorithm.metadata.algorithm_name}' æ²¡æœ‰åˆ†æå™¨ï¼Œè·³è¿‡")
                    continue
                
                # ä½¿ç”¨å†…éƒ¨çš„algorithm_nameä½œä¸ºkeyï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
                algorithm_name = algorithm.metadata.algorithm_name
                display_name = algorithm.metadata.display_name
                delay_analysis = DelayAnalysis(algorithm.analyzer)
                result = delay_analysis.analyze_key_force_interaction()
                
                if result.get('status') == 'success':
                    # åœ¨resultä¸­æ·»åŠ display_nameï¼Œç”¨äºUIæ˜¾ç¤º
                    result['display_name'] = display_name
                    algorithm_results[algorithm_name] = result
                    logger.info(f"ç®—æ³• '{display_name}' (å†…éƒ¨: {algorithm_name}) çš„æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆ")
            
            if not algorithm_results:
                logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•")
                return {
                    'status': 'error',
                    'message': 'æ²¡æœ‰æˆåŠŸåˆ†æçš„ç®—æ³•'
                }
                
            # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼ˆä¸å†åŒºåˆ†å•ç®—æ³•/å¤šç®—æ³•æ¨¡å¼ï¼‰
            logger.info(f"æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå®Œæˆï¼Œå…± {len(algorithm_results)} ä¸ªç®—æ³•")
            return {
                    'status': 'success',
                'multi_algorithm_mode': True,  # å§‹ç»ˆä½¿ç”¨ç»Ÿä¸€æ ¼å¼
                    'algorithm_results': algorithm_results,
            }
        
        except Exception as e:
            logger.error(f"æŒ‰é”®-åŠ›åº¦äº¤äº’åˆ†æå¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return {
                'status': 'error',
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    
    def generate_key_force_interaction_plot(self) -> Any:
        """ç”ŸæˆæŒ‰é”®-åŠ›åº¦äº¤äº’æ•ˆåº”å›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_key_force_interaction_plot()
    def enable_multi_algorithm_mode(self, max_algorithms: Optional[int] = None) -> Tuple[bool, bool, Optional[str]]:
        """
        å¯ç”¨å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼Œç»Ÿä¸€æ¨¡å¼ä¸‹ç®¡ç†å™¨å·²åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºï¼‰
        
        Args:
            max_algorithms: æœ€å¤§ç®—æ³•æ•°é‡ï¼ˆå·²åºŸå¼ƒï¼Œç®¡ç†å™¨åœ¨åˆå§‹åŒ–æ—¶å·²åˆ›å»ºï¼‰
            
        Returns:
            Tuple[bool, bool, Optional[str]]: (æ˜¯å¦æˆåŠŸ, æ˜¯å¦æœ‰ç°æœ‰æ•°æ®éœ€è¦è¿ç§», æ–‡ä»¶å)
        """
        try:
            # æ³¨æ„ï¼šmulti_algorithm_manager åœ¨ __init__ ä¸­å·²ç»åˆå§‹åŒ–
            # max_algorithms å‚æ•°åœ¨ç»Ÿä¸€æ¨¡å¼ä¸‹è¢«å¿½ç•¥
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„åˆ†ææ•°æ®éœ€è¦è¿ç§»
            has_existing_data = False
            existing_filename = None
            
            analyzer = self._get_current_analyzer()
            if analyzer and analyzer.note_matcher and hasattr(analyzer, 'matched_pairs') and len(analyzer.matched_pairs) > 0:
                # æœ‰å·²åˆ†æçš„æ•°æ®
                has_existing_data = True
                # è·å–æ–‡ä»¶å
                data_source_info = self.get_data_source_info()
                existing_filename = data_source_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                logger.info(f"æ£€æµ‹åˆ°ç°æœ‰åˆ†ææ•°æ®ï¼Œæ–‡ä»¶å: {existing_filename}")
            
            return True, has_existing_data, existing_filename
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return False, False, None
    
    def migrate_existing_data_to_algorithm(self, algorithm_name: str) -> Tuple[bool, str]:
        """
        å°†ç°æœ‰çš„å•ç®—æ³•åˆ†ææ•°æ®è¿ç§»åˆ°å¤šç®—æ³•æ¨¡å¼
        
        Args:
            algorithm_name: ç”¨æˆ·æŒ‡å®šçš„ç®—æ³•åç§°
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        # multi_algorithm_manager åœ¨åˆå§‹åŒ–æ—¶å·²åˆ›å»º
        
        analyzer = self._get_current_analyzer()
        if not analyzer or not analyzer.note_matcher:
            return False, "æ²¡æœ‰å¯è¿ç§»çš„åˆ†ææ•°æ®"
        
        try:
            # è·å–åŸå§‹æ•°æ®
            record_data = self.data_manager.get_record_data()
            replay_data = self.data_manager.get_replay_data()
            
            if not record_data or not replay_data:
                return False, "åŸå§‹æ•°æ®ä¸å­˜åœ¨"
            
            # è·å–æ–‡ä»¶å
            data_source_info = self.get_data_source_info()
            filename = data_source_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            
            # ç”Ÿæˆå”¯ä¸€çš„ç®—æ³•åç§°ï¼ˆç®—æ³•å_æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰ï¼‰
            unique_algorithm_name = self.multi_algorithm_manager._generate_unique_algorithm_name(algorithm_name, filename)
            
            # åˆ›å»ºç®—æ³•æ•°æ®é›†ï¼ˆä½¿ç”¨å”¯ä¸€åç§°ä½œä¸ºå†…éƒ¨æ ‡è¯†ï¼ŒåŸå§‹åç§°ä½œä¸ºæ˜¾ç¤ºåç§°ï¼‰
            color_index = 0
            algorithm = AlgorithmDataset(unique_algorithm_name, algorithm_name, filename, color_index)
            
            # è·å–å½“å‰çš„åˆ†æå™¨
            current_analyzer = self._get_current_analyzer()
            algorithm.analyzer = current_analyzer
            algorithm.record_data = record_data
            algorithm.replay_data = replay_data

            # é”™è¯¯æ•°æ®å·²ç»åœ¨ analyzer ä¸­ï¼Œæ— éœ€é¢å¤–åŒæ­¥
            logger.info(f"è¿ç§»ç®—æ³•æ•°æ®: ä¸¢é”¤={len(current_analyzer.drop_hammers)}, "
                       f"å¤šé”¤={len(current_analyzer.multi_hammers)}")

            algorithm.metadata.status = AlgorithmStatus.READY

            # æ·»åŠ åˆ°ç®¡ç†å™¨
            self.multi_algorithm_manager.algorithms[unique_algorithm_name] = algorithm
            
            logger.info(f"ç°æœ‰æ•°æ®å·²è¿ç§»ä¸ºç®—æ³•: {algorithm_name}")
            return True, ""
            
        except Exception as e:
            logger.error(f"è¿ç§»ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False, str(e)


    def get_current_analysis_mode(self) -> Tuple[str, int]:
        """
        è·å–å½“å‰çš„åˆ†ææ¨¡å¼å’Œæ´»è·ƒç®—æ³•æ•°é‡

        Returns:
            Tuple[str, int]: (æ¨¡å¼åç§°, æ´»è·ƒç®—æ³•æ•°é‡)
                           æ¨¡å¼: "multi" (å¤šç®—æ³•), "single" (å•ç®—æ³•), "none" (æ— æ•°æ®)
        """
        # æ£€æŸ¥æ´»è·ƒçš„å¤šç®—æ³•
        active_algorithms = []
        if self.multi_algorithm_manager:
            active_algorithms = self.get_active_algorithms()

        if active_algorithms:
            # æœ‰æ´»è·ƒçš„å¤šç®—æ³•
            return "multi", len(active_algorithms)
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰å•ç®—æ³•æ•°æ®
            analyzer = self._get_current_analyzer()
            if analyzer:
                # æœ‰å•ç®—æ³•åˆ†æå™¨
                return "single", 1
            else:
                # ä¸¤è€…éƒ½æ²¡æœ‰
                return "none", 0

    def has_active_multi_algorithm_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„å¤šç®—æ³•æ•°æ®"""
        if self.multi_algorithm_manager:
            active_algorithms = self.get_active_algorithms()
            return len(active_algorithms) > 0
        return False

    def has_single_algorithm_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å•ç®—æ³•æ•°æ®"""
        analyzer = self._get_current_analyzer()
        return analyzer is not None
    
    async def add_algorithm(self, algorithm_name: str, filename: str, 
                           contents: bytes) -> Tuple[bool, str]:
        """
        æ·»åŠ ç®—æ³•åˆ°å¤šç®—æ³•ç®¡ç†å™¨ï¼ˆå¼‚æ­¥ï¼‰
        
        Args:
            algorithm_name: ç®—æ³•åç§°ï¼ˆç”¨æˆ·æŒ‡å®šï¼‰
            filename: æ–‡ä»¶å
            contents: æ–‡ä»¶å†…å®¹ï¼ˆäºŒè¿›åˆ¶æ•°æ®ï¼‰
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        # multi_algorithm_manager åœ¨åˆå§‹åŒ–æ—¶å·²åˆ›å»º
        
        try:
            # åŠ è½½SPMIDæ•°æ®
            from .spmid_loader import SPMIDLoader
            loader = SPMIDLoader()
            success = loader.load_spmid_data(contents)
            
            if not success:
                return False, "SPMIDæ–‡ä»¶è§£æå¤±è´¥"
            
            # è·å–æ•°æ®
            record_data = loader.get_record_data()
            replay_data = loader.get_replay_data()
            
            if not record_data or not replay_data:
                return False, "æ•°æ®ä¸ºç©º"
            
            # å¼‚æ­¥æ·»åŠ ç®—æ³•
            success, error_msg = await self.multi_algorithm_manager.add_algorithm_async(
                algorithm_name, filename, record_data, replay_data
            )

            return success, error_msg
            
        except Exception as e:
            logger.error(f"æ·»åŠ ç®—æ³•å¤±è´¥: {e}")
            
            logger.error(traceback.format_exc())
            return False, str(e)
    
    def remove_algorithm(self, algorithm_name: str) -> bool:
        """
        ä»å¤šç®—æ³•ç®¡ç†å™¨ä¸­ç§»é™¤ç®—æ³•

        Args:
            algorithm_name: ç®—æ³•åç§°

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_manager:
            return False

        return self.multi_algorithm_manager.remove_algorithm(algorithm_name)
    
    def get_all_algorithms(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰ç®—æ³•çš„ä¿¡æ¯åˆ—è¡¨
        
        Returns:
            List[Dict[str, Any]]: ç®—æ³•ä¿¡æ¯åˆ—è¡¨
        """
        if not self.multi_algorithm_manager:
            return []
        
        algorithms = []
        for alg in self.multi_algorithm_manager.get_all_algorithms():
            # ç¡®ä¿is_activeæœ‰å€¼ï¼Œå¦‚æœä¸ºNoneåˆ™é»˜è®¤ä¸ºTrueï¼ˆæ–°ä¸Šä¼ çš„æ–‡ä»¶åº”è¯¥é»˜è®¤æ˜¾ç¤ºï¼‰
            is_active = alg.is_active if alg.is_active is not None else True
            if alg.is_active is None:
                alg.is_active = True
                logger.info(f"âœ… ç¡®ä¿ç®—æ³• '{alg.metadata.algorithm_name}' é»˜è®¤æ˜¾ç¤º: is_active={is_active}")
            
            algorithms.append({
                'algorithm_name': alg.metadata.algorithm_name,  # å†…éƒ¨å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºæŸ¥æ‰¾ï¼‰
                'display_name': alg.metadata.display_name,  # æ˜¾ç¤ºåç§°ï¼ˆç”¨äºUIæ˜¾ç¤ºï¼‰
                'filename': alg.metadata.filename,
                'status': alg.metadata.status.value,
                'is_active': is_active,
                'color': alg.color,
                'is_ready': alg.is_ready()
            })
        
        return algorithms
    
    def get_key_matched_pairs_by_algorithm(self, algorithm_name: str, key_id: int) -> List[Tuple[int, int, Any, Any, float]]:
        """
        è·å–æŒ‡å®šç®—æ³•å’ŒæŒ‰é”®IDçš„æ‰€æœ‰åŒ¹é…å¯¹ï¼ŒæŒ‰æ—¶é—´æˆ³æ’åº

        Args:
            algorithm_name: ç®—æ³•åç§°
            key_id: æŒ‰é”®ID

        Returns:
            List[Tuple[int, int, Note, Note, float]]: åŒ¹é…å¯¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(record_index, replay_index, record_note, replay_note, record_keyon)
        """
        if not self.multi_algorithm_manager:
            return []

        # æ ¹æ® display_name æŸ¥æ‰¾ç®—æ³•
        algorithm = None
        for alg in self.multi_algorithm_manager.get_all_algorithms():
            if alg.metadata.display_name == algorithm_name:
                algorithm = alg
                break

        if not algorithm or not algorithm.is_ready() or not algorithm.analyzer:
            return []

        matched_pairs = algorithm.analyzer.matched_pairs if hasattr(algorithm.analyzer, 'matched_pairs') else []
        if not matched_pairs:
            return []

        # è·å–åç§»å¯¹é½æ•°æ®ä»¥è·å–æ—¶é—´æˆ³
        offset_data = algorithm.analyzer.get_offset_alignment_data()
        offset_map = {}
        for item in offset_data:
            record_idx = item.get('record_index')
            replay_idx = item.get('replay_index')
            if record_idx is not None and replay_idx is not None:
                offset_map[(record_idx, replay_idx)] = item.get('record_keyon', 0)

        # ç­›é€‰æŒ‡å®šæŒ‰é”®IDçš„åŒ¹é…å¯¹
        key_pairs = []
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            if record_note.id == key_id:
                record_keyon = offset_map.get((record_idx, replay_idx), 0)
                key_pairs.append((record_idx, replay_idx, record_note, replay_note, record_keyon))

        # æŒ‰æ—¶é—´æˆ³æ’åº
        key_pairs.sort(key=lambda x: x[4])

        return key_pairs
    
    def get_active_algorithms(self) -> List[AlgorithmDataset]:
        """
        è·å–æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯”æ˜¾ç¤ºï¼‰
        
        Returns:
            List[AlgorithmDataset]: æ¿€æ´»çš„ç®—æ³•åˆ—è¡¨
        """
        if not self.multi_algorithm_manager:
            return []
        
        return self.multi_algorithm_manager.get_active_algorithms()
    
    def toggle_algorithm(self, algorithm_name: str) -> bool:
        """
        åˆ‡æ¢ç®—æ³•çš„æ˜¾ç¤º/éšè—çŠ¶æ€
        
        Args:
            algorithm_name: ç®—æ³•åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_manager:
            return False
        
        return self.multi_algorithm_manager.toggle_algorithm(algorithm_name)
    
    def rename_algorithm(self, old_name: str, new_name: str) -> bool:
        """
        é‡å‘½åç®—æ³•
        
        Args:
            old_name: æ—§åç§°
            new_name: æ–°åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.multi_algorithm_manager:
            return False
        
        return self.multi_algorithm_manager.rename_algorithm(old_name, new_name)
    
    def get_multi_algorithm_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤šç®—æ³•å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.multi_algorithm_manager:
            return {}
        
        return self.multi_algorithm_manager.get_comparison_statistics()
    
    def get_same_algorithm_relative_delay_analysis(self) -> Dict[str, Any]:
        """åˆ†æåŒç§ç®—æ³•ä¸åŒæ›²å­çš„ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒï¼ˆå§”æ‰˜ç»™RelativeDelayAnalyzerï¼‰"""
        return self.relative_delay_analyzer.analyze_same_algorithm_relative_delays()
    
    def get_relative_delay_range_data_points_by_subplot(
        self, 
        display_name: str, 
        filename_display: str, 
        relative_delay_min_ms: float, 
        relative_delay_max_ms: float
    ) -> List[Dict[str, Any]]:
        """æ ¹æ®å­å›¾ä¿¡æ¯è·å–æŒ‡å®šç›¸å¯¹å»¶æ—¶èŒƒå›´å†…çš„æ•°æ®ç‚¹è¯¦æƒ…ï¼ˆå§”æ‰˜ç»™RelativeDelayAnalyzerï¼‰"""
        return self.relative_delay_analyzer.get_data_points_by_range(
            display_name, filename_display, relative_delay_min_ms, relative_delay_max_ms
        )
    
    def generate_relative_delay_distribution_plot(self) -> Any:
        """ç”ŸæˆåŒç§ç®—æ³•ä¸åŒæ›²å­çš„ç›¸å¯¹å»¶æ—¶åˆ†å¸ƒå›¾ï¼ˆå§”æ‰˜ç»™PlotServiceï¼‰"""
        return self.plot_service.generate_relative_delay_distribution_plot()

    def get_delay_metrics(self, algorithm=None) -> Dict[str, Any]:
        """
        è·å–å»¶æ—¶è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡

        Args:
            algorithm: æŒ‡å®šç®—æ³•ï¼ˆç”¨äºå¤šç®—æ³•æ¨¡å¼ä¸‹çš„å•ç®—æ³•æŸ¥è¯¢ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«å»¶æ—¶è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡çš„æ•°æ®
        """
        try:
            # å¦‚æœæŒ‡å®šäº†ç®—æ³•ï¼Œä½¿ç”¨è¯¥ç®—æ³•çš„æ•°æ®
            if algorithm:
                if algorithm.analyzer and algorithm.analyzer.note_matcher:
                    delay_metrics = algorithm.analyzer.note_matcher._get_delay_metrics()
                    return delay_metrics.get_all_metrics()
                else:
                    return {'error': f'ç®—æ³• {algorithm} æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨'}

            # å•ç®—æ³•æ¨¡å¼
            if self.analyzer and self.analyzer.note_matcher:
                delay_metrics = self.analyzer.note_matcher._get_delay_metrics()
                return delay_metrics.get_all_metrics()
            else:
                return {'error': 'æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨'}

        except Exception as e:
            logger.error(f"è·å–å»¶æ—¶è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_graded_error_stats(self, algorithm=None) -> Dict[str, Any]:
        """
        è·å–åˆ†çº§è¯¯å·®ç»Ÿè®¡æ•°æ®

        Args:
            algorithm: æŒ‡å®šç®—æ³•ï¼ˆç”¨äºå¤šç®—æ³•æ¨¡å¼ä¸‹çš„å•ç®—æ³•æŸ¥è¯¢ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«å„çº§åˆ«è¯¯å·®ç»Ÿè®¡çš„æ•°æ®
        """
        from utils.constants import GRADE_LEVELS
        
        try:
            # å¦‚æœæŒ‡å®šäº†ç®—æ³•ï¼Œä½¿ç”¨è¯¥ç®—æ³•çš„æ•°æ®
            if algorithm:
                if algorithm.analyzer and algorithm.analyzer.note_matcher:
                    return algorithm.analyzer.note_matcher.get_graded_error_stats()
                else:
                    return {'error': f'ç®—æ³• {algorithm} æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨'}

            # è·å–æ¿€æ´»çš„ç®—æ³•å¹¶ç”Ÿæˆæ•°æ®
            active_algorithms = self.get_active_algorithms()

            if not active_algorithms:
                return {'error': 'æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•'}

            is_multi_algorithm = len(active_algorithms) > 1

            # å¤šç®—æ³•æ¨¡å¼ï¼šæ±‡æ€»æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„è¯„çº§ç»Ÿè®¡
            if is_multi_algorithm:
                # åŠ¨æ€åˆå§‹åŒ–
                total_stats = {level: {'count': 0, 'percent': 0.0} for level in GRADE_LEVELS}

                total_count = 0
                for algorithm in active_algorithms:
                    if algorithm.analyzer and algorithm.analyzer.note_matcher:
                        alg_stats = algorithm.analyzer.note_matcher.get_graded_error_stats()
                        if alg_stats and 'error' not in alg_stats:
                            for level in GRADE_LEVELS:
                                if level in alg_stats:
                                    total_stats[level]['count'] += alg_stats[level].get('count', 0)
                                    total_count += alg_stats[level].get('count', 0)

                # è®¡ç®—ç™¾åˆ†æ¯”ï¼Œç¡®ä¿æ€»å’Œä¸º100%ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
                if total_count > 0:
                    raw_percentages = {}
                    for level in GRADE_LEVELS:
                        raw_percentages[level] = (total_stats[level]['count'] / total_count) * 100.0

                    rounded_percentages = {}
                    for level in GRADE_LEVELS:
                        rounded_percentages[level] = round(raw_percentages[level], 4)

                    total_rounded = sum(rounded_percentages.values())
                    if total_rounded != 100.0:
                        max_level = max(rounded_percentages.keys(), key=lambda x: rounded_percentages[x])
                        rounded_percentages[max_level] += (100.0 - total_rounded)

                    for level in GRADE_LEVELS:
                        total_stats[level]['percent'] = rounded_percentages[level]

                return total_stats

            # å•ç®—æ³•æ¨¡å¼
            else:
                single_algorithm = active_algorithms[0]
                if not single_algorithm.analyzer or not single_algorithm.analyzer.note_matcher:
                    return {'error': 'ç®—æ³•æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æå™¨æˆ–éŸ³ç¬¦åŒ¹é…å™¨'}

                return single_algorithm.analyzer.note_matcher.get_graded_error_stats()

        except Exception as e:
            logger.error(f"è·å–åˆ†çº§è¯¯å·®ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}
