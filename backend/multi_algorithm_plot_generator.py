#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨

è´Ÿè´£ç”Ÿæˆæ”¯æŒå¤šç®—æ³•å¯¹æ¯”çš„å›¾è¡¨ï¼Œä½¿ç”¨é¢å‘å¯¹è±¡è®¾è®¡ã€‚
"""
import math
import traceback
from typing import List, Optional, Any, Dict, Tuple
import plotly.graph_objects as go
import numpy as np
from backend.multi_algorithm_manager import AlgorithmDataset
from utils.logger import Logger
from utils.colors import ALGORITHM_COLOR_PALETTE
from spmid.note_matcher import MatchType

logger = Logger.get_logger()


class MultiAlgorithmPlotGenerator:
    """
    å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨ç±»
    
    è´Ÿè´£ç”Ÿæˆæ”¯æŒå¤šç®—æ³•å¯¹æ¯”çš„å›¾è¡¨ï¼ŒåŒ…æ‹¬ï¼š
    - ç€‘å¸ƒå›¾ï¼ˆå¤šç®—æ³•å åŠ æ˜¾ç¤ºï¼‰
    - åç§»å¯¹é½åˆ†æå›¾ï¼ˆå¤šç®—æ³•å¹¶æ’æŸ±çŠ¶å›¾ï¼‰
    - å»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå¤šç®—æ³•å åŠ æ˜¾ç¤ºï¼‰
    """
    
    def __init__(self, key_filter=None):
        """
        åˆå§‹åŒ–å¤šç®—æ³•å›¾è¡¨ç”Ÿæˆå™¨
        
        Args:
            key_filter: æŒ‰é”®è¿‡æ»¤å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.key_filter = key_filter
        
        # ä½¿ç”¨å…¨å±€é¢œè‰²æ–¹æ¡ˆ
        self.COLORS = ALGORITHM_COLOR_PALETTE
        
    
    def generate_unified_waterfall_plot(
        self,
        backend,                        # åç«¯å®ä¾‹ï¼Œç”¨äºè·å–å…¨å±€å¹³å‡å»¶æ—¶
        analyzers: List[Any],           # åˆ†æå™¨åˆ—è¡¨ï¼Œæ ¹æ®SPMIDæ–‡ä»¶æ•°é‡è‡ªåŠ¨å¤„ç†
        algorithm_names: List[str],     # ç®—æ³•åç§°åˆ—è¡¨
        time_filter=None,
        key_filter=None,
        data_types: List[str] = None,   # è¦æ˜¾ç¤ºçš„æ•°æ®ç±»å‹åˆ—è¡¨
        key_ids: List[int] = None       # è¦æ˜¾ç¤ºçš„æŒ‰é”®IDåˆ—è¡¨
    ) -> Any:
        """
        ç”Ÿæˆç»Ÿä¸€çš„ç€‘å¸ƒå›¾ï¼ˆè‡ªåŠ¨æ ¹æ®SPMIDæ–‡ä»¶æ•°é‡å¤„ç†ï¼‰
        
        æ ¹æ®analyzersçš„æ•°é‡è‡ªåŠ¨åˆ¤æ–­ï¼š
        - 1ä¸ªæ–‡ä»¶ï¼šä¸éœ€è¦yè½´åç§»
        - å¤šä¸ªæ–‡ä»¶ï¼šæ¯ä¸ªæ–‡ä»¶åˆ†é…ç‹¬ç«‹çš„yè½´èŒƒå›´
        
        Args:
            backend: åç«¯å®ä¾‹
            analyzers: åˆ†æå™¨åˆ—è¡¨
            algorithm_names: ç®—æ³•åç§°åˆ—è¡¨
            time_filter: æ—¶é—´è¿‡æ»¤å™¨
            key_filter: æŒ‰é”®è¿‡æ»¤å™¨
            
        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        if not analyzers:
            logger.warning("æ²¡æœ‰åˆ†æå™¨ï¼Œæ— æ³•ç”Ÿæˆç€‘å¸ƒå›¾")
            return self._create_empty_plot("æ²¡æœ‰åˆ†æå™¨")

        try:
            # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ–‡ä»¶æ¨¡å¼
            is_multi_file = len(analyzers) > 1
            logger.info(f"å¼€å§‹ç”Ÿæˆç€‘å¸ƒå›¾ï¼Œå…± {len(analyzers)} ä¸ªSPMIDæ–‡ä»¶")

            # æ ¹æ®æ–‡ä»¶æ•°é‡å’Œç­›é€‰èŒƒå›´å†³å®šæ˜¯å¦åˆ†é…y_offsetèŒƒå›´
            if is_multi_file:
                # å¦‚æœç­›é€‰äº†å°‘é‡æŒ‰é”®ï¼Œå‡å°åç§»é‡ä»¥ä¾¿åœ¨æœ‰é™çš„è§†è§‰èŒƒå›´å†…å¯¹æ¯”ä¸åŒç®—æ³•/æ–‡ä»¶
                if key_ids and len(key_ids) <= 5:
                    algorithm_y_range = 10
                else:
                    algorithm_y_range = 100
            else:
                algorithm_y_range = 0

            # è·å–å¹³å‡å»¶æ—¶æ•°æ®
            avg_delay_ms = self._get_average_delay(backend, is_multi_file, algorithm_names)
            
            # æ”¶é›†æ‰€æœ‰æ•°æ®ç‚¹ç”¨äºå…¨å±€å½’ä¸€åŒ–
            all_bars_by_algorithm = []

            # å¤„ç†æ¯ä¸ªåˆ†æå™¨
            for alg_idx, (analyzer, algorithm_name) in enumerate(zip(analyzers, algorithm_names)):
                if not analyzer:
                    logger.warning(f"åˆ†æå™¨ '{algorithm_name}' ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # è®¡ç®—å½“å‰ç®—æ³•çš„y_offset
                current_y_offset = alg_idx * algorithm_y_range if is_multi_file else 0

                # æ”¶é›†å½“å‰åˆ†æå™¨çš„æ•°æ®ï¼ˆæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ•°æ®ç±»å‹å’ŒæŒ‰é”®ï¼‰
                algorithm_bars = self._collect_algorithm_data_by_types(
                    analyzer, current_y_offset, algorithm_name, alg_idx, avg_delay_ms, data_types, key_ids
                )

                all_bars_by_algorithm.append({
                    'analyzer': analyzer,
                    'bars': algorithm_bars,
                    'algorithm_name': algorithm_name,
                    'y_offset': current_y_offset
                })

            if not all_bars_by_algorithm:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ç‚¹ï¼Œæ— æ³•ç”Ÿæˆç€‘å¸ƒå›¾")
                return self._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ç‚¹")

            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„é”¤é€Ÿå€¼å¹¶è®¡ç®—å…¨å±€èŒƒå›´ï¼ˆç”¨äºé¢œè‰²å½’ä¸€åŒ–ï¼‰
            all_values = self._collect_velocity_values(all_bars_by_algorithm)
            vmin, vmax = self._calculate_velocity_range(all_values)

            # åˆ›å»ºå›¾è¡¨
            fig = go.Figure()
            
            # ä¸ºæ¯ä¸ªæ¡å½¢æ®µæ·»åŠ trace
            total_bars = 0
            drop_hammer_bars = 0
            multi_hammer_bars = 0
            matched_bars = 0
            
            for alg_data in all_bars_by_algorithm:
                bars = alg_data['bars']
                algorithm_name = alg_data['algorithm_name']

                logger.info(f"ç®—æ³• '{algorithm_name}': å‡†å¤‡ç»˜åˆ¶ {len(bars)} ä¸ªbars")

                for bar in bars:
                    total_bars += 1
                    data_type = bar.get('data_type', '')
                    if data_type == 'drop_hammer':
                        drop_hammer_bars += 1
                    elif data_type == 'multi_hammer':
                        multi_hammer_bars += 1
                    else:
                        matched_bars += 1

                    # æ·»åŠ barçš„trace
                    success = self._add_waterfall_bar_trace(fig, bar, algorithm_name, vmin, vmax)
                    if not success:
                        # æ•°æ®æ— æ•ˆè¢«è·³è¿‡ï¼Œç»Ÿè®¡éœ€è¦è°ƒæ•´
                        total_bars -= 1
                        if data_type == 'drop_hammer':
                            drop_hammer_bars -= 1
                        elif data_type == 'multi_hammer':
                            multi_hammer_bars -= 1
                        else:
                            matched_bars -= 1
            
            # é…ç½®å›¾è¡¨å¸ƒå±€
            self._configure_unified_waterfall_layout(fig, all_bars_by_algorithm, is_multi_file)

            logger.info(f"ç€‘å¸ƒå›¾ç”ŸæˆæˆåŠŸ: æ€»è®¡ {total_bars} ä¸ªbars (åŒ¹é…å¯¹: {matched_bars}, ä¸¢é”¤: {drop_hammer_bars}, å¤šé”¤: {multi_hammer_bars})")
            return fig

        except Exception as e:
            return self._handle_generation_error(e, "ç€‘å¸ƒå›¾")

    def _collect_algorithm_comprehensive_data(self, analyzer, y_offset: float, algorithm_name: str, alg_idx: int, avg_delay_ms: float = 0.0) -> List[Dict]:
        """
        æ”¶é›†å•ä¸ªç®—æ³•çš„å®Œæ•´ç€‘å¸ƒå›¾æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ•°æ®æ¥å£ä½œä¸ºé™çº§/å®Œæ•´æ”¶é›†æ–¹æ¡ˆï¼‰

        Args:
            analyzer: SPMIDAnalyzerå®ä¾‹
            y_offset: Yè½´åç§»é‡
            algorithm_name: ç®—æ³•åç§°
            alg_idx: ç®—æ³•ç´¢å¼•
            avg_delay_ms: å¹³å‡å»¶æ—¶

        Returns:
            List[Dict]: è¯¥ç®—æ³•çš„æ‰€æœ‰ç€‘å¸ƒå›¾æ•°æ®
        """
        algorithm_bars = []
        logger.info(f"å¼€å§‹æ”¶é›†ç®—æ³• '{algorithm_name}' çš„å®Œæ•´ç€‘å¸ƒå›¾æ•°æ®")

        # ä½¿ç”¨ NoteMatcher çš„ç»Ÿä¸€æ•°æ®æ¥å£
        note_matcher = getattr(analyzer, 'note_matcher', None)
        if note_matcher and hasattr(note_matcher, 'get_all_display_data'):
            display_data = note_matcher.get_all_display_data()

            # éå†æ‰€æœ‰å·²çŸ¥ç±»åˆ«è¿›è¡Œæ”¶é›†
            for category in ['matched_pairs', 'drop_hammers', 'multi_hammers', 'abnormal_matches']:
                results = display_data.get(category, [])
                bars = self._collect_matched_pair_data(
                    analyzer, y_offset, algorithm_name, avg_delay_ms, results
                )
                algorithm_bars.extend(bars)

        return algorithm_bars

    def _collect_algorithm_data_by_types(self, analyzer, y_offset: float, algorithm_name: str, alg_idx: int, avg_delay_ms: float, data_types: List[str] = None, key_ids: List[int] = None) -> List[Dict]:
        """
        æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ•°æ®ç±»å‹æ”¶é›†ç®—æ³•æ•°æ®

        Args:
            analyzer: SPMIDAnalyzerå®ä¾‹
            y_offset: Yè½´åç§»é‡
            algorithm_name: ç®—æ³•åç§°
            alg_idx: ç®—æ³•ç´¢å¼•
            avg_delay_ms: å¹³å‡å»¶æ—¶
            data_types: ç”¨æˆ·é€‰æ‹©çš„æ•°æ®ç±»å‹åˆ—è¡¨
            key_ids: ç”¨æˆ·é€‰æ‹©çš„æŒ‰é”®IDåˆ—è¡¨

        Returns:
            List[Dict]: è¯¥ç®—æ³•çš„ç€‘å¸ƒå›¾æ•°æ®ï¼ˆåªåŒ…å«é€‰æ‹©çš„æ•°æ®ç±»å‹å’ŒæŒ‰é”®ï¼‰
        """
        algorithm_bars = []

        logger.info(f"å¼€å§‹æ”¶é›†ç®—æ³• '{algorithm_name}' çš„æ•°æ®ï¼Œç±»å‹: {data_types}")

        # è·å– NoteMatcher çš„ç»Ÿä¸€æ•°æ®æ¥å£
        note_matcher = getattr(analyzer, 'note_matcher', None)
        if not note_matcher:
            logger.warning(f"NoteMatcher æ²¡æœ‰ç»Ÿä¸€æ•°æ®æ¥å£ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ”¶é›†ç®—æ³• '{algorithm_name}' çš„æ•°æ®")
            # é™çº§åˆ°ä¼ ç»Ÿæ–¹å¼
            bars = self._collect_algorithm_comprehensive_data(analyzer, y_offset, algorithm_name, alg_idx, avg_delay_ms)

            # å¯¹ä¼ ç»Ÿæ–¹å¼æ”¶é›†çš„æ•°æ®ä¹Ÿåº”ç”¨æŒ‰é”®è¿‡æ»¤
            if key_ids:
                bars = [bar for bar in bars if bar.get('key_id', 0) in key_ids]
                logger.info(f"ä¼ ç»Ÿæ–¹å¼æŒ‰é”®è¿‡æ»¤: {len(bars)} ä¸ªbars (æŒ‰é”®: {key_ids})")

            return bars

        display_data = note_matcher.get_all_display_data()

        # æ ¹æ®é€‰æ‹©çš„æ•°æ®ç±»å‹æ”¶é›†ç›¸åº”æ•°æ®
        if not data_types:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®ç±»å‹ï¼Œé»˜è®¤æ˜¾ç¤ºåŒ¹é…å¯¹
            data_types = ['matched_pairs']

        # æ˜ å°„å…³ç³»ï¼šUIç±»å‹æ ‡è¯† -> NoteMatcherä¸­çš„ç»“æœå
        type_mapping = {
            'matched_pairs': 'matched_pairs',
            'drop_hammers': 'drop_hammers',
            'multi_hammers': 'multi_hammers',
            'abnormal_matches': 'abnormal_matches'
        }

        # éå†é€‰æ‹©çš„ç±»å‹ï¼Œç»Ÿä¸€é€šè¿‡ _collect_matched_pair_data æ”¶é›†
        for ui_type in data_types:
            internal_key = type_mapping.get(ui_type)
            if internal_key and internal_key in display_data:
                results = display_data[internal_key]
                # æ³¨æ„ï¼šæ–°çš„ _collect_matched_pair_data å·²ç»å†…ç½®äº† key_ids è¿‡æ»¤
                bars = self._collect_matched_pair_data(
                    analyzer, y_offset, algorithm_name, avg_delay_ms, results, key_ids
                )
                algorithm_bars.extend(bars)

        logger.info(f"ç®—æ³• '{algorithm_name}' æ•°æ®æ”¶é›†å®Œæˆ: {len(algorithm_bars)} ä¸ªbars")
        return algorithm_bars

    def _collect_matched_pair_data(self, analyzer, y_offset: float, algorithm_name: str, avg_delay_ms: float, matched_results: List = None, key_ids: List[int] = None) -> List[Dict]:
        """
        æ”¶é›†åŒ¹é…ç›¸å…³æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ MatchResult å¯¹è±¡æ¥å£ï¼‰

        Args:
            analyzer: SPMIDAnalyzerå®ä¾‹
            y_offset: Yè½´åç§»é‡
            algorithm_name: ç®—æ³•åç§°
            avg_delay_ms: å¹³å‡å»¶æ—¶
            matched_results: MatchResultå¯¹è±¡åˆ—è¡¨
            key_ids: å¯é€‰çš„æŒ‰é”®IDè¿‡æ»¤åˆ—è¡¨

        Returns:
            List[Dict]: ç€‘å¸ƒå›¾æ•°æ®å—åˆ—è¡¨
        """
        bars = []

        # å¦‚æœæ²¡æœ‰æä¾›åŒ¹é…æ•°æ®ï¼Œä»analyzerå°è¯•è·å–
        if matched_results is None:
            note_matcher = getattr(analyzer, 'note_matcher', None)
            if note_matcher and hasattr(note_matcher, 'get_all_display_data'):
                display_data = note_matcher.get_all_display_data()
                matched_results = display_data.get('matched_pairs', [])
            else:
                logger.warning(f"æ— æ³•è·å–ç®—æ³• '{algorithm_name}' çš„åŒ¹é…æ•°æ®")
                return bars

        logger.info(f"å¼€å§‹å¤„ç†åŒ¹é…æ•°æ®: {len(matched_results)} ä¸ªç»“æœ")

        for result in matched_results:
            try:
                # ä» MatchResult å¯¹è±¡ä¸­å®‰å…¨è·å–å½•åˆ¶å’Œæ’­æ”¾éŸ³ç¬¦
                # pair å±æ€§å¯èƒ½ä¸º (record_note, replay_note), (record_note, None) æˆ– (None, replay_note)
                if not result.pair:
                    continue
                
                record_note, replay_note = result.pair
                
                # å®‰å…¨è·å–æœ‰æ•ˆçš„ ID (ä¼˜å…ˆä½¿ç”¨ record_note)
                target_note = record_note or replay_note
                if not target_note:
                    continue

                # æŒ‰é”®è¿‡æ»¤
                if key_ids and target_note.id not in key_ids:
                    continue

                # è®¡ç®—å»¶æ—¶å’Œè¯„çº§ï¼ˆç›´æ¥å¤ç”¨ result å¯¹è±¡ï¼‰
                grade_name, color_intensity, delay_ms, relative_delay_ms = self._calculate_match_grading(
                    result, record_note, replay_note, avg_delay_ms
                )

                # è·å–å”¯ä¸€æ ‡è¯†
                record_match_index = getattr(record_note, 'uuid', 'N/A') if record_note else 'N/A'
                replay_match_index = getattr(replay_note, 'uuid', 'N/A') if replay_note else 'N/A'
                note_key_id = target_note.id
                
                # ç¡®å®šæ•°æ®ç±»å‹æ ‡è¯† (drop_hammer / multi_hammer / None)
                data_type = None
                if not replay_note:
                    data_type = 'drop_hammer'
                elif not record_note:
                    data_type = 'multi_hammer'

                # 1. å¤„ç†å½•åˆ¶éƒ¨åˆ†
                record_bars = []
                if record_note and record_note.after_touch is not None:
                    record_bars = self._extract_note_bars_for_multi(
                        record_note, 'record', y_offset, color_intensity,
                        algorithm_name, grade_name, record_match_index, delay_ms, relative_delay_ms,
                        data_type=data_type, record_key_id=note_key_id
                    )
                    bars.extend(record_bars)

                # 2. å¤„ç†æ’­æ”¾éƒ¨åˆ†
                replay_bars = []
                if replay_note and replay_note.after_touch is not None:
                    replay_bars = self._extract_note_bars_for_multi(
                        replay_note, 'replay', y_offset, color_intensity,
                        algorithm_name, grade_name, replay_match_index, delay_ms, relative_delay_ms,
                        data_type=data_type, record_key_id=note_key_id
                    )
                    bars.extend(replay_bars)

                # 3. å¦‚æœä¸¤è¾¹éƒ½æœ‰æ•°æ®ï¼Œåˆå¹¶æ‚¬åœæç¤ºä¿¡æ¯ï¼Œå¹¶å†™å…¥ record_uuid/replay_uuid ä¾›ç‚¹å‡»å¼¹çª—æŸ¥æ‰¾åŒ¹é…å¯¹
                if record_bars and replay_bars:
                    self._merge_matched_hover_info(record_bars, replay_bars, avg_delay_ms)
                    ru = str(getattr(record_note, 'uuid', '') or '')
                    rpu = str(getattr(replay_note, 'uuid', '') or '')
                    for b in record_bars + replay_bars:
                        b['record_uuid'] = ru
                        b['replay_uuid'] = rpu

            except Exception as e:
                logger.warning(f"å¤„ç†åŒ¹é…ç»“æœé¡¹å¤±è´¥: {e}")
                continue

        return bars


    def _calculate_match_grading(self, result, record_note, replay_note, avg_delay_ms: float):
        """
        è®¡ç®—åŒ¹é…ç»“æœçš„è¯„çº§å’Œå»¶æ—¶ä¿¡æ¯

        Returns:
            tuple: (grade_name, color_intensity, delay_ms, relative_delay_ms)
        """
        grade_name = "æœªçŸ¥"
        delay_ms = 0.0
        relative_delay_ms = 0.0

        if result.is_success and record_note and replay_note:
            # è®¡ç®—å»¶æ—¶ï¼ˆç›´æ¥ä½¿ç”¨Note.key_on_msï¼Œå•ä½å·²æ˜¯msï¼‰
            if record_note.key_on_ms is not None and replay_note.key_on_ms is not None:
                delay_ms = replay_note.key_on_ms - record_note.key_on_ms
                relative_delay_ms = delay_ms - avg_delay_ms
            else:
                delay_ms = 0.0
                relative_delay_ms = 0.0

            # è¯„çº§
            if delay_ms <= 20:
                color_intensity, grade_name = 0.8, "ä¼˜ç§€"
            elif delay_ms <= 30:
                color_intensity, grade_name = 0.6, "è‰¯å¥½"
            elif delay_ms <= 50:
                color_intensity, grade_name = 0.4, "ä¸€èˆ¬"
            elif delay_ms <= 1000:
                color_intensity, grade_name = 0.3, "è¾ƒå·®"
            else:
                color_intensity, grade_name = 0.2, "ä¸¥é‡"
        else:
            color_intensity, grade_name = 0.1, "å¤±è´¥"
            relative_delay_ms = 0.0 - avg_delay_ms

        return grade_name, color_intensity, delay_ms, relative_delay_ms


    # TODO
    def _merge_matched_hover_info(self, record_bars: List[Dict], replay_bars: List[Dict], avg_delay_ms: float) -> None:
        """
        å°†åŒ¹é…çš„replayä¿¡æ¯åˆå¹¶åˆ°record barsçš„hoveræ–‡æœ¬ä¸­ï¼Œå®ç°ç»Ÿä¸€çš„æ‚¬åœæ˜¾ç¤ºã€‚

        Args:
            record_bars: å½•åˆ¶æ•°æ®çš„æ¡å½¢åˆ—è¡¨
            replay_bars: æ’­æ”¾æ•°æ®çš„æ¡å½¢åˆ—è¡¨
            avg_delay_ms: å¹³å‡å»¶æ—¶ï¼ˆæ¯«ç§’ï¼‰
        """
        # åŒ¹é…é€»è¾‘ï¼šæŒ‰é”®ä½å¯¹åº”ï¼ˆç¡®ä¿recordå’Œreplayçš„ç›¸åŒé”®ä½æ•°æ®é…å¯¹ï¼‰
        for record_bar in record_bars:
            # æŸ¥æ‰¾å¯¹åº”çš„replay barï¼ˆé€šè¿‡original_key_idåŒ¹é…ï¼‰
            replay_info = None
            record_key_id = record_bar.get('original_key_id')

            for replay_bar in replay_bars:
                if replay_bar.get('original_key_id') == record_key_id:
                    replay_info = replay_bar
                    break

            if replay_info:
                # è·å–recordçš„åŸå§‹æ–‡æœ¬
                original_text = record_bar.get('text', '')

                # æå–replayç›¸å…³çš„å®Œæ•´ä¿¡æ¯
                replay_velocity = replay_info.get('velocity', 'N/A')
                replay_key_press = replay_info.get('t_on', 0) / 10
                replay_key_release = replay_info.get('t_off', 0) / 10
                replay_grade = replay_info.get('grade_name', 'æœªçŸ¥')
                replay_match_index = replay_info.get('match_index', 'N/A')
                replay_delay_ms = replay_info.get('delay_ms', 0.0)
                replay_relative_delay_ms = replay_info.get('relative_delay_ms', 0.0)
                replay_first_hammer = replay_info.get('first_hammer_time', 'N/A')

                # åœ¨recordçš„hoveræ–‡æœ¬ä¸­æ·»åŠ å®Œæ•´çš„replayä¿¡æ¯éƒ¨åˆ†
                replay_section = '<br><b>æ’­æ”¾æ•°æ®:</b><br>' + \
                            f'ç±»å‹: replay<br>' + \
                            f'é”®ä½: {record_key_id}<br>' + \
                            f'é”¤é€Ÿ: {replay_velocity}<br>' + \
                            f'ç­‰çº§: {replay_grade}<br>' + \
                            f'ç´¢å¼•: {replay_match_index}<br>' + \
                            f'ç»å¯¹å»¶æ—¶: {replay_delay_ms:.2f}ms<br>' + \
                            f'ç›¸å¯¹å»¶æ—¶: {replay_relative_delay_ms:+.2f}ms<br>' + \
                            f'å¹³å‡å»¶æ—¶: {avg_delay_ms:.2f}ms<br>' + \
                            f'é¦–é”¤æ—¶é—´: {replay_first_hammer} ({replay_first_hammer/10:.2f}ms)<br>' + \
                            f'æŒ‰é”®æŒ‰ä¸‹: {replay_key_press:.2f}ms<br>' + \
                            f'æŒ‰é”®é‡Šæ”¾: {replay_key_release:.2f}ms<br>'

                merged_text = original_text + replay_section

                record_bar['text'] = merged_text
                # ä¸ºreplay baråˆ›å»ºç‹¬ç«‹çš„æ‚¬åœä¿¡æ¯
                replay_text = '<b>æ’­æ”¾æ•°æ®:</b><br>' + \
                             f'ç±»å‹: replay<br>' + \
                             f'é”®ä½: {record_key_id}<br>' + \
                             f'é”¤é€Ÿ: {replay_velocity}<br>' + \
                             f'ç­‰çº§: {replay_grade}<br>' + \
                             f'ç´¢å¼•: {replay_match_index}<br>' + \
                             f'ç»å¯¹å»¶æ—¶: {replay_delay_ms:.2f}ms<br>' + \
                             f'ç›¸å¯¹å»¶æ—¶: {replay_relative_delay_ms:+.2f}ms<br>' + \
                             f'å¹³å‡å»¶æ—¶: {avg_delay_ms:.2f}ms<br>' + \
                             f'é¦–é”¤æ—¶é—´: {replay_first_hammer} ({replay_first_hammer/10:.2f}ms)<br>' + \
                             f'æŒ‰é”®æŒ‰ä¸‹: {replay_key_press:.2f}ms<br>' + \
                             f'æŒ‰é”®é‡Šæ”¾: {replay_key_release:.2f}ms<br>'
                replay_info['text'] = replay_text
            else:
                logger.warning(f"é”®ä½ {record_key_id}: æœªæ‰¾åˆ°å¯¹åº”çš„æ’­æ”¾æ•°æ®ï¼Œæ— æ³•åˆå¹¶hoverä¿¡æ¯")
                # ä¸ºæ²¡æœ‰åŒ¹é…æ’­æ”¾æ•°æ®çš„record baræ·»åŠ æç¤º
                original_text = record_bar.get('text', '')
                no_replay_section = '<br><b>æ’­æ”¾æ•°æ®:</b><br>æœªæ‰¾åˆ°åŒ¹é…çš„æ’­æ”¾æ•°æ®<br>'
                record_bar['text'] = original_text + no_replay_section


    def _extract_note_bars_for_multi(self, note, label: str, y_offset: float, color_intensity: float, algorithm_name: str, grade_name: str = "æœªçŸ¥", match_index: str = "N/A", delay_ms: float = 0.0, relative_delay_ms: float = 0.0, data_type: str = None, record_key_id: int = None) -> List[Dict]:
        """
        ä¸ºå¤šç®—æ³•æ¨¡å¼æå–éŸ³ç¬¦æ¡å½¢æ•°æ®

        Args:
            note: Noteå¯¹è±¡
            label: 'record' æˆ– 'replay'
            y_offset: Yè½´åç§»é‡
            color_intensity: é¢œè‰²å¼ºåº¦ (0.0-1.0)
            algorithm_name: ç®—æ³•åç§°
            data_type: æ•°æ®ç±»å‹ ('drop_hammer', 'multi_hammer', None)

        Returns:
            List[Dict]: æ¡å½¢æ•°æ®åˆ—è¡¨
        """
        # éªŒè¯noteæ•°æ®
        if not note:
            return []
        
        key_id = note.id
        
        # ä»Noteå¯¹è±¡è·å–é¢„è®¡ç®—çš„æ—¶é—´å±æ€§
        # Noteå¯¹è±¡åœ¨æ„å»ºæ—¶å·²ç»è®¡ç®—äº†key_on_mså’Œkey_off_msï¼Œå¦‚æœafter_touchå­˜åœ¨åˆ™è®¡ç®—ï¼Œå¦åˆ™ä¸º0.0
        # å¯¹äºä¸¢é”¤/å¤šé”¤ï¼Œå®ƒä»¬è‚¯å®šæœ‰hammersæ•°æ®ï¼Œä½†å¯èƒ½æ²¡æœ‰after_touchæ•°æ®
        key_on_time = None
        key_off_time = None

        # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„key_on_ms/key_off_msï¼ˆå¦‚æœ>0ï¼Œè¯´æ˜æœ‰after_touchæ•°æ®ï¼‰
        if note.key_on_ms is not None and note.key_off_ms is not None and note.key_on_ms > 0 and note.key_off_ms > 0:
            # æœ‰å®Œæ•´çš„after_touchæ•°æ®ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„æ—¶é—´ï¼ˆè½¬æ¢ä¸ºåŸå§‹æ—¶é—´æˆ³å•ä½ï¼‰
            key_on_time = note.key_on_ms * 10.0
            key_off_time = note.key_off_ms * 10.0
        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„æ—¶é—´ï¼Œä½¿ç”¨hammersæ•°æ®ï¼ˆä¸¢é”¤/å¤šé”¤è‚¯å®šæœ‰hammersï¼‰
        elif note.hammers is not None and not note.hammers.empty:
            # ä½¿ç”¨é”¤å‡»æ•°æ®æ¨æ–­æ—¶é—´èŒƒå›´
            hammer_times = note.hammers.index.tolist()
            if hammer_times:
                min_time = min(hammer_times)
                max_time = max(hammer_times)
                # è½¬æ¢ä¸ºåŸå§‹æ—¶é—´æˆ³å•ä½ï¼ŒåŠ ä¸Šoffset
                key_on_time = (min_time + note.offset)
                key_off_time = (max_time + note.offset)
            else:
                logger.warning(f"âš ï¸ noteçš„hammersç´¢å¼•ä¸ºç©º: key_id={key_id}")
                return []
        else:
            # æ—¢æ²¡æœ‰é¢„è®¡ç®—æ—¶é—´ä¹Ÿæ²¡æœ‰hammersæ•°æ®ï¼Œè¿™ç§æƒ…å†µä¸åº”è¯¥å‡ºç°ï¼ˆå› ä¸ºNoteMatcherå·²ç»è¿‡æ»¤ï¼‰
            logger.warning(f"âš ï¸ noteç¼ºå°‘æ—¶é—´æ•°æ®å’Œé”¤å‡»æ•°æ®: key_id={key_id}")
            return []
        
        # éªŒè¯æ—¶é—´æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if key_on_time is None or key_off_time is None:
            logger.warning(f"âš ï¸ noteæ—¶é—´æ•°æ®æå–å¤±è´¥: key_id={key_id}")
            return []
        
        # key_on_time å’Œ key_off_time å·²ç»åœ¨ä¸Šé¢è®¡ç®—å¥½äº†ï¼ˆå•ä½ä¸ºåŸå§‹æ—¶é—´æˆ³ï¼Œéœ€è¦åç»­å¤„ç†ï¼‰
        
        try:
            # è®¡ç®—Yè½´ä½ç½®ï¼ˆå¯¹äºåŒ¹é…å¯¹ï¼Œä½¿ç”¨record_key_idç¡®ä¿recordå’Œreplayåœ¨åŒä¸€ä½ç½®ï¼‰
            actual_key_id = self._calculate_y_position(key_id, y_offset, label, record_key_id)
            
            # æå–é”¤é€Ÿä¿¡æ¯
            hammer_velocity = self._extract_hammer_velocity(note)
            
            # è§£æmatch_index
            source_index = self._parse_match_index(match_index)
            
            # åˆ›å»ºbarå­—å…¸
            bar = self._create_bar_dict(
                key_on_time, key_off_time, actual_key_id, key_id,
                hammer_velocity, color_intensity,
                algorithm_name, label, data_type, grade_name, match_index,
                source_index, delay_ms, relative_delay_ms
            )
            
            # ç”Ÿæˆhoveræ–‡æœ¬
            bar['text'] = self._generate_hover_text(
                bar, label, data_type, algorithm_name, key_id,
                hammer_velocity, grade_name, match_index,
                delay_ms, relative_delay_ms, key_on_time, key_off_time
            )

            return [bar]
            
        except (TypeError, ValueError, AttributeError) as e:
            logger.warning(f"ğŸš« åˆ›å»º {data_type} barå¤±è´¥: {e}")
            return []
    
    def _calculate_y_position(self, key_id: int, y_offset: float, label: str, record_key_id: int = None) -> float:
        """è®¡ç®—Yè½´ä½ç½®
        
        Args:
            key_id: å½“å‰noteçš„key_id
            y_offset: Yè½´åç§»é‡
            label: 'record' æˆ– 'replay'
            record_key_id: åŒ¹é…å¯¹ä¸­recordçš„key_idï¼ˆç”¨äºç¡®ä¿recordå’ŒreplayåŸºäºåŒä¸€æŒ‰é”®IDï¼‰
        """
        # å¯¹äºåŒ¹é…å¯¹ï¼Œä½¿ç”¨recordçš„key_idç¡®ä¿recordå’ŒreplayåŸºäºåŒä¸€æŒ‰é”®ID
        if record_key_id is not None:
            base_key_id = record_key_id + y_offset
        else:
            base_key_id = key_id + y_offset
        
        # å½•åˆ¶åœ¨ä¸‹ï¼Œæ’­æ”¾åœ¨ä¸Šï¼šç¡®ä¿æœ‰ç»†å¾®é—´è·
        if label == 'replay':
            actual_key_id = base_key_id + 0.15  # æ’­æ”¾å‘ä¸Šåç§»0.15ï¼Œä¸å½•åˆ¶ä¿æŒç»†å¾®é—´è·
        else:
            actual_key_id = base_key_id - 0.15  # å½•åˆ¶å‘ä¸‹åç§»0.15ï¼Œç¡®ä¿åœ¨æŒ‰é”®IDä¸‹æ–¹
        
        return actual_key_id
    
    def _extract_hammer_velocity(self, note) -> Any:
        """æå–é”¤é€Ÿä¿¡æ¯"""
        velocity = note.get_first_hammer_velocity()
        return velocity if velocity is not None else "N/A"

    def _normalize_velocity_value(self, velocity: float, vmin: float, vmax: float) -> float:
        """å°†é”¤é€Ÿå€¼å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ç”¨äºé¢œè‰²æ˜ å°„

        Args:
            velocity: é”¤é€Ÿå€¼
            vmin: é”¤é€Ÿæœ€å°å€¼
            vmax: é”¤é€Ÿæœ€å¤§å€¼

        Returns:
            float: å½’ä¸€åŒ–åçš„å€¼ (0.0-1.0)
        """
        if vmax > vmin:
            # æ­£å¸¸æƒ…å†µï¼šæ•°æ®æœ‰å˜åŒ–èŒƒå›´ï¼Œè¿›è¡Œmin-maxå½’ä¸€åŒ–
            return (velocity - vmin) / (vmax - vmin)
        elif vmax == vmin:
            # ç‰¹æ®Šæƒ…å†µï¼šæ‰€æœ‰é”¤é€Ÿå€¼éƒ½ç›¸åŒï¼Œæ‰€æœ‰ç‚¹ç”¨ç›¸åŒé¢œè‰²
            return 0.5  # ä¸­é—´è‰²è°ƒ
        else:
            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œä½†ä¿æŒå¥å£®æ€§
            return 0.5

    def _calculate_velocity_color(self, velocity: float, vmin: float, vmax: float) -> str:
        """æ ¹æ®é”¤é€Ÿå€¼è®¡ç®—é¢œè‰²

        Args:
            velocity: é”¤é€Ÿå€¼
            vmin: é”¤é€Ÿæœ€å°å€¼
            vmax: é”¤é€Ÿæœ€å¤§å€¼

        Returns:
            str: RGBAé¢œè‰²å­—ç¬¦ä¸²
        """
        import matplotlib.pyplot as plt
        cmap = plt.colormaps['YlOrRd']  # ä»æµ…é»„åˆ°æ·±çº¢ï¼Œè¶Šå¤§è¶Šæ·±

        # å½’ä¸€åŒ–å¹¶æ˜ å°„åˆ°é¢œè‰²
        normalized = self._normalize_velocity_value(velocity, vmin, vmax)
        # å¢åŠ ä¸é€æ˜åº¦ï¼Œç¡®ä¿é¢œè‰²æ›´æ˜æ˜¾ï¼ˆä»0.9æé«˜åˆ°0.95ï¼‰
        color = 'rgba' + str(tuple(int(255*x) for x in cmap(normalized)[:3]) + (0.95,))
        return color

    def _create_bar_trace_name(self, algorithm_name: str, data_type: str, bar_label: str) -> str:
        """åˆ›å»ºbarçš„traceåç§°

        Args:
            algorithm_name: ç®—æ³•åç§°
            data_type: æ•°æ®ç±»å‹ ('drop_hammer', 'multi_hammer', æˆ–å…¶ä»–)
            bar_label: baræ ‡ç­¾

        Returns:
            str: traceåç§°
        """
        if data_type == 'drop_hammer':
            return f"{algorithm_name} - ä¸¢é”¤"
        elif data_type == 'multi_hammer':
            return f"{algorithm_name} - å¤šé”¤"
        else:
            return f"{algorithm_name} - {bar_label}"

    def _collect_velocity_values(self, all_bars_by_algorithm: List[Dict]) -> List[float]:
        """ä»æ‰€æœ‰ç®—æ³•çš„barsä¸­æ”¶é›†æœ‰æ•ˆçš„é”¤é€Ÿå€¼

        Args:
            all_bars_by_algorithm: æ‰€æœ‰ç®—æ³•çš„barsæ•°æ®åˆ—è¡¨

        Returns:
            List[float]: æœ‰æ•ˆçš„é”¤é€Ÿå€¼åˆ—è¡¨
        """
        all_values = []
        total_bars = 0
        valid_bars = 0
        na_bars = 0
        
        for alg_data in all_bars_by_algorithm:
            for bar in alg_data['bars']:
                total_bars += 1
                velocity = bar.get('velocity')
                if velocity != "N/A" and isinstance(velocity, (int, float)):
                    all_values.append(velocity)
                    valid_bars += 1
                else:
                    na_bars += 1
        
        logger.info(f"ğŸ¨ é”¤é€Ÿæ”¶é›†ç»Ÿè®¡: æ€»bars={total_bars}, æœ‰æ•ˆé”¤é€Ÿ={valid_bars}, æ— æ•ˆ/N/A={na_bars}")
        if all_values:
            logger.info(f"ğŸ¨ é”¤é€ŸèŒƒå›´: min={min(all_values)}, max={max(all_values)}, æ ·æœ¬æ•°={len(all_values)}")
        
        return all_values

    def _calculate_velocity_range(self, velocity_values: List[float]) -> Tuple[float, float]:
        """è®¡ç®—é”¤é€Ÿå€¼çš„å…¨å±€èŒƒå›´

        Args:
            velocity_values: é”¤é€Ÿå€¼åˆ—è¡¨

        Returns:
            Tuple[float, float]: (vmin, vmax)
        """
        if velocity_values:
            return min(velocity_values), max(velocity_values)
        else:
            return 0.0, 1.0

    def _add_waterfall_bar_trace(self, fig: go.Figure, bar: Dict, algorithm_name: str,
                                vmin: float, vmax: float) -> bool:
        """æ·»åŠ å•ä¸ªç€‘å¸ƒå›¾barçš„trace

        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            bar: baræ•°æ®å­—å…¸
            algorithm_name: ç®—æ³•åç§°
            vmin: é”¤é€Ÿæœ€å°å€¼
            vmax: é”¤é€Ÿæœ€å¤§å€¼

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ ï¼ˆFalseè¡¨ç¤ºæ•°æ®æ— æ•ˆè¢«è·³è¿‡ï¼‰
        """
        # è·å–é”¤é€Ÿæ•°æ®å¹¶è®¡ç®—é¢œè‰²
        velocity = bar.get('velocity')
        data_type = bar.get('data_type', '')
        
        # å¯¹äºä¸¢é”¤/å¤šé”¤é”™è¯¯ï¼Œä½¿ç”¨æ›´æ˜æ˜¾çš„é¢œè‰²
        if data_type in ['drop_hammer', 'multi_hammer']:
            if data_type == 'drop_hammer':
                # ä¸¢é”¤ä½¿ç”¨æ˜æ˜¾çš„çº¢è‰²
                color = 'rgba(255, 0, 0, 0.9)'  # çº¢è‰²ï¼Œä¸é€æ˜
            else:
                # å¤šé”¤ä½¿ç”¨æ˜æ˜¾çš„æ©™è‰²
                color = 'rgba(255, 165, 0, 0.9)'  # æ©™è‰²ï¼Œä¸é€æ˜
        elif velocity == "N/A" or not isinstance(velocity, (int, float)):
            # å¦‚æœæ²¡æœ‰é”¤é€Ÿæ•°æ®ï¼Œä½¿ç”¨æ›´æ˜æ˜¾çš„ç°è‰²
            color = 'rgba(100, 100, 100, 0.95)'  # æ›´æ·±çš„ç°è‰²ï¼Œæ›´ä¸é€æ˜
        else:
            # æœ‰é”¤é€Ÿæ•°æ®ï¼Œä½¿ç”¨åŸºäºé”¤é€Ÿçš„é¢œè‰²æ˜ å°„ï¼ˆå¢åŠ ä¸é€æ˜åº¦ï¼‰
            base_color = self._calculate_velocity_color(velocity, vmin, vmax)
            # ç¡®ä¿é¢œè‰²è¶³å¤Ÿæ˜æ˜¾ï¼ˆå¦‚æœalphaå¤ªä½ï¼Œæé«˜å®ƒï¼‰
            if 'rgba' in base_color:
                # è§£ærgbaå¹¶ç¡®ä¿alphaè‡³å°‘ä¸º0.8
                import re
                match = re.search(r'rgba\((\d+),\s*(\d+),\s*(\d+),\s*([\d.]+)\)', base_color)
                if match:
                    r, g, b, a = map(float, match.groups())
                    a = max(a, 0.85)  # ç¡®ä¿alphaè‡³å°‘ä¸º0.85
                    color = f'rgba({int(r)},{int(g)},{int(b)},{a:.2f})'
                else:
                    color = base_color
            else:
                color = base_color

        # åˆ›å»ºtraceåç§°
        data_type = bar.get('data_type', '')
        trace_name = self._create_bar_trace_name(algorithm_name, data_type, bar['label'])

        # æ·»åŠ æ°´å¹³çº¿æ®µ
        fig.add_trace(go.Scattergl(
            x=[bar['t_on']/10, bar['t_off']/10],
            y=[bar['key_id'], bar['key_id']],
            mode='lines',
            line=dict(color=color, width=3),
            name=trace_name,
            showlegend=False,
            legendgroup=algorithm_name,
            hoverinfo='text' if bar.get('text') else 'skip',
            text=bar.get('text', ''),
            customdata=[[
                bar['t_on']/10,
                bar['t_off']/10,
                int(bar.get('original_key_id', bar.get('key_id', 0))),
                bar.get('velocity', 'N/A'),
                bar.get('label', 'unknown'),
                bar.get('source_index', 0),
                algorithm_name,
                bar.get('record_uuid', ''),
                bar.get('replay_uuid', '')
            ]]
        ))
        return True

    def _handle_generation_error(self, error: Exception, plot_type: str, include_traceback: bool = True,
                                return_dict: bool = False, return_list: bool = False) -> Any:
        """é€šç”¨å›¾è¡¨ç”Ÿæˆé”™è¯¯å¤„ç†æ–¹æ³•

        Args:
            error: æ•è·çš„å¼‚å¸¸
            plot_type: å›¾è¡¨ç±»å‹æè¿°ï¼ˆç”¨äºé”™è¯¯æ¶ˆæ¯ï¼‰
            include_traceback: æ˜¯å¦åŒ…å«å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„é”™è¯¯ç»“æœï¼ˆç”¨äºè¿”å›å¤šä¸ªå›¾è¡¨çš„æ–¹æ³•ï¼‰
            return_list: æ˜¯å¦è¿”å›åˆ—è¡¨æ ¼å¼çš„é”™è¯¯ç»“æœï¼ˆç”¨äºè¿”å›å›¾è¡¨åˆ—è¡¨çš„æ–¹æ³•ï¼‰

        Returns:
            Any: ç©ºçš„å›¾è¡¨å¯¹è±¡ã€é”™è¯¯å­—å…¸æˆ–é”™è¯¯åˆ—è¡¨
        """
        logger.error(f"âŒ ç”Ÿæˆ{plot_type}å¤±è´¥: {error}")
        if include_traceback:
            logger.error(traceback.format_exc())

        empty_plot = self._create_empty_plot(f"ç”Ÿæˆå¤±è´¥: {str(error)}")

        if return_dict:
            return {
                'raw_delay_plot': empty_plot,
                'relative_delay_plot': empty_plot
            }
        elif return_list:
            return [{'title': 'ç”Ÿæˆå¤±è´¥', 'figure': empty_plot}]
        else:
            return empty_plot

    def _get_multi_file_average_delay(self, backend, algorithm_names: List[str]) -> float:
        """è·å–å¤šæ–‡ä»¶æ¨¡å¼çš„å¹³å‡å»¶æ—¶

        Args:
            backend: åç«¯å®ä¾‹
            algorithm_names: ç®—æ³•åç§°åˆ—è¡¨

        Returns:
            float: å¹³å‡å»¶æ—¶(ms)
        """
        if not algorithm_names or algorithm_names[0] == 'single':
            return 0.0

        try:
            # è·å–æ´»è·ƒç®—æ³•åˆ—è¡¨
            active_algorithms = backend.get_active_algorithms()
            
            if not active_algorithms:
                return 0.0

            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç®—æ³•
            target_algorithm_name = algorithm_names[0]
            target_algorithm = None
            for alg in active_algorithms:
                if (hasattr(alg, 'metadata') and
                    hasattr(alg.metadata, 'algorithm_name') and
                    alg.metadata.algorithm_name == target_algorithm_name):
                    target_algorithm = alg
                    break

            # è·å–è¯¥ç®—æ³•çš„å¹³å‡å»¶æ—¶
            if (target_algorithm and
                target_algorithm.analyzer and
                hasattr(target_algorithm.analyzer, 'get_global_average_delay')):

                avg_delay_0_1ms = target_algorithm.analyzer.get_global_average_delay()
                return avg_delay_0_1ms / 10.0

        except Exception as e:
            logger.warning(f"è·å–å¤šæ–‡ä»¶å¹³å‡å»¶æ—¶å¤±è´¥: {e}")

        return 0.0

    def _get_single_file_average_delay(self, backend) -> float:
        """è·å–å•æ–‡ä»¶æ¨¡å¼çš„å¹³å‡å»¶æ—¶

        Args:
            backend: åç«¯å®ä¾‹

        Returns:
            float: å¹³å‡å»¶æ—¶(ms)
        """
        try:
            avg_delay_0_1ms = backend.get_global_average_delay()
            return avg_delay_0_1ms / 10.0
        except Exception as e:
            logger.warning(f"è·å–å•æ–‡ä»¶å¹³å‡å»¶æ—¶å¤±è´¥: {e}")
            return 0.0

    def _get_average_delay(self, backend, is_multi_file: bool, algorithm_names: List[str]) -> float:
        """ç»Ÿä¸€è·å–å¹³å‡å»¶æ—¶ï¼ˆè‡ªåŠ¨é€‰æ‹©å•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶æ¨¡å¼ï¼‰

        Args:
            backend: åç«¯å®ä¾‹
            is_multi_file: æ˜¯å¦ä¸ºå¤šæ–‡ä»¶æ¨¡å¼
            algorithm_names: ç®—æ³•åç§°åˆ—è¡¨

        Returns:
            float: å¹³å‡å»¶æ—¶(ms)
        """
        if is_multi_file:
            return self._get_multi_file_average_delay(backend, algorithm_names)
        else:
            return self._get_single_file_average_delay(backend)
    
    def _parse_match_index(self, match_index) -> int:
        """è§£æmatch_indexä¸ºsource_index"""
        source_index = 0
        try:
            if isinstance(match_index, str) and match_index != "N/A":
                source_index = int(match_index)
            elif isinstance(match_index, int):
                source_index = match_index
        except (ValueError, TypeError):
            source_index = 0
        return source_index
    
    def _create_bar_dict(self, key_on_time: float, key_off_time: float, actual_key_id: float,
                        key_id: int, hammer_velocity,
                        color_intensity: float, algorithm_name: str, label: str,
                        data_type: str, grade_name: str, match_index, source_index: int,
                        delay_ms: float, relative_delay_ms: float) -> Dict:
        """åˆ›å»ºbarå­—å…¸"""
        return {
            't_on': float(key_on_time),
            't_off': float(key_off_time),
            'key_id': actual_key_id,
            'original_key_id': key_id,
            'velocity': hammer_velocity,
            'color_intensity': color_intensity,
            'algorithm_name': algorithm_name,
            'label': label,
            'data_type': data_type,
            'hammer_index': 0,
            'grade_name': grade_name,
            'match_index': match_index,
            'source_index': source_index,
            'delay_ms': delay_ms,
            'relative_delay_ms': relative_delay_ms,
            'first_hammer_time': key_on_time
        }
    
    def _generate_hover_text(self, bar: Dict, label: str, data_type: str,
                            algorithm_name: str, key_id: int, hammer_velocity,
                            grade_name: str, match_index, delay_ms: float,
                            relative_delay_ms: float, key_on_time: float,
                            key_off_time: float) -> str:
        """ç”Ÿæˆhoveræ–‡æœ¬"""
        # ç”Ÿæˆæ•°æ®ç±»å‹åç¼€
        bar_type_suffix = ""
        if data_type == "drop_hammer":
            bar_type_suffix = " (ä¸¢é”¤)"
        elif data_type == "multi_hammer":
            bar_type_suffix = " (å¤šé”¤)"
        
        # æ ¹æ®labelæ„å»ºä¸åŒçš„hoveræ–‡æœ¬
        if label == 'record':
            return (
                f'ç®—æ³•: {algorithm_name}<br>'
                f'ç±»å‹: {label}{bar_type_suffix}<br>'
                f'é”®ä½: {key_id}<br>'
                f'é”¤é€Ÿ: {hammer_velocity}<br>'
                f'ç­‰çº§: {grade_name}<br>'
                f'ç´¢å¼•: {match_index}<br>'
                f'æŒ‰é”®æŒ‰ä¸‹: {key_on_time/10:.2f}ms<br>'
                f'æŒ‰é”®é‡Šæ”¾: {key_off_time/10:.2f}ms<br>'
            )
        else:
            return (
                f'ç®—æ³•: {algorithm_name}<br>'
                f'ç±»å‹: {label}{bar_type_suffix}<br>'
                f'é”®ä½: {key_id}<br>'
                f'é”¤é€Ÿ: {hammer_velocity}<br>'
                f'ç­‰çº§: {grade_name}<br>'
                f'ç´¢å¼•: {match_index}<br>'
                f'ç»å¯¹å»¶æ—¶: {delay_ms:.2f}ms<br>'
                f'ç›¸å¯¹å»¶æ—¶: {relative_delay_ms:+.2f}ms<br>'
                f'æŒ‰é”®æŒ‰ä¸‹: {key_on_time/10:.2f}ms<br>'
                f'æŒ‰é”®é‡Šæ”¾: {key_off_time/10:.2f}ms<br>'
            )
    
    def _apply_key_filter(self, data: List, key_filter: set) -> List:
        """åº”ç”¨æŒ‰é”®è¿‡æ»¤"""
        if not key_filter:
            return data
        return [note for note in data if note.keyId in key_filter]
    
    
    def generate_multi_algorithm_offset_alignment_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå¤šç®—æ³•åç§»å¯¹é½åˆ†æå›¾ï¼ˆå¹¶æ’æŸ±çŠ¶å›¾ï¼Œä¸åŒé¢œè‰²ï¼‰
        
        è¿”å›5ä¸ªç‹¬ç«‹çš„å›¾è¡¨ï¼Œæ¯ä¸ªå›¾è¡¨æ˜¾ç¤ºä¸€ä¸ªæŒ‡æ ‡ï¼š
        - ä¸­ä½æ•°åç§»ã€å‡å€¼åç§»ã€æ ‡å‡†å·®ã€æ–¹å·®ã€ç›¸å¯¹å»¶æ—¶
        
        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: åŒ…å«å›¾è¡¨ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        # éªŒè¯è¾“å…¥
        if not algorithms:
            return self._create_empty_offset_figures("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
        
        try:
            # è¿‡æ»¤å°±ç»ªçš„ç®—æ³•
            ready_algorithms = [alg for alg in algorithms if alg.is_ready()]
            if not ready_algorithms:
                logger.warning("âš ï¸ æ²¡æœ‰å°±ç»ªçš„ç®—æ³•ï¼Œæ— æ³•ç”Ÿæˆå¤šç®—æ³•åç§»å¯¹é½åˆ†æå›¾")
                return self._create_empty_offset_figures("æ²¡æœ‰å°±ç»ªçš„ç®—æ³•")
            
            # æ”¶é›†æ‰€æœ‰ç®—æ³•çš„æ•°æ®
            all_algorithms_data = self._collect_all_algorithms_offset_data(ready_algorithms)
            
            if not all_algorithms_data:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åç§»å¯¹é½æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
                return self._create_empty_offset_figures("æ²¡æœ‰æœ‰æ•ˆçš„åç§»å¯¹é½æ•°æ®")
            
            # ç”Ÿæˆå›¾è¡¨
            figures_list = self._generate_offset_metric_figures(all_algorithms_data)
            
            return figures_list
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•åç§»å¯¹é½åˆ†æå›¾", return_list=True)
    
    def _create_empty_offset_figures(self, message: str) -> List[Dict[str, Any]]:
        """åˆ›å»ºç©ºçš„åç§»å¯¹é½å›¾è¡¨åˆ—è¡¨"""
        empty_fig = self._create_empty_plot(message)
        return [
            {'title': 'æ ·æœ¬æ€»æ•°', 'figure': empty_fig},
            {'title': 'å¹³å‡å»¶æ—¶', 'figure': empty_fig},
            {'title': 'æ—¶å»¶æ–¹å·®', 'figure': empty_fig},
            {'title': 'ç›¸å¯¹å»¶æ—¶', 'figure': empty_fig}
        ]
    
    def _collect_all_algorithms_offset_data(self, ready_algorithms: List[AlgorithmDataset]) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰ç®—æ³•çš„åç§»å¯¹é½æ•°æ®"""
        all_algorithms_data = []
        colors = ALGORITHM_COLOR_PALETTE
        
        for alg_idx, algorithm in enumerate(ready_algorithms):
            algorithm_data = self._collect_single_algorithm_offset_data(
                algorithm, alg_idx, colors
            )
            if algorithm_data:
                all_algorithms_data.append(algorithm_data)
        
        return all_algorithms_data
    
    def _collect_single_algorithm_offset_data(self, algorithm: AlgorithmDataset, 
                                              alg_idx: int, colors: List[str]) -> Optional[Dict]:
        """æ”¶é›†å•ä¸ªç®—æ³•çš„åç§»å¯¹é½æ•°æ®"""
        algorithm_name = algorithm.metadata.algorithm_name
        
        if not algorithm.analyzer:
            logger.warning(f"âš ï¸ ç®—æ³• '{algorithm_name}' æ²¡æœ‰åˆ†æå™¨ï¼Œè·³è¿‡")
            return None
        
        try:
            # ç›´æ¥è·å–æŒ‰ key_id åˆ†ç»„çš„ç²¾ç¡®åŒ¹é…å»¶æ—¶æ•°æ® (ms)
            grouped_offsets = algorithm.analyzer.get_grouped_precision_match_data()
            
            # è·å–ç®—æ³•æ•´ä½“å¹³å‡å»¶æ—¶ (ms)
            algorithm_mean_delay = self._calculate_mean_delay(algorithm.analyzer)
            
            # è®¡ç®—å„é¡¹ç»Ÿè®¡æŒ‡æ ‡
            statistics = self._calculate_offset_statistics(grouped_offsets, algorithm_mean_delay)
            
            if not statistics['key_ids']:
                return None
            
            return {
                'name': algorithm_name,
                'display_name': algorithm.metadata.display_name,
                'color': colors[alg_idx % len(colors)],
                'analyzer': algorithm.analyzer,
                **statistics
            }
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ç®—æ³• '{algorithm_name}' çš„åç§»å¯¹é½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _calculate_mean_delay(self, analyzer) -> float:
        """è®¡ç®—ç®—æ³•çš„å¹³å‡å»¶æ—¶"""
        me_0_1ms = analyzer.get_mean_error() if hasattr(analyzer, 'get_mean_error') else 0.0
        return me_0_1ms / 10.0  # è½¬æ¢ä¸ºms
    
    def _calculate_offset_statistics(self, grouped_offsets: Dict[int, List[float]], 
                                     algorithm_mean_delay: float) -> Dict:
        """
        è®¡ç®—æ¯ä¸ªæŒ‰é”®ç»„çš„ç»Ÿè®¡æŒ‡æ ‡
        
        Args:
            grouped_offsets: key_id -> [offset_ms, ...]
            algorithm_mean_delay: ç®—æ³•æ•´ä½“å¹³å‡å»¶æ—¶ (ms)
        """
        key_ids = []
        count = []
        mean = []
        variance = []
        relative_mean = []
        
        for key_id, offsets in grouped_offsets.items():
            if not offsets:
                continue
            
            key_ids.append(int(key_id))
            count.append(len(offsets))
            
            # å¹³å‡å»¶æ—¶ï¼šè¯¥é”®ä½æ‰€æœ‰ç²¾ç¡®åŒ¹é…å¯¹çš„ç»å¯¹å»¶æ—¶å‡å€¼
            mean.append(np.mean([abs(o) for o in offsets]))
            
            # æ—¶å»¶æ–¹å·®ï¼šè¯¥é”®ä½å»¶æ—¶çš„æ–¹å·®
            variance.append(np.var(offsets))
            
            # ç›¸å¯¹å»¶æ—¶ï¼šè¯¥é”®ä½å¹³å‡å»¶æ—¶ä¸ç®—æ³•æ•´ä½“å¹³å‡å»¶æ—¶çš„å·®å¼‚
            # ç®—æ³•ï¼šmean(offset_i) - algorithm_mean
            relative_mean.append(np.mean(offsets) - algorithm_mean_delay)
        
        return {
            'key_ids': key_ids,
            'count': count,
            'mean': mean,
            'variance': variance,
            'relative_mean': relative_mean
        }
        
        return {
            'key_ids': key_ids,
            'count': count,
            'mean': mean,
            'variance': variance,
            'relative_mean': relative_mean
        }
    
    def _generate_offset_metric_figures(self, all_algorithms_data: List[Dict]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰æŒ‡æ ‡çš„å›¾è¡¨"""
        # å®šä¹‰æŒ‡æ ‡é…ç½®
        metrics = [
            ('æ ·æœ¬æ€»æ•°', 'count', 'ä¸ª'),
            ('å¹³å‡å»¶æ—¶', 'mean', 'ms'),
            ('æ—¶å»¶æ–¹å·®', 'variance', 'msÂ²'),
            ('ç›¸å¯¹å»¶æ—¶', 'relative_mean', 'ms')
        ]
        
        # è®¡ç®—å…¨å±€å‚æ•°
        all_key_ids = self._get_all_key_ids(all_algorithms_data)
        num_algorithms = len(all_algorithms_data)
        bar_width = 0.8 / num_algorithms
        min_key_id = max(1, min(all_key_ids)) if all_key_ids else 1
        max_key_id = max(all_key_ids) if all_key_ids else 90
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆå›¾è¡¨
        figures_list = []
        for metric_name, data_key, unit in metrics:
            fig = self._create_single_offset_metric_figure(
                metric_name, data_key, unit, all_algorithms_data,
                all_key_ids, num_algorithms, bar_width, min_key_id, max_key_id
            )
            figures_list.append({'title': metric_name, 'figure': fig})
        
        return figures_list
    
    def _get_all_key_ids(self, all_algorithms_data: List[Dict]) -> List[int]:
        """è·å–æ‰€æœ‰é”®ä½IDçš„å¹¶é›†"""
        all_key_ids = set()
        for alg_data in all_algorithms_data:
            all_key_ids.update(alg_data['key_ids'])
        return sorted(list(all_key_ids))
    
    def _create_single_offset_metric_figure(self, metric_name: str, data_key: str, 
                                            unit: str, all_algorithms_data: List[Dict],
                                            all_key_ids: List[int], num_algorithms: int,
                                            bar_width: float, min_key_id: int, 
                                            max_key_id: int) -> go.Figure:
        """åˆ›å»ºå•ä¸ªæŒ‡æ ‡çš„å›¾è¡¨"""
        fig = go.Figure()
        
        # ä¸ºæ¯ä¸ªç®—æ³•æ·»åŠ trace
        for alg_idx, alg_data in enumerate(all_algorithms_data):
            self._add_algorithm_trace_to_figure(
                fig, alg_data, alg_idx, data_key, metric_name, unit,
                all_key_ids, num_algorithms, bar_width
            )
        
        # é…ç½®å¸ƒå±€
        self._configure_offset_figure_layout(
            fig, metric_name, unit, min_key_id, max_key_id
        )
        
        return fig
    
    def _add_algorithm_trace_to_figure(self, fig: go.Figure, alg_data: Dict, 
                                       alg_idx: int, data_key: str, metric_name: str,
                                       unit: str, all_key_ids: List[int], 
                                       num_algorithms: int, bar_width: float):
        """æ·»åŠ ç®—æ³•çš„traceåˆ°å›¾è¡¨"""
        algorithm_name = alg_data['name']
        display_name = alg_data.get('display_name', algorithm_name)
        color = alg_data['color']
        
        # å‡†å¤‡æ•°æ®
        x_positions = []
        y_values = []
        key_to_val = dict(zip(alg_data['key_ids'], alg_data[data_key]))
        
        for key_id in all_key_ids:
            if key_id in alg_data['key_ids']:
                x_pos = key_id + (alg_idx - num_algorithms / 2 + 0.5) * bar_width
                x_positions.append(x_pos)
                y_values.append(key_to_val[key_id])
        
        if not x_positions:
            return
        
        # æ·»åŠ æŸ±çŠ¶å›¾trace
        fig.add_trace(go.Bar(
            x=x_positions,
            y=y_values,
            name=display_name,
            marker_color=color,
            opacity=0.8,
            width=bar_width,
            text=[f'{val:.2f}' if data_key != 'count' else f'{int(val)}' for val in y_values],
            textposition='outside',
            textfont=dict(size=8),
            showlegend=True,
            legendgroup=algorithm_name,
            hovertemplate=f'ç®—æ³•: {display_name}<br>é”®ä½: %{{x:.0f}}<br>{metric_name}: %{{y:.2f}}{unit}<extra></extra>'
        ))
    
    def _configure_offset_figure_layout(self, fig: go.Figure, metric_name: str, 
                                        unit: str, min_key_id: int, max_key_id: int):
        """é…ç½®åç§»å›¾è¡¨çš„å¸ƒå±€"""
        fig.update_layout(
            title=dict(text=metric_name, x=0.5, xanchor='center'),
            xaxis_title='é”®ä½ID',
            yaxis_title=f'{metric_name} ({unit})',
            xaxis=dict(
                tickmode='linear',
                tick0=min_key_id,
                dtick=1,
                range=[min_key_id - 1, max_key_id + 1]
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            template='simple_white',
            showlegend=True,
            legend=dict(
                x=0.01, y=1.12, xanchor='left', yanchor='top',
                bgcolor='rgba(255,255,255,0.8)',
                bordercolor='rgba(0,0,0,0.2)',
                borderwidth=1,
                orientation='h',
                font=dict(size=11),
                title_text=metric_name
            ),
            margin=dict(l=60, r=40, t=100, b=60),
            height=500,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(size=12)
        )

    def generate_relative_delay_distribution_plot(self, algorithms: List[AlgorithmDataset]) -> Any:
        """
        ç”Ÿæˆç›¸å¯¹å»¶æ—¶åˆ†å¸ƒå›¾ï¼ˆç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆï¼‰
        
        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        # å¤ç”¨ generate_multi_algorithm_delay_histogram_plot çš„é€»è¾‘
        # å› ä¸ºè¯¥æ–¹æ³•å†…éƒ¨å·²ç»å®ç°äº†ç›¸å¯¹å»¶æ—¶è®¡ç®—å’Œç›´æ–¹å›¾ç»˜åˆ¶
        return self.generate_multi_algorithm_delay_histogram_plot(algorithms)

    def generate_multi_algorithm_delay_histogram_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•å»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå åŠ æ˜¾ç¤ºï¼Œä¸åŒé¢œè‰²ï¼Œå›¾ä¾‹æ§åˆ¶ï¼‰

        ä¸ºæ¯ä¸ªç®—æ³•ç”Ÿæˆç›´æ–¹å›¾å’Œæ­£æ€æ‹Ÿåˆæ›²çº¿ï¼Œä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†ï¼Œå åŠ æ˜¾ç¤ºåœ¨åŒä¸€å›¾è¡¨ä¸­ã€‚
        æ•°æ®ç­›é€‰ï¼šåªä½¿ç”¨è¯¯å·®â‰¤50msçš„æŒ‰é”®æ•°æ®

        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨

        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        # éªŒè¯å¹¶å‡†å¤‡ç®—æ³•
        ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
        if not ready_algorithms:
            return self._create_empty_plot("æ²¡æœ‰å°±ç»ªçš„ç®—æ³•")
        
        try:
            fig = go.Figure()
            all_delays = []
            
            # ä¸ºæ¯ä¸ªç®—æ³•æ”¶é›†æ•°æ®å¹¶æ·»åŠ traces
            for alg_idx, algorithm in enumerate(ready_algorithms):
                delay_data = self._collect_algorithm_delay_data(algorithm, alg_idx)
                if not delay_data:
                    continue
                
                all_delays.extend(delay_data['relative_delays'])
                
                # æ·»åŠ ç›´æ–¹å›¾å’Œæ­£æ€æ‹Ÿåˆæ›²çº¿
                self._add_histogram_traces(fig, delay_data)
            
            if not all_delays:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å»¶æ—¶æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç›´æ–¹å›¾")
                return self._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„å»¶æ—¶æ•°æ®")
            
            # é…ç½®å¸ƒå±€
            self._configure_histogram_layout(fig)
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•å»¶æ—¶åˆ†å¸ƒç›´æ–¹å›¾")
    
    def _collect_algorithm_delay_data(self, algorithm: AlgorithmDataset, alg_idx: int) -> Optional[Dict]:
        """æ”¶é›†å•ä¸ªç®—æ³•çš„å»¶æ—¶æ•°æ®"""
        # æå–å…ƒæ•°æ®
        metadata = self._extract_algorithm_metadata(algorithm)
        descriptive_name = metadata['descriptive_name']
        
        # éªŒè¯åˆ†æå™¨
        if not self._validate_analyzer(algorithm, descriptive_name):
            return None
        
        try:
            # è·å–åç§»æ•°æ®
            offset_data = self._get_offset_data(algorithm)
            if not offset_data:
                logger.warning(f"âš ï¸ ç®—æ³• '{metadata['algorithm_name']}' æ²¡æœ‰ç²¾ç¡®åŒ¹é…æ•°æ®ï¼ˆâ‰¤50msï¼‰ï¼Œè·³è¿‡")
                return None
            
            # æå–ç»å¯¹å»¶æ—¶
            absolute_delays_ms = [item.get('keyon_offset', 0.0) / 10.0 for item in offset_data]
            if not absolute_delays_ms:
                logger.warning(f"âš ï¸ ç®—æ³• '{metadata['algorithm_name']}' ç­›é€‰åæ²¡æœ‰æœ‰æ•ˆå»¶æ—¶æ•°æ®ï¼Œè·³è¿‡")
                return None
            
            # è®¡ç®—å¹³å‡å»¶æ—¶
            mean_delay_ms = self._calculate_mean_delay(algorithm.analyzer)
            
            # è®¡ç®—ç›¸å¯¹å»¶æ—¶å’Œç»Ÿè®¡é‡
            relative_delays_ms = [delay - mean_delay_ms for delay in absolute_delays_ms]
            statistics = self._calculate_delay_statistics(
                absolute_delays_ms, relative_delays_ms, mean_delay_ms
            )
            
            return {
                'descriptive_name': descriptive_name,
                'relative_delays': relative_delays_ms,
                'statistics': statistics,
                'color': self._get_algorithm_color(alg_idx)
            }
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ç®—æ³• '{metadata['algorithm_name']}' çš„å»¶æ—¶æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _calculate_delay_statistics(self, absolute_delays: List[float], 
                                    relative_delays: List[float], 
                                    mean_delay: float) -> Dict[str, float]:
        """è®¡ç®—å»¶æ—¶ç»Ÿè®¡é‡"""
        n = len(absolute_delays)
        
        # ç»å¯¹å»¶æ—¶ç»Ÿè®¡ï¼ˆåæ˜ æ•´ä½“ç¨³å®šæ€§ï¼‰
        if n > 1:
            var_offset = sum((x - mean_delay) ** 2 for x in absolute_delays) / (n - 1)
            std_offset = var_offset ** 0.5
        else:
            std_offset = 0.0
        
        # ç›¸å¯¹å»¶æ—¶ç»Ÿè®¡ï¼ˆç”¨äºæ­£æ€æ‹Ÿåˆï¼‰
        if n > 1:
            var_relative = sum(x ** 2 for x in relative_delays) / (n - 1)
            std_relative = var_relative ** 0.5
        else:
            std_relative = 0.0
        
        return {
            'mean_offset': mean_delay,
            'std_offset': std_offset,
            'std_relative': std_relative
        }
    
    def _add_histogram_traces(self, fig: go.Figure, delay_data: Dict):
        """æ·»åŠ ç›´æ–¹å›¾å’Œæ­£æ€æ‹Ÿåˆæ›²çº¿traces"""
        descriptive_name = delay_data['descriptive_name']
        relative_delays = delay_data['relative_delays']
        stats = delay_data['statistics']
        color = delay_data['color']
        
        # æ·»åŠ ç›´æ–¹å›¾
        fig.add_trace(go.Histogram(
            x=relative_delays,
            histnorm='probability density',
            name=f'{descriptive_name} - å»¶æ—¶åˆ†å¸ƒ',
            marker_color=color,
            opacity=0.85,
            marker_line_color=color,
            marker_line_width=0.5,
            legendgroup=descriptive_name,
            showlegend=True
        ))
        
        # æ·»åŠ æ­£æ€æ‹Ÿåˆæ›²çº¿
        if stats['std_relative'] > 0:
            xs, ys = self._generate_normal_curve(
                relative_delays, stats['std_relative']
            )
            fig.add_trace(go.Scatter(
                x=xs,
                y=ys,
                mode='lines',
                name=f'{descriptive_name} - æ­£æ€æ‹Ÿåˆ (Î¼={stats["mean_offset"]:.2f}ms, Ïƒ={stats["std_offset"]:.2f}ms)',
                line=dict(color=color, width=2),
                legendgroup=descriptive_name,
                showlegend=True
            ))
    
    def _generate_normal_curve(self, relative_delays: List[float], 
                               std_relative: float) -> Tuple[List[float], List[float]]:
        """ç”Ÿæˆæ­£æ€åˆ†å¸ƒæ‹Ÿåˆæ›²çº¿çš„åæ ‡"""
        min_x = min(relative_delays)
        max_x = max(relative_delays)
        span = max(1e-6, 3 * std_relative)
        x_start = min(-span, min_x)
        x_end = max(span, max_x)
        
        num_pts = 200
        step = (x_end - x_start) / (num_pts - 1) if num_pts > 1 else 1.0
        xs = [x_start + i * step for i in range(num_pts)]
        ys = [(1.0 / (std_relative * (2 * math.pi) ** 0.5)) *
              math.exp(-0.5 * (x / std_relative) ** 2) for x in xs]
        
        return xs, ys
    
    def _configure_histogram_layout(self, fig: go.Figure):
        """é…ç½®ç›´æ–¹å›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
            yaxis_title='æ¦‚ç‡å¯†åº¦',
            bargap=0.05,
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=500,
            clickmode='event+select',
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.05,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=100, b=60, l=60, r=60)
        )
    
    def generate_multi_algorithm_key_delay_scatter_plot(
        self,
        algorithms: List[AlgorithmDataset],
        only_common_keys: bool = False,
        selected_algorithm_names: List[str] = None
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•æŒ‰é”®ä¸å»¶æ—¶æ•£ç‚¹å›¾ï¼ˆå åŠ æ˜¾ç¤ºï¼Œä¸åŒé¢œè‰²ï¼Œå›¾ä¾‹æ§åˆ¶ï¼‰

        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            only_common_keys: æ˜¯å¦åªæ˜¾ç¤ºå…¬å…±æŒ‰é”®
            selected_algorithm_names: æŒ‡å®šå‚ä¸å¯¹æ¯”çš„ç®—æ³•åç§°åˆ—è¡¨

        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            # ç­›é€‰å’Œå‡†å¤‡ç®—æ³•
            filtered_algorithms = self._filter_algorithms_by_names(
                algorithms, selected_algorithm_names
            )
            ready_algorithms = self._validate_and_prepare_algorithms(filtered_algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            logger.info(f"å¼€å§‹ç”Ÿæˆå¤šç®—æ³•æŒ‰é”®ä¸å»¶æ—¶æ•£ç‚¹å›¾ï¼Œå…± {len(ready_algorithms)} ä¸ªæ¿€æ´»ç®—æ³•")
            
            # è®¡ç®—å…¬å…±æŒ‰é”®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            common_keys = self._calculate_common_keys(ready_algorithms) if only_common_keys else None
            
            fig = go.Figure()
            
            # æ”¶é›†æ‰€æœ‰ç®—æ³•æ•°æ®
            algorithm_data_list = []
            for alg_idx, algorithm in enumerate(ready_algorithms):
                alg_data = self._collect_scatter_algorithm_data(algorithm, alg_idx, common_keys)
                if alg_data:
                    algorithm_data_list.append(alg_data)
            
            if not algorithm_data_list:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ç®—æ³•æ•°æ®")
                return self._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„ç®—æ³•æ•°æ®")
            
            # æ·»åŠ æ•£ç‚¹å›¾å’Œé˜ˆå€¼çº¿
            self._add_scatter_plot_traces(fig, algorithm_data_list)
            self._add_scatter_threshold_lines(fig, algorithm_data_list)
            
            # é…ç½®å¸ƒå±€
            self._configure_scatter_plot_layout(fig)
            
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•æŒ‰é”®ä¸å»¶æ—¶æ•£ç‚¹å›¾")
    
    def generate_multi_algorithm_key_delay_zscore_scatter_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•æŒ‰é”®ä¸å»¶æ—¶Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾
        
        Z-Scoreæ ‡å‡†åŒ–å…¬å¼ï¼šz = (x_i - Î¼) / Ïƒ
        - x_i: æ¯ä¸ªæ•°æ®ç‚¹çš„å»¶æ—¶å€¼
        - Î¼: è¯¥ç®—æ³•çš„æ€»ä½“å‡å€¼
        - Ïƒ: è¯¥ç®—æ³•çš„æ€»ä½“æ ‡å‡†å·®
        
        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        if not algorithms:
            return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
        
        try:
            # éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
            ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            fig = go.Figure()
            all_key_ids = set()
            
            # æ”¶é›†æ‰€æœ‰æ¿€æ´»ç®—æ³•çš„æ•°æ®
            for alg_idx, algorithm in enumerate(ready_algorithms):
                metadata = self._extract_algorithm_metadata(algorithm)
                descriptive_name = metadata['descriptive_name']
                
                if not self._validate_analyzer(algorithm, descriptive_name):
                    continue
                
                try:
                    # è·å–åŒ¹é…å¯¹å’ŒéŸ³ç¬¦å­—å…¸
                    matched_pairs = self._get_matched_pairs(algorithm)
                    record_note_dict, replay_note_dict = self._create_note_dicts(matched_pairs)

                    # è·å–åç§»æ•°æ®
                    offset_data = self._get_offset_data(algorithm)

                    # æå–æ•£ç‚¹å›¾æ•°æ®ï¼ˆZ-Scoreä¸éœ€è¦è¿‡æ»¤å…¬å…±æŒ‰é”®ï¼‰
                    scatter_data = self._extract_scatter_delay_data(
                        offset_data, record_note_dict, replay_note_dict,
                        None, metadata['algorithm_name']  # common_keysä¸ºNoneï¼Œä¸è¿‡æ»¤
                    )
                    if not scatter_data or not scatter_data['key_ids']:
                        continue
                    
                    key_ids = scatter_data['key_ids']
                    delays_ms = scatter_data['delays_ms']
                    customdata_list = scatter_data['customdata']
                    
                    # è®¡ç®—Z-Scoreå€¼
                    z_scores, mu, sigma = self._calculate_zscore_values(delays_ms, algorithm)

                    # æ’åºæ•°æ® - å…ˆç»„ç»‡æˆå­—å…¸æ ¼å¼
                    scatter_dict = {
                        'key_ids': key_ids,
                        'delays_ms': z_scores,  # Z-Scoreå€¼ä½œä¸ºå»¶æ—¶
                        'relative_delays_ms': z_scores,  # å¯¹äºZ-Scoreï¼Œç›¸å¯¹å»¶æ—¶ä¹Ÿä½¿ç”¨Z-Scoreå€¼
                        'customdata': customdata_list
                    }
                    self._sort_scatter_data(scatter_dict)

                    # ä»æ’åºåçš„å­—å…¸æå–æ•°æ®
                    key_ids = scatter_dict['key_ids']
                    z_scores = scatter_dict['delays_ms']
                    customdata_list = scatter_dict['customdata']
                    
                    # æ·»åŠ æ•£ç‚¹å›¾traces
                    color = self._get_algorithm_color(alg_idx)
                    self._add_zscore_scatter_traces(
                        fig, key_ids, z_scores, customdata_list, descriptive_name, color
                    )
                    
                    # æ”¶é›†æŒ‰é”®ID
                    all_key_ids.update(key_ids)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–ç®—æ³• '{descriptive_name}' çš„Z-Scoreæ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ·»åŠ é˜ˆå€¼çº¿ï¼ˆZ=0, Â±3ï¼‰
            sorted_key_ids = sorted(all_key_ids)
            key_labels = [str(kid) for kid in sorted_key_ids]
            self._add_zscore_threshold_lines(fig, key_labels, ready_algorithms)
            
            # é…ç½®å¸ƒå±€
            self._configure_zscore_layout(fig)
            
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•Z-Scoreæ ‡å‡†åŒ–æ•£ç‚¹å›¾")

    def generate_single_key_delay_comparison_plot(
        self,
        algorithms: List[AlgorithmDataset],
        target_key_id: int
    ) -> Any:
        """
        ç”Ÿæˆå•é”®å¤šæ›²å»¶æ—¶å¯¹æ¯”å›¾ï¼ˆæ•£ç‚¹å›¾+ç®±çº¿å›¾ï¼‰
        
        Args:
            algorithms: ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            target_key_id: ç›®æ ‡æŒ‰é”®ID
            
        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        if not algorithms:
            return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
        if target_key_id is None:
            return self._create_empty_plot("è¯·é€‰æ‹©ä¸€ä¸ªæŒ‰é”®è¿›è¡Œåˆ†æ")
            
        try:
            # éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
            ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»ä¸”å°±ç»ªçš„ç®—æ³•")
            
            fig = go.Figure()
            has_data = False
            
            # éå†æ¯ä¸ªç®—æ³•
            for alg_idx, algorithm in enumerate(ready_algorithms):
                metadata = self._extract_algorithm_metadata(algorithm)
                display_name = metadata['display_name']
                filename = metadata['filename']
                descriptive_name = metadata['descriptive_name']
                
                # éªŒè¯åˆ†æå™¨
                if not self._validate_analyzer(algorithm, descriptive_name):
                    continue
                
                # è·å–åç§»æ•°æ®
                offset_data = self._get_offset_data(algorithm)
                if not offset_data:
                    continue
                
                # æå–ç›®æ ‡æŒ‰é”®çš„å»¶æ—¶æ•°æ®
                key_delays, customdata_list = self._extract_single_key_delays(
                    offset_data, target_key_id, filename
                )
                
                if not key_delays:
                    continue
                
                has_data = True
                
                # æ·»åŠ ç®±çº¿å›¾
                color = self._get_algorithm_color(alg_idx)
                self._add_box_trace(
                    fig, key_delays, display_name, target_key_id, customdata_list, color
                )
            
            if not has_data:
                return self._create_empty_plot(f"æŒ‰é”® {target_key_id} åœ¨é€‰å®šçš„ç®—æ³•ä¸­æ²¡æœ‰æ•°æ®")
            
            # é…ç½®å¸ƒå±€
            self._configure_single_key_layout(fig, target_key_id)
            
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å•é”®å¯¹æ¯”å›¾")

    def generate_multi_algorithm_hammer_velocity_relative_delay_scatter_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶æ•£ç‚¹å›¾ï¼ˆå åŠ æ˜¾ç¤ºï¼Œä¸åŒé¢œè‰²ï¼Œå›¾ä¾‹æ§åˆ¶ï¼‰

        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨

        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            # éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
            ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

            logger.info(f"å¼€å§‹ç”Ÿæˆå¤šç®—æ³•é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶æ•£ç‚¹å›¾ï¼Œå…± {len(ready_algorithms)} ä¸ªæ¿€æ´»ç®—æ³•")
            
            fig = go.Figure()
            
            # è®¡ç®—xè½´èŒƒå›´
            x_min, x_max = self._calculate_log_velocity_range(ready_algorithms)

            for alg_idx, algorithm in enumerate(ready_algorithms):
                metadata = self._extract_algorithm_metadata(algorithm)
                algorithm_name = metadata['algorithm_name']
                descriptive_name = metadata['descriptive_name']
                display_name = metadata['display_name']

                # éªŒè¯åˆ†æå™¨
                if not self._validate_analyzer(algorithm, descriptive_name):
                    continue

                try:
                    # è·å–åŒ¹é…å¯¹
                    matched_pairs = self._get_matched_pairs(algorithm)
                    if not matched_pairs:
                        logger.warning(f"ç®—æ³• '{descriptive_name}' æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # è·å–åç§»æ•°æ®å¹¶åˆ›å»ºæ˜ å°„
                    offset_data = self._get_offset_data(algorithm)
                    offset_map = self._create_offset_map(offset_data)

                    # æå–é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®ï¼ˆä½¿ç”¨algorithm_nameä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                    hammer_velocities, delays_ms, scatter_customdata = \
                        self._extract_hammer_velocity_delay_data(matched_pairs, offset_map, algorithm_name)

                    if not hammer_velocities:
                        logger.warning(f"ç®—æ³• '{descriptive_name}' æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # è®¡ç®—ç›¸å¯¹å»¶æ—¶ç»Ÿè®¡
                    relative_delays, statistics = \
                        self._calculate_relative_delay_statistics(delays_ms, algorithm)

                    # å°†é”¤é€Ÿè½¬æ¢ä¸ºå¯¹æ•°å½¢å¼
                    log_velocities = [math.log10(v) for v in hammer_velocities]

                    # è·å–ç®—æ³•é¢œè‰²
                    color = self._get_algorithm_color(alg_idx)

                    # æ·»åŠ æ•£ç‚¹å›¾trace
                    self._add_hammer_velocity_scatter_trace(
                        fig, log_velocities, relative_delays, delays_ms,
                        hammer_velocities, scatter_customdata, descriptive_name, color
                    )

                    # æ·»åŠ é˜ˆå€¼çº¿
                    if len(log_velocities) > 0:
                        self._add_relative_delay_threshold_lines(
                            fig, x_min, x_max, statistics, descriptive_name, color
                        )

                except Exception as e:
                    logger.warning(f"è·å–ç®—æ³• '{descriptive_name}' çš„é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶æ•°æ®å¤±è´¥: {e}")
                    continue

            # é…ç½®å¸ƒå±€
            self._configure_hammer_velocity_layout(fig)

            logger.info(f"å¤šç®—æ³•é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶æ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")
            return fig

        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶æ•£ç‚¹å›¾")

    def generate_multi_algorithm_hammer_velocity_delay_scatter_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•é”¤é€Ÿä¸å»¶æ—¶æ•£ç‚¹å›¾ï¼ˆå åŠ æ˜¾ç¤ºï¼Œä¸åŒé¢œè‰²ï¼Œå›¾ä¾‹æ§åˆ¶ï¼‰
        
        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            # éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
            ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆå¤šç®—æ³•é”¤é€Ÿä¸å»¶æ—¶æ•£ç‚¹å›¾ï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")
            
            fig = go.Figure()
            
            # è®¡ç®—xè½´èŒƒå›´
            x_min, x_max = self._calculate_log_velocity_range(ready_algorithms)

            for alg_idx, algorithm in enumerate(ready_algorithms):
                metadata = self._extract_algorithm_metadata(algorithm)
                algorithm_name = metadata['algorithm_name']
                descriptive_name = metadata['descriptive_name']
                display_name = metadata['display_name']

                # éªŒè¯åˆ†æå™¨
                if not self._validate_analyzer(algorithm, descriptive_name):
                    continue

                try:
                    # è·å–åŒ¹é…å¯¹
                    matched_pairs = self._get_matched_pairs(algorithm)
                    if not matched_pairs:
                        logger.warning(f"âš ï¸ ç®—æ³• '{descriptive_name}' æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # è·å–åç§»æ•°æ®å¹¶åˆ›å»ºæ˜ å°„
                    offset_data = self._get_offset_data(algorithm)
                    offset_map = self._create_offset_map(offset_data)

                    # æå–é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®ï¼ˆä½¿ç”¨algorithm_nameä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                    hammer_velocities, delays_ms, scatter_customdata = \
                        self._extract_hammer_velocity_delay_data(matched_pairs, offset_map, algorithm_name)

                    if not hammer_velocities:
                        logger.warning(f"âš ï¸ ç®—æ³• '{descriptive_name}' æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # è®¡ç®—Z-Score
                    z_scores, mu, sigma = self._calculate_zscore_values(delays_ms, algorithm)

                    # å°†é”¤é€Ÿè½¬æ¢ä¸ºå¯¹æ•°å½¢å¼
                    log_velocities = [math.log10(v) for v in hammer_velocities]

                    # è·å–ç®—æ³•é¢œè‰²
                    color = self._get_algorithm_color(alg_idx)

                    # æ·»åŠ æ•£ç‚¹å›¾trace
                    self._add_hammer_velocity_zscore_scatter_trace(
                        fig, log_velocities, z_scores, delays_ms,
                        hammer_velocities, scatter_customdata, descriptive_name, color
                    )

                    # æ·»åŠ Z-Scoreé˜ˆå€¼çº¿
                    if len(log_velocities) > 0:
                        self._add_zscore_threshold_lines_for_velocity(
                            fig, x_min, x_max, descriptive_name, color
                        )

                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–ç®—æ³• '{descriptive_name}' çš„é”¤é€Ÿä¸å»¶æ—¶æ•°æ®å¤±è´¥: {e}")
                    continue

            # é…ç½®å¸ƒå±€
            self._configure_hammer_velocity_zscore_layout(fig)

            logger.info(f"âœ… å¤šç®—æ³•é”¤é€Ÿä¸å»¶æ—¶æ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•é”¤é€Ÿä¸å»¶æ—¶æ•£ç‚¹å›¾")
    
    def generate_multi_algorithm_key_hammer_velocity_scatter_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾ï¼ˆé¢œè‰²è¡¨ç¤ºå»¶æ—¶ï¼Œå åŠ æ˜¾ç¤ºï¼Œä¸åŒæ ‡è®°å½¢çŠ¶åŒºåˆ†ç®—æ³•ï¼‰
        
        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            go.Figure: Plotlyå›¾è¡¨å¯¹è±¡
        """
        try:
            # éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
            ready_algorithms = self._validate_and_prepare_algorithms(algorithms)
            if not ready_algorithms:
                return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")
            
            logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆå¤šç®—æ³•æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾ï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")
            
            fig = go.Figure()
            all_delays = []
            
            for alg_idx, algorithm in enumerate(ready_algorithms):
                metadata = self._extract_algorithm_metadata(algorithm)
                algorithm_name = metadata['algorithm_name']
                descriptive_name = metadata['descriptive_name']

                # éªŒè¯åˆ†æå™¨
                if not self._validate_analyzer(algorithm, descriptive_name):
                    continue

                try:
                    # è·å–åŒ¹é…å¯¹
                    matched_pairs = self._get_matched_pairs(algorithm)
                    if not matched_pairs:
                        logger.warning(f"âš ï¸ ç®—æ³• '{descriptive_name}' æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # è·å–åç§»æ•°æ®å¹¶åˆ›å»ºæ˜ å°„
                    offset_data = self._get_offset_data(algorithm)
                    offset_map = self._create_offset_map(offset_data)
                    
                    # æå–æŒ‰é”®IDã€é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®
                    key_ids, hammer_velocities, delays_ms = \
                        self._extract_key_hammer_velocity_data(matched_pairs, offset_map)
                    
                    if not key_ids:
                        logger.warning(f"âš ï¸ ç®—æ³• '{descriptive_name}' æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®ï¼Œè·³è¿‡")
                        continue
                    
                    # æ·»åŠ åˆ°æ€»å»¶æ—¶åˆ—è¡¨
                    all_delays.extend(delays_ms)
                    
                    # æ·»åŠ æ•£ç‚¹å›¾trace
                    self._add_key_hammer_velocity_scatter_trace(
                        fig, key_ids, hammer_velocities, delays_ms,
                        descriptive_name, algorithm_name, alg_idx,
                        len(ready_algorithms), all_delays
                    )
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–ç®—æ³• '{descriptive_name}' çš„æŒ‰é”®ä¸é”¤é€Ÿæ•°æ®å¤±è´¥: {e}")
                    continue
            
            if not all_delays:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
                return self._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®")
            
            # é…ç½®å¸ƒå±€
            self._configure_key_hammer_velocity_layout(fig)
            
            logger.info(f"âœ… å¤šç®—æ³•æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")
            return fig
            
        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾")
    
    def _create_empty_plot(self, message: str) -> go.Figure:
        """åˆ›å»ºç©ºå›¾è¡¨ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰"""
        fig = go.Figure()
        fig.add_annotation(
            x=0.5,
            y=0.5,
            text=message,
            showarrow=False,
            font=dict(size=16, color='gray'),
            xref='paper',
            yref='paper'
        )
        fig.update_layout(
            xaxis=dict(showgrid=False, showticklabels=False),
            yaxis=dict(showgrid=False, showticklabels=False),
            height=400
        )
        return fig
    
    # ==================== é€šç”¨è¾…åŠ©æ–¹æ³• ====================
    
    def _validate_and_prepare_algorithms(self, algorithms: List[AlgorithmDataset]) -> Optional[List[AlgorithmDataset]]:
        """
        éªŒè¯å’Œå‡†å¤‡ç®—æ³•åˆ—è¡¨
        
        Args:
            algorithms: ç®—æ³•æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            Optional[List[AlgorithmDataset]]: å°±ç»ªçš„ç®—æ³•åˆ—è¡¨ï¼Œå¦‚æœéªŒè¯å¤±è´¥è¿”å›None
        """
        if not algorithms:
            logger.warning("æ²¡æœ‰ä¼ å…¥ä»»ä½•ç®—æ³•")
            return None
        
        ready_algorithms = [alg for alg in algorithms if alg.is_active and alg.is_ready()]
        
        if not ready_algorithms:
            logger.warning("æ²¡æœ‰æ¿€æ´»ä¸”å°±ç»ªçš„ç®—æ³•")
            return None
        
        logger.info(f"éªŒè¯é€šè¿‡: å…± {len(ready_algorithms)} ä¸ªå°±ç»ªç®—æ³•")
        return ready_algorithms
    
    def _extract_algorithm_metadata(self, algorithm: AlgorithmDataset) -> Dict[str, str]:
        """
        æå–ç®—æ³•å…ƒæ•°æ®
        
        Args:
            algorithm: ç®—æ³•æ•°æ®é›†
            
        Returns:
            Dict[str, str]: åŒ…å« algorithm_name, display_name, filename, descriptive_name çš„å­—å…¸
        """
        return {
            'algorithm_name': algorithm.metadata.algorithm_name,
            'display_name': algorithm.metadata.display_name,
            'filename': algorithm.metadata.filename,
            'descriptive_name': f"{algorithm.metadata.display_name} ({algorithm.metadata.filename})"
        }
    
    def _validate_analyzer(self, algorithm: AlgorithmDataset, descriptive_name: str) -> bool:
        """
        éªŒè¯ç®—æ³•çš„åˆ†æå™¨æ˜¯å¦å¯ç”¨
        
        Args:
            algorithm: ç®—æ³•æ•°æ®é›†
            descriptive_name: ç®—æ³•çš„æè¿°æ€§åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            bool: åˆ†æå™¨å¯ç”¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
            logger.warning(f"âš ï¸ ç®—æ³• '{descriptive_name}' æ²¡æœ‰åˆ†æå™¨æˆ–åŒ¹é…å™¨ï¼Œè·³è¿‡")
            return False
        return True
    
    def _get_algorithm_color(self, algorithm_index: int) -> str:
        """
        è·å–ç®—æ³•å¯¹åº”çš„é¢œè‰²
        
        Args:
            algorithm_index: ç®—æ³•ç´¢å¼•
            
        Returns:
            str: é¢œè‰²ä»£ç 
        """
        return self.COLORS[algorithm_index % len(self.COLORS)]
    
    # ==================== æ•°æ®è·å–å’Œè½¬æ¢æ–¹æ³•ï¼ˆæ¶ˆé™¤é‡å¤ä»£ç ï¼‰ ====================
    
    def _get_offset_data(self, algorithm: AlgorithmDataset):
        """
        è·å–ç®—æ³•çš„ç²¾ç¡®åç§»å¯¹é½æ•°æ®
        
        Args:
            algorithm: ç®—æ³•æ•°æ®é›†
            
        Returns:
            åç§»å¯¹é½æ•°æ®åˆ—è¡¨ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            if algorithm.analyzer:
                return algorithm.analyzer.get_precision_offset_alignment_data() or []
        except Exception as e:
            logger.warning(f"è·å–åç§»æ•°æ®å¤±è´¥: {e}")
        return []
    
    def _get_matched_pairs(self, algorithm: AlgorithmDataset):
        """
        è·å–ç®—æ³•çš„åŒ¹é…å¯¹æ•°æ®ï¼Œè½¬æ¢ä¸ºæ•£ç‚¹å›¾æ‰€éœ€çš„æ ¼å¼

        Args:
            algorithm: ç®—æ³•æ•°æ®é›†

        Returns:
            åŒ¹é…å¯¹åˆ—è¡¨ [(record_idx, replay_idx, record_note, replay_note), ...]
        """
        try:
            if algorithm.analyzer and algorithm.analyzer.matched_pairs:
                # å°†å½“å‰æ ¼å¼ (record_note, replay_note, match_type, error_ms)
                # è½¬æ¢ä¸ºæ•£ç‚¹å›¾æ‰€éœ€çš„æ ¼å¼ (record_idx, replay_idx, record_note, replay_note)
                result = []
                for idx, (rec_note, rep_note, match_type, error_ms) in enumerate(algorithm.analyzer.matched_pairs):
                    # ä½¿ç”¨UUIDä½œä¸ºç´¢å¼•ï¼Œå› ä¸ºUUIDæ˜¯å…¨å±€å”¯ä¸€çš„æ ‡è¯†ç¬¦
                    record_idx = getattr(rec_note, 'uuid', f"rec_{idx}")
                    replay_idx = getattr(rep_note, 'uuid', f"rep_{idx}")
                    result.append((record_idx, replay_idx, rec_note, rep_note))
                return result
        except Exception as e:
            logger.warning(f"è·å–åŒ¹é…å¯¹å¤±è´¥: {e}")
        return []
    
    def _create_note_dicts(self, matched_pairs):
        """
        ä»åŒ¹é…å¯¹åˆ›å»ºéŸ³ç¬¦å­—å…¸
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨ [(record_idx, replay_idx, record_note, replay_note), ...]
            
        Returns:
            Tuple[Dict, Dict]: (record_note_dict, replay_note_dict)
        """
        record_note_dict = {r_idx: r_note for r_idx, _, r_note, _ in matched_pairs}
        replay_note_dict = {p_idx: p_note for _, p_idx, _, p_note in matched_pairs}
        return record_note_dict, replay_note_dict
    
    def _create_offset_map(self, offset_data):
        """
        åˆ›å»ºåç§»æ•°æ®ç´¢å¼•æ˜ å°„
        
        Args:
            offset_data: åç§»å¯¹é½æ•°æ®åˆ—è¡¨
            
        Returns:
            Dict: ä»¥ (record_index, replay_index) ä¸ºé”®çš„åç§»æ•°æ®å­—å…¸
        """
        offset_map = {}
        for item in offset_data:
            record_idx = item.get('record_index')
            replay_idx = item.get('replay_index')
            if record_idx is not None and replay_idx is not None:
                offset_map[(record_idx, replay_idx)] = item
        return offset_map
    
    def _convert_offset_to_ms(self, keyon_offset) -> float:
        """
        å°†åç§»é‡ä» 0.1ms å•ä½è½¬æ¢ä¸º ms
        
        Args:
            keyon_offset: åç§»é‡ï¼ˆ0.1ms å•ä½ï¼‰
            
        Returns:
            float: åç§»é‡ï¼ˆms å•ä½ï¼‰
        """
        return keyon_offset / 10.0
    
    def _filter_algorithms_by_names(self, algorithms: List[AlgorithmDataset], 
                                    selected_names: List[str] = None) -> List[AlgorithmDataset]:
        """æ ¹æ®åç§°ç­›é€‰ç®—æ³•"""
        if selected_names:
            filtered = [alg for alg in algorithms 
                       if alg.metadata.algorithm_name in selected_names]
            logger.info(f"æ ¹æ®ç”¨æˆ·é€‰æ‹©ç­›é€‰ç®—æ³•: {selected_names} -> æ‰¾åˆ° {len(filtered)} ä¸ªåŒ¹é…ç®—æ³•")
            return filtered
        else:
            logger.info("æœªæŒ‡å®šç®—æ³•ç­›é€‰ï¼Œä½¿ç”¨æ‰€æœ‰ä¼ å…¥ç®—æ³•")
            return algorithms
    
    def _calculate_common_keys(self, algorithms: List[AlgorithmDataset]) -> Optional[set]:
        """è®¡ç®—æ‰€æœ‰ç®—æ³•çš„å…¬å…±æŒ‰é”®"""
        key_sets = []
        for alg in algorithms:
            if alg.analyzer and alg.analyzer.note_matcher:
                offset_data = alg.analyzer.note_matcher.get_precision_offset_alignment_data()
                if offset_data:
                    keys = set(item.get('key_id') for item in offset_data 
                              if item.get('key_id') is not None)
                    key_sets.append(keys)
        
        if key_sets:
            common_keys = set.intersection(*key_sets)
            logger.info(f"åªæ˜¾ç¤ºå…¬å…±æŒ‰é”®: å…± {len(common_keys)} ä¸ª")
            return common_keys
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…¬å…±æŒ‰é”®")
            return set()
    
    def _collect_scatter_algorithm_data(self, algorithm: AlgorithmDataset, alg_idx: int,
                                       common_keys: Optional[set]) -> Optional[Dict]:
        """æ”¶é›†å•ä¸ªç®—æ³•çš„æ•£ç‚¹å›¾æ•°æ®"""
        metadata = self._extract_algorithm_metadata(algorithm)
        if not self._validate_analyzer(algorithm, metadata['descriptive_name']):
            return None
        
        try:
            # è·å–åŸºç¡€æ•°æ®
            offset_data = self._get_offset_data(algorithm)
            if not offset_data:
                logger.warning(f"âš ï¸ ç®—æ³• '{metadata['descriptive_name']}' æ²¡æœ‰ç²¾ç¡®åŒ¹é…æ•°æ®ï¼Œè·³è¿‡")
                return None
            
            # è·å–å¹³å‡å»¶æ—¶
            algorithm_mean_delay_ms = self._calculate_mean_delay(algorithm.analyzer)
            
            # è·å–åŒ¹é…å¯¹
            matched_pairs = self._get_matched_pairs(algorithm)
            record_note_dict, replay_note_dict = self._create_note_dicts(matched_pairs)
            
            # æå–å»¶æ—¶æ•°æ®ï¼ˆä¼ é€’algorithm_nameä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
            delay_data = self._extract_scatter_delay_data(
                offset_data, record_note_dict, replay_note_dict, 
                common_keys, metadata['algorithm_name']
            )
            
            if not delay_data['key_ids']:
                logger.warning(f"âš ï¸ ç®—æ³• '{metadata['descriptive_name']}' æ²¡æœ‰æœ‰æ•ˆçš„æ•£ç‚¹å›¾æ•°æ®ï¼Œè·³è¿‡")
                return None
            
            # è®¡ç®—ç»Ÿè®¡é‡å’Œç›¸å¯¹å»¶æ—¶
            stats = self._calculate_scatter_statistics(
                delay_data['delays_ms'], algorithm.analyzer
            )
            
            # åˆå¹¶æ•°æ®ï¼ˆå…ˆåˆå¹¶å†æ’åºï¼Œå› ä¸ºæ’åºéœ€è¦relative_delays_msï¼‰
            result_data = {
                **metadata,
                **delay_data,
                **stats,
                'color': self._get_algorithm_color(alg_idx),
                'algorithm_mean_delay_ms': algorithm_mean_delay_ms
            }
            
            # æ’åºæ•°æ®
            self._sort_scatter_data(result_data)
            
            return result_data
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ç®—æ³• '{metadata['descriptive_name']}' çš„æŒ‰é”®ä¸å»¶æ—¶æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_scatter_delay_data(self, offset_data: List[Dict], 
                                    record_note_dict: Dict, replay_note_dict: Dict,
                                    common_keys: Optional[set], algorithm_name: str) -> Dict:
        """æå–æ•£ç‚¹å›¾çš„å»¶æ—¶æ•°æ®"""
        key_ids = []
        delays_ms = []
        customdata_list = []
        
        for item in offset_data:
            key_id = item.get('key_id')
            if key_id is None or key_id == 'N/A':
                continue
            
            # è¿‡æ»¤éå…¬å…±æŒ‰é”®
            if common_keys is not None and key_id not in common_keys:
                continue
            
            try:
                key_id_int = int(key_id)
                keyon_offset = item.get('keyon_offset', 0)
                delay_ms = self._convert_offset_to_ms(keyon_offset)
                
                # è·å–é”¤å­æ—¶é—´
                record_index = item.get('record_index')
                replay_index = item.get('replay_index')
                record_hammer_time = self._get_hammer_time(record_note_dict, record_index)
                replay_hammer_time = self._get_hammer_time(replay_note_dict, replay_index)

                # è·å–é”¤é€Ÿå€¼å’ŒæŒç»­æ—¶é—´
                record_velocity = self._get_velocity_from_note_dict(record_note_dict, record_index)
                replay_velocity = self._get_velocity_from_note_dict(replay_note_dict, replay_index)
                record_duration = self._get_duration_from_note_dict(record_note_dict, record_index)
                replay_duration = self._get_duration_from_note_dict(replay_note_dict, replay_index)

                key_ids.append(key_id_int)
                delays_ms.append(delay_ms)
                customdata_list.append([
                    record_index, replay_index, key_id_int, delay_ms,
                    algorithm_name, record_hammer_time, replay_hammer_time,
                    record_velocity, replay_velocity, record_duration, replay_duration
                ])
            except (ValueError, TypeError):
                continue
        
        return {
            'key_ids': key_ids,
            'delays_ms': delays_ms,
            'customdata': customdata_list
        }
    
    def _get_hammer_time(self, note_dict: Dict, index: int) -> float:
        """è·å–éŸ³ç¬¦çš„é”¤å­æ—¶é—´"""
        if index in note_dict:
            note = note_dict[index]
            return note.first_hammer_time
        return 0.0

    def _get_velocity_from_note_dict(self, note_dict: Dict, index: int) -> int:
        """ä»éŸ³ç¬¦å­—å…¸è·å–é”¤é€Ÿå€¼"""
        if index in note_dict:
            note = note_dict[index]
            return note.first_hammer_velocity
        return 0

    def _get_duration_from_note_dict(self, note_dict: Dict, index: int) -> float:
        """ä»éŸ³ç¬¦å­—å…¸è·å–æŒç»­æ—¶é—´"""
        if index in note_dict:
            note = note_dict[index]
            return note.duration_ms
        return 0.0
    
    def _calculate_scatter_statistics(self, delays_ms: List[float], analyzer) -> Dict:
        """è®¡ç®—æ•£ç‚¹å›¾ç»Ÿè®¡é‡"""
        # è·å–æ€»ä½“ç»Ÿè®¡ï¼ˆå¤ç”¨analyzeræ–¹æ³•ï¼‰
        me_0_1ms = analyzer.get_mean_error()
        std_0_1ms = analyzer.get_standard_deviation()
        mu = me_0_1ms / 10.0
        sigma = std_0_1ms / 10.0
        
        # è®¡ç®—ç›¸å¯¹å»¶æ—¶
        delays_array = np.array(delays_ms)
        relative_delays_array = delays_array - mu
        relative_delays_ms = relative_delays_array.tolist()
        
        # è®¡ç®—ç›¸å¯¹å»¶æ—¶çš„é˜ˆå€¼
        if len(relative_delays_ms) > 1:
            relative_mu = np.mean(relative_delays_array)
            relative_sigma = np.std(relative_delays_array, ddof=1)
            upper_threshold = relative_mu + 3 * relative_sigma
            lower_threshold = relative_mu - 3 * relative_sigma
        else:
            relative_mu = 0.0
            relative_sigma = 0.0
            upper_threshold = 0.0
            lower_threshold = 0.0
        
        return {
            'mu': mu,
            'sigma': sigma,
            'relative_delays_ms': relative_delays_ms,
            'relative_mu': relative_mu,
            'relative_sigma': relative_sigma,
            'upper_threshold': upper_threshold,
            'lower_threshold': lower_threshold
        }
    
    def _sort_scatter_data(self, data: Dict):
        """æŒ‰é”®IDæ’åºæ•£ç‚¹å›¾æ•°æ®"""
        sorted_indices = sorted(range(len(data['key_ids'])), 
                               key=lambda i: data['key_ids'][i])
        data['key_ids'][:] = [data['key_ids'][i] for i in sorted_indices]
        data['delays_ms'][:] = [data['delays_ms'][i] for i in sorted_indices]
        data['relative_delays_ms'][:] = [data['relative_delays_ms'][i] for i in sorted_indices]
        data['customdata'][:] = [data['customdata'][i] for i in sorted_indices]
    
    def _add_scatter_plot_traces(self, fig: go.Figure, algorithm_data_list: List[Dict]):
        """æ·»åŠ æ•£ç‚¹å›¾traces"""
        for alg_data in algorithm_data_list:
            # è®¡ç®—markeræ ·å¼
            marker_colors = []
            marker_sizes = []
            for relative_delay in alg_data['relative_delays_ms']:
                if (relative_delay > alg_data['upper_threshold'] or 
                    relative_delay < alg_data['lower_threshold']):
                    marker_colors.append(alg_data['color'])
                    marker_sizes.append(12)
                else:
                    marker_colors.append(alg_data['color'])
                    marker_sizes.append(8)
            
            key_id_strings = [str(kid) for kid in alg_data['key_ids']]
            
            fig.add_trace(go.Scattergl(
                x=key_id_strings,
                y=alg_data['relative_delays_ms'],
                mode='markers',
                name=f"{alg_data['descriptive_name']} - åŒ¹é…å¯¹",
                marker=dict(
                    size=marker_sizes,
                    color=marker_colors,
                    opacity=0.6,
                    line=dict(width=1, color=alg_data['color'])
                ),
                customdata=alg_data['customdata'],
                legendgroup=alg_data['descriptive_name'],
                showlegend=True,
                hovertemplate=f"ç®—æ³•: {alg_data['descriptive_name']}<br>æŒ‰é”®: %{{customdata[2]}}<br>ç›¸å¯¹å»¶æ—¶: %{{y:.2f}}ms<br>ç»å¯¹å»¶æ—¶: %{{customdata[3]:.2f}}ms<br>å¹³å‡å»¶æ—¶: {alg_data['algorithm_mean_delay_ms']:.2f}ms<br>å½•åˆ¶é”¤å‡»æ—¶é—´: %{{customdata[5]:.2f}}ms<br>æ’­æ”¾é”¤å‡»æ—¶é—´: %{{customdata[6]:.2f}}ms<br>å½•åˆ¶é”¤é€Ÿ: %{{customdata[7]}}<br>æ’­æ”¾é”¤é€Ÿ: %{{customdata[8]}}<br>å½•åˆ¶æŒç»­æ—¶é—´: %{{customdata[9]:.2f}}ms<br>æ’­æ”¾æŒç»­æ—¶é—´: %{{customdata[10]:.2f}}ms<extra></extra>"
            ))
    
    def _add_scatter_threshold_lines(self, fig: go.Figure, algorithm_data_list: List[Dict]):
        """æ·»åŠ æ•£ç‚¹å›¾é˜ˆå€¼çº¿"""
        # è·å–æ‰€æœ‰æŒ‰é”®ID
        all_key_ids = set()
        for alg_data in algorithm_data_list:
            all_key_ids.update(alg_data['key_ids'])
        key_labels = [str(kid) for kid in sorted(all_key_ids)]
        
        for alg_data in algorithm_data_list:
            descriptive_name = alg_data['descriptive_name']
            color = alg_data['color']
            
            # å¹³å‡å€¼çº¿ï¼ˆ0çº¿ï¼‰
            fig.add_trace(go.Scattergl(
                x=key_labels,
                y=[0] * len(key_labels),
                mode='lines',
                name=f"{descriptive_name} - å¹³å‡å€¼",
                line=dict(color=color, width=1.5, dash='dot'),
                legendgroup=descriptive_name,
                showlegend=True,
                hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶å¹³å‡å€¼ = 0ms<br>ç»å¯¹å»¶æ—¶å¹³å‡å€¼ = {alg_data['mu']:.2f}ms<extra></extra>"
            ))
            
            # ä¸Šé˜ˆå€¼çº¿
            fig.add_trace(go.Scattergl(
                x=key_labels,
                y=[alg_data['upper_threshold']] * len(key_labels),
                mode='lines',
                name=f"{descriptive_name} - ä¸Šé˜ˆå€¼",
                line=dict(color=color, width=2, dash='dash'),
                legendgroup=descriptive_name,
                showlegend=True,
                hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶ä¸Šé˜ˆå€¼ = {alg_data['upper_threshold']:.2f}ms<extra></extra>"
            ))
            
            # ä¸‹é˜ˆå€¼çº¿
            fig.add_trace(go.Scattergl(
                x=key_labels,
                y=[alg_data['lower_threshold']] * len(key_labels),
                mode='lines',
                name=f"{descriptive_name} - ä¸‹é˜ˆå€¼",
                line=dict(color=color, width=2, dash='dash'),
                legendgroup=descriptive_name,
                showlegend=True,
                hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶ä¸‹é˜ˆå€¼ = {alg_data['lower_threshold']:.2f}ms<extra></extra>"
            ))
    
    def _configure_scatter_plot_layout(self, fig: go.Figure):
        """é…ç½®æ•£ç‚¹å›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='æŒ‰é”®',
            yaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
            xaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1,
                type='category'
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            hovermode='closest',
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=800,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=90, b=60, l=60, r=60)
        )
    
    def _calculate_zscore_values(self, delays_ms: List[float], algorithm) -> Tuple[List[float], float, float]:
        """
        è®¡ç®—Z-Scoreå€¼
        
        Args:
            delays_ms: ç»å¯¹å»¶æ—¶åˆ—è¡¨ï¼ˆæ¯«ç§’ï¼‰
            algorithm: ç®—æ³•æ•°æ®é›†å¯¹è±¡
            
        Returns:
            Tuple[List[float], float, float]: (Z-Scoreåˆ—è¡¨, å‡å€¼, æ ‡å‡†å·®)
        """
        # è·å–æ€»ä½“å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆ0.1mså•ä½ï¼‰
        me_0_1ms = algorithm.analyzer.get_mean_error()
        std_0_1ms = algorithm.analyzer.get_standard_deviation()
        
        # è½¬æ¢ä¸ºmså•ä½
        mu = me_0_1ms / 10.0
        sigma = std_0_1ms / 10.0
        
        # è®¡ç®—Z-Scoreï¼šz = (x_i - Î¼) / Ïƒ
        if sigma > 0:
            delays_array = np.array(delays_ms)
            z_scores_array = (delays_array - mu) / sigma
            z_scores = z_scores_array.tolist()
            
            logger.info(f"ğŸ” Z-Scoreè®¡ç®—: Î¼={mu:.2f}ms, Ïƒ={sigma:.2f}ms, "
                       f"åŸå§‹å»¶æ—¶èŒƒå›´=[{delays_array.min():.2f}, {delays_array.max():.2f}]ms, "
                       f"Z-ScoreèŒƒå›´=[{z_scores_array.min():.2f}, {z_scores_array.max():.2f}]")
        else:
            z_scores = [0.0] * len(delays_ms)
            logger.warning(f"âš ï¸ æ ‡å‡†å·®ä¸º0ï¼Œæ— æ³•è¿›è¡ŒZ-Scoreæ ‡å‡†åŒ–")
        
        return z_scores, mu, sigma
    
    def _add_zscore_scatter_traces(self, fig: go.Figure, key_ids: List[int], z_scores: List[float],
                                   customdata_list: List, descriptive_name: str, color: str):
        """æ·»åŠ Z-Scoreæ•£ç‚¹å›¾traces"""
        fig.add_trace(go.Scattergl(
            x=[str(kid) for kid in key_ids],
            y=z_scores,
            mode='markers',
            name=f"{descriptive_name} - Z-Score",
            marker=dict(
                size=8,
                color=color,
                opacity=0.6,
                line=dict(width=1, color=color)
            ),
            customdata=customdata_list,
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f"ç®—æ³•: {descriptive_name}<br>é”®ä½: %{{x}}<br>"
                         f"å»¶æ—¶: %{{customdata[3]:.2f}}ms<br>Z-Score: %{{y:.2f}}<br>"
                         f"å½•åˆ¶é”¤å‡»æ—¶é—´: %{{customdata[5]:.2f}}ms<br>"
                         f"æ’­æ”¾é”¤å‡»æ—¶é—´: %{{customdata[6]:.2f}}ms<br>"
                         f"å½•åˆ¶é”¤é€Ÿ: %{{customdata[7]}}<br>"
                         f"æ’­æ”¾é”¤é€Ÿ: %{{customdata[8]}}<br>"
                         f"å½•åˆ¶æŒç»­æ—¶é—´: %{{customdata[9]:.2f}}ms<br>"
                         f"æ’­æ”¾æŒç»­æ—¶é—´: %{{customdata[10]:.2f}}ms<extra></extra>"
        ))
    
    def _add_zscore_threshold_lines(self, fig: go.Figure, key_labels: List[str],
                                    ready_algorithms: List, thresholds: List[float] = [0, 3, -3]):
        """
        æ·»åŠ Z-Scoreé˜ˆå€¼çº¿
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            key_labels: æŒ‰é”®æ ‡ç­¾åˆ—è¡¨
            ready_algorithms: å‡†å¤‡å¥½çš„ç®—æ³•åˆ—è¡¨
            thresholds: é˜ˆå€¼åˆ—è¡¨ï¼Œé»˜è®¤[0, 3, -3]
        """
        threshold_names = {0: 'Z=0 (å‡å€¼çº¿)', 3: 'Z=+3 (ä¸Šé˜ˆå€¼)', -3: 'Z=-3 (ä¸‹é˜ˆå€¼)'}
        threshold_styles = {0: 'dot', 3: 'dash', -3: 'dash'}
        threshold_widths = {0: 1.5, 3: 2, -3: 2}
        
        for alg_idx, algorithm in enumerate(ready_algorithms):
            metadata = self._extract_algorithm_metadata(algorithm)
            descriptive_name = metadata['descriptive_name']
            color = self._get_algorithm_color(alg_idx)
            
            for threshold in thresholds:
                fig.add_trace(go.Scattergl(
                    x=key_labels,
                    y=[threshold] * len(key_labels),
                    mode='lines',
                    name=f"{descriptive_name} - {threshold_names[threshold].split()[0]}",
                    line=dict(
                        color=color,
                        width=threshold_widths[threshold],
                        dash=threshold_styles[threshold]
                    ),
                    legendgroup=descriptive_name,
                    showlegend=True,
                    hovertemplate=f"ç®—æ³•: {descriptive_name}<br>{threshold_names[threshold]}<extra></extra>"
                ))
    
    def _configure_zscore_layout(self, fig: go.Figure):
        """é…ç½®Z-Scoreæ•£ç‚¹å›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='æŒ‰é”®ID',
            yaxis_title='Z-Score (æ ‡å‡†åŒ–å»¶æ—¶)',
            xaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1,
                type='category'
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            hovermode='closest',
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=800,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=90, b=60, l=60, r=60)
        )
    
    def _extract_single_key_delays(self, offset_data: List[Dict], target_key_id: int, 
                                   filename: str) -> Tuple[List[float], List]:
        """
        æå–å•ä¸ªæŒ‰é”®çš„å»¶æ—¶æ•°æ®
        
        Args:
            offset_data: åç§»æ•°æ®åˆ—è¡¨
            target_key_id: ç›®æ ‡æŒ‰é”®ID
            filename: æ–‡ä»¶åï¼ˆç”¨äºcustomdataï¼‰
            
        Returns:
            Tuple[List[float], List]: (å»¶æ—¶åˆ—è¡¨, customdataåˆ—è¡¨)
        """
        key_delays = []
        customdata_list = []
        
        for item in offset_data:
            key_id = item.get('key_id')
            if key_id == target_key_id:
                keyon_offset = item.get('keyon_offset', 0)
                delay_ms = keyon_offset / 10.0  # è½¬æ¢ä¸ºms
                key_delays.append(delay_ms)
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºæ‚¬åœ
                record_index = item.get('record_index')
                replay_index = item.get('replay_index')
                customdata_list.append([record_index, replay_index, delay_ms, filename])
        
        return key_delays, customdata_list
    
    def _add_box_trace(self, fig: go.Figure, key_delays: List[float], display_name: str,
                      target_key_id: int, customdata_list: List, color: str):
        """
        æ·»åŠ ç®±çº¿å›¾trace
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            key_delays: å»¶æ—¶åˆ—è¡¨
            display_name: ç®—æ³•æ˜¾ç¤ºåç§°
            target_key_id: ç›®æ ‡æŒ‰é”®ID
            customdata_list: è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨
            color: é¢œè‰²
        """
        fig.add_trace(go.Box(
            y=key_delays,
            x=[display_name] * len(key_delays),  # Xè½´ä¸ºç®—æ³•åç§°
            name=display_name,
            boxpoints='all',  # æ˜¾ç¤ºæ‰€æœ‰ç‚¹
            jitter=0.5,       # ç‚¹çš„æŠ–åŠ¨èŒƒå›´
            pointpos=-1.8,    # ç‚¹æ˜¾ç¤ºåœ¨ç®±çº¿å›¾å·¦ä¾§
            marker=dict(
                color=color,
                size=6,
                opacity=0.7
            ),
            line=dict(color=color),
            fillcolor='rgba(255,255,255,0)',  # é€æ˜å¡«å……
            showlegend=False,  # ç®±çº¿å›¾ä¸æ˜¾ç¤ºå›¾ä¾‹ï¼Œé¿å…é‡å¤
            customdata=customdata_list,
            hovertemplate=f'ç®—æ³•: {display_name}<br>æŒ‰é”®: {target_key_id}<br>å»¶æ—¶: %{{y:.2f}}ms<extra></extra>'
        ))
    
    def _configure_single_key_layout(self, fig: go.Figure, target_key_id: int):
        """
        é…ç½®å•é”®å¯¹æ¯”å›¾å¸ƒå±€
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            target_key_id: ç›®æ ‡æŒ‰é”®ID
        """
        fig.update_layout(
            title=dict(
                text=f"æŒ‰é”® {target_key_id} å»¶æ—¶åˆ†å¸ƒå¯¹æ¯” (å¤šæ›²ç›®/ç®—æ³•)",
                x=0.5,
                xanchor='center'
            ),
            xaxis=dict(
                title="æ›²å­ / ç®—æ³•",
                showgrid=False
            ),
            yaxis=dict(
                title="ç›¸å¯¹å»¶æ—¶ (ms)",
                showgrid=True,
                gridcolor='lightgray',
                zeroline=True,
                zerolinecolor='gray'
            ),
            plot_bgcolor='white',
            paper_bgcolor='white',
            hovermode='closest',
            showlegend=False,  # ä¸éœ€è¦å›¾ä¾‹ï¼ŒXè½´æ ‡ç­¾å·²è¯´æ˜
            height=400,
            margin=dict(l=60, r=40, t=60, b=40)
        )
    
    def _extract_hammer_velocity_delay_data(self, matched_pairs: List[Tuple], 
                                           offset_map: Dict, algorithm_name: str) -> Tuple[List, List, List]:
        """
        æå–é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨
            offset_map: åç§»æ•°æ®æ˜ å°„
            algorithm_name: ç®—æ³•å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆç®—æ³•å_æ–‡ä»¶åï¼‰
            
        Returns:
            Tuple[List, List, List]: (é”¤é€Ÿåˆ—è¡¨, å»¶æ—¶åˆ—è¡¨(ms), customdataåˆ—è¡¨)
        """
        hammer_velocities = []
        delays_ms = []
        scatter_customdata = []
        
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            # è·å–æ’­æ”¾éŸ³ç¬¦çš„é”¤é€Ÿï¼ˆç¬¬ä¸€ä¸ªé”¤é€Ÿå€¼ï¼‰
            if len(replay_note.hammers) > 0 and len(replay_note.hammers.values) > 0:
                hammer_velocity = replay_note.hammers.values[0]
            else:
                continue
            
            # è·³è¿‡é”¤é€Ÿä¸º0æˆ–è´Ÿæ•°çš„æ•°æ®ç‚¹ï¼ˆå¯¹æ•°æ— æ³•å¤„ç†ï¼‰
            if hammer_velocity <= 0:
                continue
            
            # ä»åç§»æ•°æ®ä¸­è·å–å»¶æ—¶
            if (record_idx, replay_idx) not in offset_map:
                continue
            
            keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
            delay_ms = self._convert_offset_to_ms(keyon_offset)
            
            # è·å–æŒ‰é”®ID
            key_id = record_note.id if hasattr(record_note, 'id') else None
            
            hammer_velocities.append(hammer_velocity)
            delays_ms.append(delay_ms)
            # ä½¿ç”¨algorithm_nameï¼ˆå®Œæ•´çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼‰è€Œä¸æ˜¯display_name
            scatter_customdata.append([record_idx, replay_idx, algorithm_name, key_id])
        
        return hammer_velocities, delays_ms, scatter_customdata
    
    def _calculate_relative_delay_statistics(self, delays_ms: List[float], 
                                            algorithm) -> Tuple[List[float], Dict]:
        """
        è®¡ç®—ç›¸å¯¹å»¶æ—¶ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            delays_ms: ç»å¯¹å»¶æ—¶åˆ—è¡¨(ms)
            algorithm: ç®—æ³•æ•°æ®é›†
            
        Returns:
            Tuple[List[float], Dict]: (ç›¸å¯¹å»¶æ—¶åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
        """
        # è·å–è¯¥ç®—æ³•çš„æ€»ä½“å‡å€¼å’Œæ ‡å‡†å·®
        me_0_1ms = algorithm.analyzer.get_mean_error() if hasattr(algorithm.analyzer, 'get_mean_error') else 0.0
        std_0_1ms = algorithm.analyzer.get_standard_deviation() if hasattr(algorithm.analyzer, 'get_standard_deviation') else 0.0
        
        mu = me_0_1ms / 10.0  # æ€»ä½“å‡å€¼ï¼ˆmsï¼‰
        sigma = std_0_1ms / 10.0  # æ€»ä½“æ ‡å‡†å·®ï¼ˆmsï¼‰
        
        # è®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼šç»å¯¹å»¶æ—¶å‡å»å¹³å‡å»¶æ—¶
        delays_array = np.array(delays_ms)
        relative_delays = (delays_array - mu).tolist()
        
        # è®¡ç®—ç›¸å¯¹å»¶æ—¶çš„ç»Ÿè®¡å€¼ï¼ˆç”¨äºé˜ˆå€¼ï¼‰
        if len(relative_delays) > 1:
            relative_mu = np.mean(relative_delays)  # åº”è¯¥æ¥è¿‘0
            relative_sigma = np.std(relative_delays, ddof=1)  # æ ·æœ¬æ ‡å‡†å·®
            upper_threshold = relative_mu + 3 * relative_sigma
            lower_threshold = relative_mu - 3 * relative_sigma
        else:
            relative_mu = 0.0
            relative_sigma = 0.0
            upper_threshold = 0.0
            lower_threshold = 0.0
        
        statistics = {
            'mu': mu,
            'sigma': sigma,
            'relative_mu': relative_mu,
            'relative_sigma': relative_sigma,
            'upper_threshold': upper_threshold,
            'lower_threshold': lower_threshold
        }
        
        return relative_delays, statistics
    
    def _add_hammer_velocity_scatter_trace(self, fig: go.Figure, log_velocities: List[float],
                                          relative_delays: List[float], delays_ms: List[float],
                                          hammer_velocities: List[float], scatter_customdata: List,
                                          descriptive_name: str, color: str):
        """
        æ·»åŠ é”¤é€Ÿæ•£ç‚¹å›¾trace
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            log_velocities: å¯¹æ•°é”¤é€Ÿåˆ—è¡¨
            relative_delays: ç›¸å¯¹å»¶æ—¶åˆ—è¡¨
            delays_ms: ç»å¯¹å»¶æ—¶åˆ—è¡¨
            hammer_velocities: é”¤é€Ÿåˆ—è¡¨
            scatter_customdata: è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨
            descriptive_name: ç®—æ³•æè¿°åç§°
            color: é¢œè‰²
        """
        # ç»„åˆcustomdata: [delay_ms, original_velocity, record_idx, replay_idx, algorithm_name, key_id]
        combined_customdata = [[delay_ms, orig_vel, record_idx, replay_idx, alg_name, key_id]
                              for delay_ms, orig_vel, (record_idx, replay_idx, alg_name, key_id)
                              in zip(delays_ms, hammer_velocities, scatter_customdata)]
        
        fig.add_trace(go.Scattergl(
            x=log_velocities,
            y=relative_delays,
            mode='markers',
            name=f"{descriptive_name} - ç›¸å¯¹å»¶æ—¶",
            marker=dict(
                size=8,
                color=color,
                opacity=0.6,
                line=dict(width=1, color=color)
            ),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f"ç®—æ³•: {descriptive_name}<br>æŒ‰é”®: %{{customdata[5]}}<br>"
                         f"é”¤é€Ÿ: %{{customdata[1]:.0f}} (log: %{{x:.2f}})<br>"
                         f"ç›¸å¯¹å»¶æ—¶: %{{y:.2f}}ms<br>ç»å¯¹å»¶æ—¶: %{{customdata[0]:.2f}}ms<extra></extra>",
            customdata=combined_customdata
        ))
    
    def _calculate_log_velocity_range(self, ready_algorithms: List) -> Tuple[float, float]:
        """
        è®¡ç®—æ‰€æœ‰ç®—æ³•çš„å¯¹æ•°é”¤é€ŸèŒƒå›´
        
        Args:
            ready_algorithms: å‡†å¤‡å¥½çš„ç®—æ³•åˆ—è¡¨
            
        Returns:
            Tuple[float, float]: (x_min, x_max)
        """
        all_log_velocities = []
        
        for alg in ready_algorithms:
            try:
                matched_pairs = alg.analyzer.note_matcher.get_matched_pairs()
                for record_idx, replay_idx, record_note, replay_note in matched_pairs:
                    if len(replay_note.hammers) > 0 and len(replay_note.hammers.values) > 0:
                        vel = replay_note.hammers.values[0]
                        if vel > 0:
                            all_log_velocities.append(math.log10(vel))
            except:
                continue
        
        x_min = min(all_log_velocities) if all_log_velocities else 0
        x_max = max(all_log_velocities) if all_log_velocities else 2
        
        return x_min, x_max
    
    def _add_relative_delay_threshold_lines(self, fig: go.Figure, x_min: float, x_max: float,
                                           statistics: Dict, descriptive_name: str, color: str):
        """
        æ·»åŠ ç›¸å¯¹å»¶æ—¶é˜ˆå€¼çº¿
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            x_min: Xè½´æœ€å°å€¼
            x_max: Xè½´æœ€å¤§å€¼
            statistics: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
            descriptive_name: ç®—æ³•æè¿°åç§°
            color: é¢œè‰²
        """
        relative_mu = statistics['relative_mu']
        upper_threshold = statistics['upper_threshold']
        lower_threshold = statistics['lower_threshold']
        
        # æ·»åŠ å¹³å‡å€¼å‚è€ƒçº¿ï¼ˆ0çº¿ï¼‰
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[relative_mu, relative_mu],
            mode='lines',
            name=f'{descriptive_name} - å¹³å‡å€¼',
            line=dict(color=color, width=1.5, dash='dot'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶å¹³å‡å€¼ = {relative_mu:.2f}ms<extra></extra>"
        ))
        
        # æ·»åŠ ä¸Šé˜ˆå€¼çº¿
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[upper_threshold, upper_threshold],
            mode='lines',
            name=f'{descriptive_name} - ä¸Šé˜ˆå€¼',
            line=dict(color=color, width=2, dash='dash'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶ä¸Šé˜ˆå€¼ = {upper_threshold:.2f}ms<extra></extra>"
        ))
        
        # æ·»åŠ ä¸‹é˜ˆå€¼çº¿
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[lower_threshold, lower_threshold],
            mode='lines',
            name=f'{descriptive_name} - ä¸‹é˜ˆå€¼',
            line=dict(color=color, width=2, dash='dash'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f"ç®—æ³•: {descriptive_name}<br>ç›¸å¯¹å»¶æ—¶ä¸‹é˜ˆå€¼ = {lower_threshold:.2f}ms<extra></extra>"
        ))
    
    def _configure_hammer_velocity_layout(self, fig: go.Figure):
        """é…ç½®é”¤é€Ÿä¸ç›¸å¯¹å»¶æ—¶å›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='logâ‚â‚€(é”¤é€Ÿ)',
            yaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
            xaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            hovermode='closest',
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=800,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=90, b=60, l=60, r=60)
        )
    
    def _add_hammer_velocity_zscore_scatter_trace(self, fig: go.Figure, log_velocities: List[float],
                                                  z_scores: List[float], delays_ms: List[float],
                                                  hammer_velocities: List[float], scatter_customdata: List,
                                                  descriptive_name: str, color: str):
        """
        æ·»åŠ é”¤é€ŸZ-Scoreæ•£ç‚¹å›¾trace
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            log_velocities: å¯¹æ•°é”¤é€Ÿåˆ—è¡¨
            z_scores: Z-Scoreåˆ—è¡¨
            delays_ms: ç»å¯¹å»¶æ—¶åˆ—è¡¨
            hammer_velocities: é”¤é€Ÿåˆ—è¡¨
            scatter_customdata: è‡ªå®šä¹‰æ•°æ®åˆ—è¡¨
            descriptive_name: ç®—æ³•æè¿°åç§°
            color: é¢œè‰²
        """
        # ç»„åˆcustomdata: [delay_ms, original_velocity, record_idx, replay_idx, algorithm_name, key_id]
        combined_customdata = [[delay_ms, orig_vel, record_idx, replay_idx, alg_name, key_id]
                              for delay_ms, orig_vel, (record_idx, replay_idx, alg_name, key_id)
                              in zip(delays_ms, hammer_velocities, scatter_customdata)]
        
        fig.add_trace(go.Scattergl(
            x=log_velocities,
            y=z_scores,
            mode='markers',
            name=f'{descriptive_name} - Z-Score',
            marker=dict(
                size=8,
                color=color,
                opacity=0.6,
                line=dict(width=1, color=color)
            ),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f'ç®—æ³•: {descriptive_name}<br>æŒ‰é”®: %{{customdata[5]}}<br>'
                         f'é”¤é€Ÿ: %{{customdata[1]:.0f}} (log: %{{x:.2f}})<br>'
                         f'å»¶æ—¶: %{{customdata[0]:.2f}}ms<br>Z-Score: %{{y:.2f}}<extra></extra>',
            customdata=combined_customdata
        ))
    
    def _add_zscore_threshold_lines_for_velocity(self, fig: go.Figure, x_min: float, x_max: float,
                                                 descriptive_name: str, color: str):
        """
        ä¸ºé”¤é€Ÿå›¾æ·»åŠ Z-Scoreé˜ˆå€¼çº¿
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            x_min: Xè½´æœ€å°å€¼
            x_max: Xè½´æœ€å¤§å€¼
            descriptive_name: ç®—æ³•æè¿°åç§°
            color: é¢œè‰²
        """
        # æ·»åŠ Z=0çš„æ°´å¹³è™šçº¿ï¼ˆå‡å€¼çº¿ï¼‰
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[0, 0],
            mode='lines',
            name=f'{descriptive_name} - Z=0',
            line=dict(color=color, width=1.5, dash='dot'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f'ç®—æ³•: {descriptive_name}<br>Z-Score = 0 (å‡å€¼çº¿)<extra></extra>'
        ))
        
        # æ·»åŠ Z=+3çš„æ°´å¹³è™šçº¿ï¼ˆä¸Šé˜ˆå€¼ï¼‰
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[3, 3],
            mode='lines',
            name=f'{descriptive_name} - Z=+3',
            line=dict(color=color, width=2, dash='dash'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f'ç®—æ³•: {descriptive_name}<br>Z-Score = +3 (ä¸Šé˜ˆå€¼)<extra></extra>'
        ))
        
        # æ·»åŠ Z=-3çš„æ°´å¹³è™šçº¿ï¼ˆä¸‹é˜ˆå€¼ï¼‰
        fig.add_trace(go.Scattergl(
            x=[x_min, x_max],
            y=[-3, -3],
            mode='lines',
            name=f'{descriptive_name} - Z=-3',
            line=dict(color=color, width=2, dash='dash'),
            legendgroup=descriptive_name,
            showlegend=True,
            hovertemplate=f'ç®—æ³•: {descriptive_name}<br>Z-Score = -3 (ä¸‹é˜ˆå€¼)<extra></extra>'
        ))
    
    def _configure_hammer_velocity_zscore_layout(self, fig: go.Figure):
        """é…ç½®é”¤é€ŸZ-Scoreå›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='é”¤é€Ÿï¼ˆlogâ‚â‚€ï¼‰',
            yaxis_title='Z-Scoreï¼ˆæ ‡å‡†åŒ–å»¶æ—¶ï¼‰',
            xaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1,
                autorange=True,
                tickformat='.1f',
                dtick=0.2
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1,
                range=[-5, 5],
                dtick=1,
                tickformat='.1f'
            ),
            hovermode='closest',
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=500,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=70, b=60, l=60, r=60)
        )
    
    def _extract_key_hammer_velocity_data(self, matched_pairs: List[Tuple], 
                                          offset_map: Dict) -> Tuple[List[int], List[float], List[float]]:
        """
        æå–æŒ‰é”®IDã€é”¤é€Ÿå’Œå»¶æ—¶æ•°æ®
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨
            offset_map: åç§»æ•°æ®æ˜ å°„
            
        Returns:
            Tuple[List[int], List[float], List[float]]: (æŒ‰é”®IDåˆ—è¡¨, é”¤é€Ÿåˆ—è¡¨, å»¶æ—¶åˆ—è¡¨)
        """
        key_ids = []
        hammer_velocities = []
        delays_ms = []
        
        for record_idx, replay_idx, record_note, replay_note in matched_pairs:
            key_id = record_note.id
            
            # è·å–é”¤é€Ÿ
            if len(replay_note.hammers) > 0 and len(replay_note.hammers.values) > 0:
                hammer_velocity = replay_note.hammers.values[0]
            else:
                continue
            
            # è·å–å»¶æ—¶
            if (record_idx, replay_idx) not in offset_map:
                continue
            
            keyon_offset = offset_map[(record_idx, replay_idx)].get('keyon_offset', 0)
            delay_ms = abs(keyon_offset) / 10.0
            
            try:
                key_id_int = int(key_id)
                key_ids.append(key_id_int)
                hammer_velocities.append(hammer_velocity)
                delays_ms.append(delay_ms)
            except (ValueError, TypeError):
                continue
        
        return key_ids, hammer_velocities, delays_ms
    
    def _add_key_hammer_velocity_scatter_trace(self, fig: go.Figure, key_ids: List[int],
                                               hammer_velocities: List[float], delays_ms: List[float],
                                               descriptive_name: str, algorithm_name: str,
                                               alg_idx: int, num_algorithms: int, all_delays: List[float]):
        """
        æ·»åŠ æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾trace
        
        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            key_ids: æŒ‰é”®IDåˆ—è¡¨
            hammer_velocities: é”¤é€Ÿåˆ—è¡¨
            delays_ms: å»¶æ—¶åˆ—è¡¨
            descriptive_name: ç®—æ³•æè¿°åç§°
            algorithm_name: ç®—æ³•åç§°
            alg_idx: ç®—æ³•ç´¢å¼•
            num_algorithms: ç®—æ³•æ€»æ•°
            all_delays: æ‰€æœ‰å»¶æ—¶åˆ—è¡¨ï¼ˆç”¨äºé¢œè‰²èŒƒå›´ï¼‰
        """
        marker_symbols = ['circle', 'square', 'diamond', 'triangle-up', 'x', 'star', 'cross', 'pentagon']
        colorscales = ['Viridis', 'Plasma', 'Inferno', 'Magma', 'Cividis', 'Turbo', 'Blues', 'Reds']
        
        marker_symbol = marker_symbols[alg_idx % len(marker_symbols)]
        colorscale = colorscales[alg_idx % len(colorscales)]
        
        fig.add_trace(go.Scattergl(
            x=key_ids,
            y=hammer_velocities,
            mode='markers',
            name=f'{descriptive_name}',
            marker=dict(
                size=8,
                color=delays_ms,
                colorscale=colorscale,
                colorbar=dict(
                    title=f'{descriptive_name}<br>å»¶æ—¶ (ms)',
                    thickness=15,
                    len=0.3,
                    x=1.02 + (alg_idx * 0.08),
                    y=0.5 - (alg_idx * 0.3 / num_algorithms)
                ),
                cmin=min(all_delays) if all_delays else 0,
                cmax=max(all_delays) if all_delays else 100,
                symbol=marker_symbol,
                line=dict(width=1, color='rgba(0,0,0,0.3)')
            ),
            legendgroup=algorithm_name,
            showlegend=True,
            hovertemplate=f'ç®—æ³•: {algorithm_name}<br>é”®ä½: %{{x}}<br>é”¤é€Ÿ: %{{y}}<br>å»¶æ—¶: %{{marker.color:.2f}}ms<extra></extra>'
        ))
    
    def _configure_key_hammer_velocity_layout(self, fig: go.Figure):
        """é…ç½®æŒ‰é”®ä¸é”¤é€Ÿæ•£ç‚¹å›¾å¸ƒå±€"""
        fig.update_layout(
            xaxis_title='æŒ‰é”®ID',
            yaxis_title='é”¤é€Ÿ',
            xaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1,
                dtick=10
            ),
            yaxis=dict(
                showgrid=True,
                gridcolor='lightgray',
                gridwidth=1
            ),
            hovermode='closest',
            plot_bgcolor='white',
            paper_bgcolor='white',
            font=dict(size=12),
            height=500,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=70, b=60, l=60, r=200)
        )
    
    
    def _should_generate_time_series_plot(self, algorithms: List[AlgorithmDataset]) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆå»¶æ—¶æ—¶é—´åºåˆ—å›¾

        æ¡ä»¶ï¼š
        1. è‡³å°‘æœ‰2ä¸ªç®—æ³•

        Args:
            algorithms: ç®—æ³•æ•°æ®é›†åˆ—è¡¨

        Returns:
            Tuple[bool, str]: (æ˜¯å¦åº”è¯¥ç”Ÿæˆ, åŸå› è¯´æ˜)
        """
        if not algorithms:
            return False, "æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•"

        # åªè¦æœ‰è‡³å°‘1ä¸ªç®—æ³•å°±å¯ä»¥ç”Ÿæˆå›¾è¡¨
        return True, ""
    
    def _filter_ready_algorithms(self, algorithms: List[AlgorithmDataset]) -> List[AlgorithmDataset]:
        """
        è¿‡æ»¤å‡ºå°±ç»ªçš„ç®—æ³•

        Args:
            algorithms: åŸå§‹ç®—æ³•åˆ—è¡¨

        Returns:
            List[AlgorithmDataset]: å°±ç»ªçš„ç®—æ³•åˆ—è¡¨
        """
        return [alg for alg in algorithms if alg.is_ready()]

    def _prepare_algorithm_colors(self) -> List[str]:
        """
        å‡†å¤‡ç®—æ³•é¢œè‰²åˆ—è¡¨

        Returns:
            List[str]: é¢œè‰²åˆ—è¡¨
        """
        return ALGORITHM_COLOR_PALETTE

    def _process_single_algorithm_data(self, algorithm: AlgorithmDataset) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªç®—æ³•çš„æ—¶é—´åºåˆ—æ•°æ®

        Args:
            algorithm: ç®—æ³•æ•°æ®é›†

        Returns:
            Optional[Dict[str, Any]]: å¤„ç†åçš„æ•°æ®ï¼ŒåŒ…å«æ—¶é—´ã€å»¶æ—¶ç­‰ä¿¡æ¯ï¼Œå¦‚æœå¤„ç†å¤±è´¥è¿”å›None
        """
        algorithm_name = algorithm.metadata.algorithm_name
        display_name = algorithm.metadata.display_name

        if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
            logger.warning(f"âš ï¸ ç®—æ³• '{display_name}' æ²¡æœ‰åˆ†æå™¨æˆ–åŒ¹é…å™¨ï¼Œè·³è¿‡")
            return None

        try:
            offset_data = algorithm.analyzer.note_matcher.get_precision_offset_alignment_data()

            if not offset_data:
                logger.warning(f"âš ï¸ ç®—æ³• '{display_name}' æ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œè·³è¿‡")
                return None

            # æå–æ—¶é—´å’Œå»¶æ—¶æ•°æ®
            data_points = []

            for item in offset_data:
                record_keyon_raw = item.get('record_keyon')  # å•ä½ï¼š0.1ms
                keyon_offset_raw = item.get('keyon_offset')  # å•ä½ï¼š0.1ms
                key_id = item.get('key_id')
                record_index = item.get('record_index')
                replay_index = item.get('replay_index')

                # æ£€æŸ¥æ•°æ®ç±»å‹æœ‰æ•ˆæ€§ï¼ˆæ”¯æŒ numpy ç±»å‹ï¼‰
                record_keyon_is_valid = isinstance(record_keyon_raw, (int, float, np.integer, np.floating))
                keyon_offset_is_valid = isinstance(keyon_offset_raw, (int, float, np.integer, np.floating))


                if not record_keyon_is_valid:
                    continue
                if not keyon_offset_is_valid:
                    continue

                # ä½¿ç”¨åŸå§‹æ•°æ®
                record_keyon = record_keyon_raw
                keyon_offset = keyon_offset_raw

                # è½¬æ¢ä¸ºmså•ä½
                time_ms = record_keyon / 10.0
                delay_ms = keyon_offset / 10.0

                data_points.append({
                    'time': time_ms,
                    'delay': delay_ms,
                    'key_id': key_id if key_id is not None else 'N/A',
                    'record_index': record_index,
                    'replay_index': replay_index
                })

            if not data_points:
                logger.warning(f"ç®—æ³• '{display_name}' æ²¡æœ‰æœ‰æ•ˆæ—¶é—´åºåˆ—æ•°æ®ï¼Œè·³è¿‡")
                return None

            # æŒ‰æ—¶é—´æ’åºï¼Œç¡®ä¿æŒ‰æ—¶é—´é¡ºåºæ˜¾ç¤º
            data_points.sort(key=lambda x: x['time'])

            # è®¡ç®—è¯¥ç®—æ³•çš„å¹³å‡å»¶æ—¶
            me_0_1ms = algorithm.analyzer.get_mean_error() if hasattr(algorithm.analyzer, 'get_mean_error') else 0.0
            mean_delay = me_0_1ms / 10.0  # å¹³å‡å»¶æ—¶ï¼ˆmsï¼Œå¸¦ç¬¦å·ï¼‰

            # è®¡ç®—ç›¸å¯¹å»¶æ—¶
            relative_delays_ms = []
            for point in data_points:
                delay_ms = point['delay']
                relative_delay = delay_ms - mean_delay
                relative_delays_ms.append(relative_delay)

            # æå–æ’åºåçš„æ•°æ®
            times_ms = [point['time'] for point in data_points]
            delays_ms = [point['delay'] for point in data_points]
            replay_times_ms = [point['time'] + point['delay'] for point in data_points]
            replay_times_offset_ms = [replay_time - mean_delay for replay_time in replay_times_ms]

            # customdata åŒ…å« [key_id, record_index, replay_index, algorithm_name, åŸå§‹å»¶æ—¶, å¹³å‡å»¶æ—¶, æ’­æ”¾æ—¶é—´, å½•åˆ¶æ—¶é—´]
            customdata_list = [[point['key_id'], point['record_index'], point['replay_index'],
                               algorithm_name, point['delay'], mean_delay, replay_time, point['time']]
                              for point, replay_time in zip(data_points, replay_times_ms)]

            return {
                'algorithm_name': algorithm_name,
                'display_name': display_name,
                'data_points': data_points,
                'times_ms': times_ms,
                'delays_ms': delays_ms,
                'relative_delays_ms': relative_delays_ms,
                'replay_times_ms': replay_times_ms,
                'replay_times_offset_ms': replay_times_offset_ms,
                'customdata_list': customdata_list,
                'mean_delay': mean_delay
            }

        except Exception as e:
            logger.warning(f"è·å–ç®—æ³• '{display_name}' çš„æ—¶é—´åºåˆ—æ•°æ®å¤±è´¥: {e}")
            return None

    def _create_relative_delay_traces(self, fig, algorithm_data: Dict[str, Any], color: str) -> None:
        """
        ä¸ºç›¸å¯¹å»¶æ—¶å›¾åˆ›å»ºtrace

        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            algorithm_data: ç®—æ³•æ•°æ®
            color: ç®—æ³•é¢œè‰²
        """
        algorithm_name = algorithm_data['algorithm_name']
        display_name = algorithm_data['display_name']
        replay_times_ms = algorithm_data['replay_times_ms']
        replay_times_offset_ms = algorithm_data['replay_times_offset_ms']
        relative_delays_ms = algorithm_data['relative_delays_ms']
        customdata_list = algorithm_data['customdata_list']
        mean_delay = algorithm_data['mean_delay']

        # æ·»åŠ åç§»åçš„æ’­æ”¾éŸ³è½¨æ•£ç‚¹å›¾ï¼ˆXè½´=åç§»åçš„æ’­æ”¾æ—¶é—´ï¼ŒYè½´=ç›¸å¯¹å»¶æ—¶ï¼‰
        fig.add_trace(go.Scattergl(
            x=replay_times_offset_ms,  # Xè½´ä½¿ç”¨åç§»åçš„æ’­æ”¾æ—¶é—´ï¼ˆæ’­æ”¾æ—¶é—´ - å¹³å‡å»¶æ—¶ï¼‰
            y=relative_delays_ms,  # Yè½´ä½¿ç”¨ç›¸å¯¹å»¶æ—¶
            mode='markers+lines',  # æ˜¾ç¤ºæ•°æ®ç‚¹å¹¶æŒ‰æ—¶é—´é¡ºåºè¿æ¥
            name=f'{display_name} (åç§»åï¼Œå¹³å‡å»¶æ—¶: {mean_delay:.2f}ms)',
            marker=dict(
                size=5,
                color=color,
                line=dict(width=0.5, color=color)
            ),
            line=dict(color=color, width=1.5),
            legendgroup=f"{algorithm_name}_offset",
            showlegend=True,
            hovertemplate='<b>ç®—æ³•</b>: ' + display_name + ' (åç§»å)<br>' +
                         '<b>åç§»åæ’­æ”¾æ—¶é—´ï¼ˆXè½´ï¼‰</b>: %{x:.2f}ms<br>' +
                         '<b>ç›¸å¯¹å»¶æ—¶ï¼ˆYè½´ï¼‰</b>: %{y:.2f}ms<br>' +
                         '<b>å®é™…æ’­æ”¾æ—¶é—´</b>: %{customdata[6]:.2f}ms<br>' +
                         '<b>å½•åˆ¶æ—¶é—´</b>: %{customdata[7]:.2f}ms<br>' +
                         '<b>åŸå§‹å»¶æ—¶</b>: %{customdata[4]:.2f}ms<br>' +
                         '<b>å¹³å‡å»¶æ—¶</b>: %{customdata[5]:.2f}ms<br>' +
                         '<b>æŒ‰é”®ID</b>: %{customdata[0]}<br>' +
                         '<extra></extra>',
            customdata=customdata_list
        ))

    def _add_algorithm_reference_lines(self, fig, algorithm_data: Dict[str, Any], color: str) -> None:
        """
        ä¸ºç®—æ³•æ·»åŠ å‚è€ƒçº¿

        Args:
            fig: Plotlyå›¾è¡¨å¯¹è±¡
            algorithm_data: ç®—æ³•æ•°æ®
            color: ç®—æ³•é¢œè‰²
        """
        algorithm_name = algorithm_data['algorithm_name']
        display_name = algorithm_data['display_name']
        delays_ms = algorithm_data['delays_ms']
        replay_times_offset_ms = algorithm_data['replay_times_offset_ms']
        algorithm = algorithm_data.get('algorithm_instance')

        if not delays_ms or len(delays_ms) == 0 or not algorithm or not algorithm.analyzer:
            return

        # è®¡ç®—æ ‡å‡†å·®
        std_0_1ms = algorithm.analyzer.get_standard_deviation() if hasattr(algorithm.analyzer, 'get_standard_deviation') else 0.0
        std_delay = std_0_1ms / 10.0

        # è·å–æ—¶é—´èŒƒå›´
        replay_time_offset_min = min(replay_times_offset_ms) if replay_times_offset_ms else 0
        replay_time_offset_max = max(replay_times_offset_ms) if replay_times_offset_ms else 1

        # æ·»åŠ é›¶çº¿å‚è€ƒçº¿
        fig.add_trace(go.Scattergl(
            x=[replay_time_offset_min, replay_time_offset_max],
            y=[0, 0],
            mode='lines',
            name=f'{display_name} - é›¶çº¿',
            line=dict(dash='dash', color=color, width=1.5),
            hovertemplate=f'<b>{display_name} é›¶çº¿</b>: 0.00ms<extra></extra>',
            showlegend=False,
            legendgroup=algorithm_name
        ))

        # æ·»åŠ Â±3Ïƒå‚è€ƒçº¿
        if std_delay > 0:
            fig.add_trace(go.Scattergl(
                x=[replay_time_offset_min, replay_time_offset_max],
                y=[3 * std_delay, 3 * std_delay],
                mode='lines',
                name=f'{display_name} - +3Ïƒ',
                line=dict(dash='dot', color=color, width=1),
                hovertemplate=f'<b>{display_name} +3Ïƒ</b>: {3 * std_delay:.2f}ms<extra></extra>',
                showlegend=False,
                legendgroup=algorithm_name
            ))
            fig.add_trace(go.Scattergl(
                x=[replay_time_offset_min, replay_time_offset_max],
                y=[-3 * std_delay, -3 * std_delay],
                mode='lines',
                name=f'{display_name} - -3Ïƒ',
                line=dict(dash='dot', color=color, width=1),
                hovertemplate=f'<b>{display_name} -3Ïƒ</b>: {-3 * std_delay:.2f}ms<extra></extra>',
                showlegend=False,
                legendgroup=algorithm_name
            ))

    def _collect_all_relative_delay_data(self, ready_algorithms: List[AlgorithmDataset], colors: List[str], apply_time_offset: bool = False) -> List[Tuple[float, float, List, str, str]]:
        """
        æ”¶é›†æ‰€æœ‰ç®—æ³•çš„ç›¸å¯¹å»¶æ—¶æ•°æ®

        Args:
            ready_algorithms: å°±ç»ªçš„ç®—æ³•åˆ—è¡¨
            colors: é¢œè‰²åˆ—è¡¨
            apply_time_offset: æ˜¯å¦åº”ç”¨æ—¶é—´è½´åç§»ï¼ˆå‡å»å¹³å‡å»¶æ—¶ï¼‰

        Returns:
            List[Tuple[float, float, List, str, str]]: ç›¸å¯¹æ•°æ®åˆ—è¡¨ (time_ms, relative_delay_ms, customdata, descriptive_name, color)
        """
        all_relative_data = []

        for alg_idx, algorithm in enumerate(ready_algorithms):
            algorithm_name = algorithm.metadata.algorithm_name
            display_name = algorithm.metadata.display_name
            filename = algorithm.metadata.filename
            descriptive_name = f"{display_name} ({filename})"
            color = colors[alg_idx % len(colors)]

            if not algorithm.analyzer or not algorithm.analyzer.note_matcher:
                continue

            try:
                offset_data = algorithm.analyzer.get_precision_offset_alignment_data()

                if not offset_data:
                    continue

                # è®¡ç®—è¯¥ç®—æ³•çš„å¹³å‡å»¶æ—¶ï¼ˆç”¨äºç›¸å¯¹å»¶æ—¶è®¡ç®—ï¼‰
                me_0_1ms = algorithm.analyzer.note_matcher.get_mean_error()
                mean_delay = me_0_1ms / 10.0  # å¹³å‡å»¶æ—¶ï¼ˆmsï¼‰

                # æå–æ’­æ”¾éŸ³è½¨æ•°æ®
                for item in offset_data:
                    record_keyon_raw = item.get('record_keyon')
                    replay_keyon_raw = item.get('replay_keyon')  # æ’­æ”¾æ—¶é—´
                    key_id = item.get('key_id')
                    record_index = item.get('record_index')
                    replay_index = item.get('replay_index')
                    record_velocity = item.get('record_velocity')
                    replay_velocity = item.get('replay_velocity')
                    velocity_diff = item.get('velocity_diff')
                    relative_delay = item.get('relative_delay', 0)

                    # ç±»å‹æ£€æŸ¥
                    record_keyon_is_valid = isinstance(record_keyon_raw, (int, float, np.integer, np.floating))
                    replay_keyon_is_valid = isinstance(replay_keyon_raw, (int, float, np.integer, np.floating))

                    if not record_keyon_is_valid or not replay_keyon_is_valid:
                        continue

                    # è½¬æ¢ä¸ºmså•ä½
                    time_ms = record_keyon_raw / 10.0  # Xè½´ï¼šå½•åˆ¶æ—¶é—´
                    replay_time_ms = replay_keyon_raw / 10.0  # æ’­æ”¾æ—¶é—´

                    # è®¡ç®—ç›¸å¯¹å»¶æ—¶ï¼ˆç»å¯¹å»¶æ—¶ - å¹³å‡å»¶æ—¶ï¼‰
                    relative_delay_ms = (replay_time_ms - time_ms) - mean_delay

                    # å¦‚æœéœ€è¦æ—¶é—´è½´åç§»ï¼Œä½¿ç”¨åç§»åçš„æ’­æ”¾æ—¶é—´è½´
                    if apply_time_offset:
                        time_ms = replay_time_ms - mean_delay  # æ’­æ”¾æ—¶é—´ - å¹³å‡å»¶æ—¶
                        y_value = relative_delay_ms  # Yè½´ï¼šç›¸å¯¹å»¶æ—¶
                    else:
                        time_ms = replay_time_ms  # Xè½´ï¼šæ’­æ”¾æ—¶é—´
                        y_value = relative_delay_ms  # Yè½´ï¼šç›¸å¯¹å»¶æ—¶

                    # å­˜å‚¨åŸå§‹æ—¶é—´å€¼ï¼ˆåœ¨ä¿®æ”¹time_msä¹‹å‰ï¼‰
                    original_record_time = record_keyon_raw / 10.0
                    customdata = [key_id, record_index, replay_index, algorithm_name, replay_time_ms - original_record_time, relative_delay, mean_delay, record_velocity, replay_velocity, velocity_diff, replay_time_ms, original_record_time]

                    all_relative_data.append((time_ms, y_value, customdata, descriptive_name, color))

            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†ç®—æ³• '{algorithm_name}' çš„ç›¸å¯¹å»¶æ—¶æ•°æ®å¤±è´¥: {e}")

        return all_relative_data

    def _create_raw_delay_plot_for_algorithms(self, all_raw_data: List[Tuple[float, float, List, str, str]]) -> Any:
        """
        ä¸ºå¤šç®—æ³•åˆ›å»ºåŸå§‹å»¶æ—¶å›¾è¡¨

        Args:
            all_raw_data: æ‰€æœ‰åŸå§‹å»¶æ—¶æ•°æ®

        Returns:
            Any: Plotlyå›¾è¡¨å¯¹è±¡
        """
        

        raw_delay_fig = go.Figure()

        # æŒ‰ç®—æ³•åˆ†ç»„æ•°æ®
        algorithm_data = {}
        for time_ms, delay_ms, customdata, descriptive_name, color in all_raw_data:
            if descriptive_name not in algorithm_data:
                algorithm_data[descriptive_name] = {
                    'times': [], 'delays': [], 'customdata': [], 'color': color
                }
            algorithm_data[descriptive_name]['times'].append(time_ms)
            algorithm_data[descriptive_name]['delays'].append(delay_ms)
            algorithm_data[descriptive_name]['customdata'].append(customdata)

        # æ·»åŠ æ¯ä¸ªç®—æ³•çš„trace
        for descriptive_name, data in algorithm_data.items():
            if data['times'] and data['delays']:
                # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                sorted_indices = sorted(range(len(data['times'])), key=lambda i: data['times'][i])
                sorted_times = [data['times'][i] for i in sorted_indices]
                sorted_delays = [data['delays'][i] for i in sorted_indices]
                sorted_customdata = [data['customdata'][i] for i in sorted_indices]

                raw_delay_fig.add_trace(go.Scattergl(
                    x=sorted_times,
                    y=sorted_delays,
                    mode='markers+lines',
                    name=f'{descriptive_name} (ç›¸å¯¹å»¶æ—¶)',
                    marker=dict(
                        size=4,
                        color=data['color'],
                        symbol='circle'  # å®å¿ƒåœ†ç‚¹
                    ),
                    line=dict(color=data['color'], width=1, dash='dot'),
                    hovertemplate='<b>ç®—æ³•</b>: ' + descriptive_name + '<br>' +
                                 '<b>æ’­æ”¾æ—¶é—´</b>: %{x:.2f}ms<br>' +
                                 '<b>å½•åˆ¶æ—¶é—´</b>: %{customdata[11]:.2f}ms<br>' +
                                 '<b>ç›¸å¯¹å»¶æ—¶</b>: %{y:.2f}ms<br>' +
                                 '<b>å¹³å‡å»¶æ—¶</b>: %{customdata[6]:.2f}ms<br>' +
                                 '<b>å½•åˆ¶é”¤é€Ÿ</b>: %{customdata[7]}<br>' +
                                 '<b>æ’­æ”¾é”¤é€Ÿ</b>: %{customdata[8]}<br>' +
                                 '<b>é”¤é€Ÿå·®å€¼</b>: %{customdata[9]}<br>' +
                                 '<b>æŒ‰é”®ID</b>: %{customdata[0]}<br>' +
                                 '<extra></extra>',
                    customdata=sorted_customdata
                ))

        # é…ç½®å¸ƒå±€
        raw_delay_fig.update_layout(
            xaxis_title='æ’­æ”¾æ—¶é—´ (ms)',
            yaxis_title='ç›¸å¯¹å»¶æ—¶ (ms)',
            showlegend=True,
            template='plotly_white',
            height=400,
            hovermode='closest',
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            margin=dict(t=100, b=60, l=60, r=60)
        )

        return raw_delay_fig

    def _process_all_algorithms_data(self, ready_algorithms: List[AlgorithmDataset], colors: List[str]) -> Tuple[Any, List[Tuple[Dict[str, Any], str]]]:
        """
        å¤„ç†æ‰€æœ‰ç®—æ³•çš„æ•°æ®å¹¶åˆ›å»ºç›¸å¯¹å»¶æ—¶å›¾çš„traces

        Args:
            ready_algorithms: å°±ç»ªçš„ç®—æ³•åˆ—è¡¨
            colors: é¢œè‰²åˆ—è¡¨

        Returns:
            Tuple[Any, List[Tuple[Dict[str, Any], str]]]: (å›¾è¡¨å¯¹è±¡, ç®—æ³•ç»“æœåˆ—è¡¨)
        """
        
        fig = go.Figure()

        all_delays = []
        algorithm_results = []

        for alg_idx, algorithm in enumerate(ready_algorithms):
            logger.debug(f"[DEBUG] å¤„ç†ç®—æ³• {alg_idx}: {algorithm.metadata.display_name}")
            algorithm_data = self._process_single_algorithm_data(algorithm)
            if algorithm_data is None:
                logger.warning(f"[warning] ç®—æ³• {algorithm.metadata.display_name} è¿”å›Noneï¼Œè·³è¿‡")
                continue

            logger.debug(f"[DEBUG] ç®—æ³• {algorithm.metadata.display_name} è¿”å›æ•°æ®: relative_delays_msé•¿åº¦={len(algorithm_data.get('relative_delays_ms', []))}")

            # æ·»åŠ ç®—æ³•å®ä¾‹å¼•ç”¨ï¼ˆç”¨äºåç»­å‚è€ƒçº¿è®¡ç®—ï¼‰
            algorithm_data['algorithm_instance'] = algorithm

            color = colors[alg_idx % len(colors)]

            # åˆ›å»ºç›¸å¯¹å»¶æ—¶å›¾çš„trace
            logger.debug(f"[DEBUG] ä¸ºç®—æ³• {algorithm.metadata.display_name} åˆ›å»ºtraces")
            self._create_relative_delay_traces(fig, algorithm_data, color)

            # æ·»åŠ å‚è€ƒçº¿
            self._add_algorithm_reference_lines(fig, algorithm_data, color)

            # æ”¶é›†æ•°æ®ç”¨äºç»Ÿè®¡
            relative_delays = algorithm_data.get('relative_delays_ms', [])
            all_delays.extend(relative_delays)
            algorithm_results.append((algorithm_data, color))

            logger.debug(f"[DEBUG] ç®—æ³• {algorithm.metadata.display_name} å¤„ç†å®Œæˆï¼Œæ·»åŠ äº† {len(relative_delays)} ä¸ªæ•°æ®ç‚¹")

        return fig, algorithm_results

    def _create_multi_algorithm_relative_plot(self, ready_algorithms: List[AlgorithmDataset], colors: List[str], apply_time_offset: bool = False) -> Any:
        """
        åˆ›å»ºå¤šç®—æ³•ç›¸å¯¹å»¶æ—¶å›¾

        Args:
            ready_algorithms: å°±ç»ªçš„ç®—æ³•åˆ—è¡¨
            colors: é¢œè‰²åˆ—è¡¨
            apply_time_offset: æ˜¯å¦åº”ç”¨æ—¶é—´è½´åç§»

        Returns:
            Any: ç›¸å¯¹å»¶æ—¶å›¾è¡¨å¯¹è±¡
        """
        all_relative_data = self._collect_all_relative_delay_data(ready_algorithms, colors, apply_time_offset)
        return self._create_raw_delay_plot_for_algorithms(all_relative_data)

    def generate_multi_algorithm_delay_time_series_plot(
        self,
        algorithms: List[AlgorithmDataset]
    ) -> Any:
        """
        ç”Ÿæˆå¤šç®—æ³•å»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼ˆä¸¤å¼ ç›¸å¯¹å»¶æ—¶å›¾ï¼šæ’­æ”¾æ—¶é—´è½´å¯¹æ¯”ï¼‰

        Args:
            algorithms: æ¿€æ´»çš„ç®—æ³•æ•°æ®é›†åˆ—è¡¨

        Returns:
            Dict[str, Any]: åŒ…å«ä¸Šæ–¹ç›¸å¯¹å»¶æ—¶å›¾å’Œä¸‹æ–¹ç›¸å¯¹å»¶æ—¶å›¾çš„å­—å…¸
        """
        if not algorithms:
            return self._create_empty_plot("æ²¡æœ‰æ¿€æ´»çš„ç®—æ³•")

        try:
            # 1. è¿‡æ»¤å°±ç»ªçš„ç®—æ³•
            ready_algorithms = self._filter_ready_algorithms(algorithms)
            if not ready_algorithms:
                logger.warning("æ²¡æœ‰å°±ç»ªçš„ç®—æ³•ï¼Œæ— æ³•ç”Ÿæˆå¤šç®—æ³•å»¶æ—¶æ—¶é—´åºåˆ—å›¾")
                return self._create_empty_plot("æ²¡æœ‰å°±ç»ªçš„ç®—æ³•")

            # 2. æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”Ÿæˆå›¾è¡¨
            should_generate, reason = self._should_generate_time_series_plot(ready_algorithms)
            if not should_generate:
                logger.warning(f"è·³è¿‡å»¶æ—¶æ—¶é—´åºåˆ—å›¾ç”Ÿæˆ: {reason}")
                return self._create_empty_plot(reason)

            logger.debug(f"å¼€å§‹ç”Ÿæˆå¤šç®—æ³•å»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼Œå…± {len(ready_algorithms)} ä¸ªç®—æ³•")

            # 3. å‡†å¤‡é¢œè‰²
            colors = self._prepare_algorithm_colors()

            # 4. å¤„ç†æ‰€æœ‰ç®—æ³•æ•°æ®å¹¶åˆ›å»ºç›¸å¯¹å»¶æ—¶å›¾
            fig, algorithm_results = self._process_all_algorithms_data(ready_algorithms, colors)

            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æ•°æ®ç”¨äºç»˜å›¾
            has_data = any(len(trace.y) > 0 for trace in fig.data) if fig.data else False
            logger.debug(f"[DEBUG] has_dataæ£€æŸ¥: fig.dataå­˜åœ¨={fig.data is not None}, tracesæ•°é‡={len(fig.data) if fig.data else 0}, has_data={has_data}")

            if not has_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
                # è®°å½•æ›´å¤šè°ƒè¯•ä¿¡æ¯
                for i, alg in enumerate(ready_algorithms):
                    logger.warning(f"  ç®—æ³• {i}: {alg.metadata.display_name}")
                return self._create_empty_plot("æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´åºåˆ—æ•°æ®")

            # 5. é…ç½®ç›¸å¯¹å»¶æ—¶å›¾çš„å›¾æ³¨
            fig.update_layout(
                title='ç›¸å¯¹å»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼ˆæ’­æ”¾æ—¶é—´è½´ï¼‰',
                height=500,
                showlegend=True,
                legend=dict(
                    orientation='h',
                    yanchor='bottom',
                    y=1.02,
                    xanchor='left',
                    x=0.0,
                    bgcolor='rgba(255, 255, 255, 0.9)',
                    bordercolor='gray',
                    borderwidth=1,
                    title='ç®—æ³•å›¾ä¾‹'
                ),
                template='plotly_white',
                hovermode='closest'
            )

            # 6. åˆ›å»ºä¸Šæ–¹ç›¸å¯¹å»¶æ—¶å›¾ï¼ˆæ’­æ”¾æ—¶é—´è½´ï¼‰
            raw_delay_plot = self._create_multi_algorithm_relative_plot(ready_algorithms, colors, apply_time_offset=False)
            if raw_delay_plot:
                raw_delay_plot.update_layout(
                    title='ç›¸å¯¹å»¶æ—¶æ—¶é—´åºåˆ—å›¾ï¼ˆæ’­æ”¾æ—¶é—´è½´ï¼‰',
                    height=500,
                    showlegend=True,
                    legend=dict(
                        orientation='h',
                        yanchor='bottom',
                        y=1.02,
                        xanchor='left',
                        x=0.0,
                        bgcolor='rgba(255, 255, 255, 0.9)',
                        bordercolor='gray',
                        borderwidth=1,
                        title='ç®—æ³•å›¾ä¾‹'
                    ),
                    template='plotly_white',
                    hovermode='closest'
                )

            return {
                'raw_delay_plot': raw_delay_plot,
                'relative_delay_plot': fig
            }

        except Exception as e:
            return self._handle_generation_error(e, "å¤šç®—æ³•å»¶æ—¶æ—¶é—´åºåˆ—å›¾", return_dict=True)

    def _update_key_statistics(self, key_stats: Dict, key_id: int, algorithm_name: str, 
                               data_type: str, is_exception: bool = False) -> None:
        """
        æ›´æ–°æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆè¾…åŠ©å‡½æ•°ï¼Œæ¶ˆé™¤å†—ä½™ï¼‰
        
        Args:
            key_stats: æŒ‰é”®ç»Ÿè®¡å­—å…¸
            key_id: æŒ‰é”®ID
            algorithm_name: ç®—æ³•åç§°
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            is_exception: æ˜¯å¦ä¸ºå¼‚å¸¸æ•°æ®
        """
        if key_id not in key_stats:
            key_stats[key_id] = {
                'key_id': key_id,
                'total_count': 0,
                'exception_count': 0,
                'data_types': set(),
                'algorithms': set()
            }
        
        key_stats[key_id]['total_count'] += 1
        key_stats[key_id]['algorithms'].add(algorithm_name)
        key_stats[key_id]['data_types'].add(data_type)
        if is_exception:
            key_stats[key_id]['exception_count'] += 1

    def get_waterfall_key_statistics(self, backend, analyzers: List[Any], algorithm_names: List[str], data_types: List[str] = None) -> Dict[str, Any]:
        """
        è·å–ç€‘å¸ƒå›¾æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ ¹æ®æ•°æ®ç±»å‹ç­›é€‰ï¼‰
        
        ç›´æ¥ä»NoteMatcherçš„æ•°æ®æºç»Ÿè®¡ï¼Œè€Œä¸æ˜¯ä»ç»˜å›¾ç”¨çš„baråˆ—è¡¨ç»Ÿè®¡ã€‚
        ç»Ÿè®¡ç»“æœä¸å¼‚å¸¸æŠ¥å‘Šç•Œé¢ä¿æŒä¸€è‡´ã€‚

        Args:
            backend: åç«¯å®ä¾‹
            analyzers: åˆ†æå™¨åˆ—è¡¨
            algorithm_names: ç®—æ³•åç§°åˆ—è¡¨
            data_types: æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ç»Ÿè®¡æ‰€æœ‰ç±»å‹

        Returns:
            Dict[str, Any]: æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not analyzers:
                return {'available_keys': [], 'summary': {}}

            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®ç±»å‹ï¼Œé»˜è®¤åªç»Ÿè®¡ç²¾ç¡®åŒ¹é…
            if data_types is None:
                data_types = ['matched_pairs']

            # æ”¶é›†æ‰€æœ‰æŒ‰é”®çš„ç»Ÿè®¡ä¿¡æ¯
            key_stats = {}
            from spmid.note_matcher import MatchType

            for analyzer, algorithm_name in zip(analyzers, algorithm_names):
                if not analyzer:
                    continue

                # ç›´æ¥ä»NoteMatcherè·å–æ•°æ®æºï¼Œè€Œä¸æ˜¯ä»ç»˜å›¾ç”¨çš„baråˆ—è¡¨
                # è¿™æ ·å¯ä»¥ç¡®ä¿ç»Ÿè®¡çš„æ˜¯åŒ¹é…å¯¹æ•°é‡ï¼Œè€Œä¸æ˜¯baræ•°é‡ï¼ˆ1ä¸ªåŒ¹é…å¯¹=2ä¸ªbarï¼‰
                note_matcher = getattr(analyzer, 'note_matcher', None)
                if not note_matcher or not hasattr(note_matcher, 'get_all_display_data'):
                    logger.warning(f"ç®—æ³• '{algorithm_name}' æ²¡æœ‰NoteMatcherï¼Œè·³è¿‡ç»Ÿè®¡")
                    continue

                display_data = note_matcher.get_all_display_data()

                # æ ¹æ®é€‰æ‹©çš„æ•°æ®ç±»å‹ç»Ÿè®¡ï¼ˆç»Ÿä¸€å¤„ç†é€»è¾‘ï¼Œé€‚é… MatchResult å¯¹è±¡ï¼‰
                if 'matched_pairs' in data_types:
                    for result in display_data.get('matched_pairs', []):
                        if result.pair and result.pair[0]:
                            is_exception = (result.match_type == MatchType.FAILED)
                            self._update_key_statistics(key_stats, result.pair[0].id, algorithm_name, 'matched_pair', is_exception)

                if 'abnormal_matches' in data_types:
                    for result in display_data.get('abnormal_matches', []):
                        if result.pair and result.pair[0]:
                            self._update_key_statistics(key_stats, result.pair[0].id, algorithm_name, 'abnormal_match', is_exception=True)

                if 'drop_hammers' in data_types:
                    for result in display_data.get('drop_hammers', []):
                        if result.pair and result.pair[0]:
                            self._update_key_statistics(key_stats, result.pair[0].id, algorithm_name, 'drop_hammer', is_exception=True)

                if 'multi_hammers' in data_types:
                    for result in display_data.get('multi_hammers', []):
                        # å¯¹äºå¤šé”¤ï¼Œå½•åˆ¶éŸ³ç¬¦å¯èƒ½ä¸ºç©ºï¼Œä¼˜å…ˆä½¿ç”¨æ’­æ”¾éŸ³ç¬¦ID
                        note = result.pair[0] or result.pair[1]
                        if note:
                            self._update_key_statistics(key_stats, note.id, algorithm_name, 'multi_hammer', is_exception=True)

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è®¡ç®—æ±‡æ€»ä¿¡æ¯
            available_keys = []
            total_data_points = 0
            total_exception_points = 0
            
            for key_id, stats in key_stats.items():
                stats['data_types'] = list(stats['data_types'])
                stats['algorithms'] = list(stats['algorithms'])
                stats['exception_rate'] = stats['exception_count'] / stats['total_count'] if stats['total_count'] > 0 else 0
                available_keys.append(stats)
                total_data_points += stats['total_count']
                total_exception_points += stats['exception_count']

            # æŒ‰key_idæ’åº
            available_keys.sort(key=lambda x: x['key_id'])

            summary = {
                'total_keys': len(available_keys),
                'total_data_points': total_data_points,
                'total_exception_points': total_exception_points,
                'exception_rate': total_exception_points / total_data_points if total_data_points > 0 else 0
            }

            return {
                'available_keys': available_keys,
                'summary': summary
            }

        except Exception as e:
            logger.error(f"è·å–æŒ‰é”®ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'available_keys': [], 'summary': {}}

    def _configure_unified_waterfall_layout(self, fig: go.Figure, all_bars_by_algorithm: List[Dict], is_multi_file: bool) -> None:
        """
        é…ç½®ç»Ÿä¸€çš„ç€‘å¸ƒå›¾å¸ƒå±€ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€è½´æ ‡ç­¾ã€å›¾ä¾‹å’ŒåŠ¨æ€é«˜åº¦è°ƒæ•´ã€‚

        Args:
            fig: Plotlyå›¾å½¢å¯¹è±¡
            all_bars_by_algorithm: æŒ‰ç®—æ³•åˆ†ç»„çš„æ‰€æœ‰æ¡å½¢æ•°æ®
            is_multi_file: æ˜¯å¦å¤šæ–‡ä»¶æ¨¡å¼
        """
        # è®¡ç®—åŠ¨æ€é«˜åº¦
        if is_multi_file:
            num_files = len(all_bars_by_algorithm)
            # å¤šæ–‡ä»¶æ¨¡å¼ï¼šæ¯ä¸ªæ–‡ä»¶åˆ†é…æ›´å¤šé«˜åº¦
            base_height_per_file = 600
            total_height = max(800, base_height_per_file * num_files)
        else:
            # å•æ–‡ä»¶æ¨¡å¼ï¼šå›ºå®šé«˜åº¦
            total_height = 800

        # è®¡ç®—yè½´èŒƒå›´ï¼ˆè€ƒè™‘å¤šæ–‡ä»¶åç§»ï¼‰
        # è‡ªé€‚åº”è®¡ç®— Y è½´æ˜¾ç¤ºèŒƒå›´
        y_min_data = float('inf')
        y_max_data = float('-inf')
        for alg_data in all_bars_by_algorithm:
            for bar in alg_data.get('bars', []):
                y_val = bar.get('key_id')
                if y_val is not None:
                    y_min_data = min(y_min_data, y_val)
                    y_max_data = max(y_max_data, y_val)
        
        if y_min_data == float('inf'):
            # æ— æ•°æ®æ—¶ä½¿ç”¨é»˜è®¤ 88 é”®é’¢ç´èŒƒå›´
            y_min, y_max = 0.5, 88.5
        else:
            # å¢åŠ è¾¹è·ï¼Œå¹¶ç¡®ä¿æœ€å°è·¨åº¦è‡³å°‘ä¸º 12 ä¸ªå•ä½ï¼ˆé˜²æ­¢å•ä¸ªæŒ‰é”®è¢«è¿‡åº¦æ‹‰ä¼¸ï¼‰
            # è¿™æ ·å½•åˆ¶ä¸æ’­æ”¾çº¿æ¡ä¹‹é—´ 0.3 çš„é—´è·åœ¨è§†è§‰ä¸Šä¼šæ˜¾å¾—éå¸¸ç´§å‡‘ï¼ˆä»…å æ€»é«˜åº¦çš„ 2.5%ï¼‰
            span = y_max_data - y_min_data
            if span < 10:
                center = (y_min_data + y_max_data) / 2
                y_min, y_max = center - 6, center + 6
            else:
                y_min, y_max = y_min_data - 2, y_max_data + 2
        
        logger.info(f"[ç€‘å¸ƒå›¾å¸ƒå±€] æ•°æ®èŒƒå›´: Y[{y_min_data:.2f}, {y_max_data:.2f}], è‡ªé€‚åº”æ˜¾ç¤ºèŒƒå›´: Y[{y_min:.2f}, {y_max:.2f}] (span={y_max-y_min:.1f})")

        if is_multi_file:
            # ä¸ºå¤šæ–‡ä»¶æ¨¡å¼ä¿ç•™åç§»ä»¥ä¾¿åŒºåˆ†ä¸åŒæ–‡ä»¶
            y_axis_config = dict(
                range=[y_min, y_max],
                autorange=False, # ä½¿ç”¨æˆ‘ä»¬è®¡ç®—çš„èŒƒå›´
                fixedrange=False
            )
        else:
            # å•æ–‡ä»¶æ¨¡å¼ï¼šæ ‡å‡†é’¢ç´é”®åˆ»åº¦
            y_axis_config = dict(
                tickmode='array',
                tickvals=list(range(1, 89)),
                range=[y_min, y_max],
                autorange=False, # ä½¿ç”¨æˆ‘ä»¬è®¡ç®—çš„èŒƒå›´
                fixedrange=False
            )
            # å¦‚æœåªæ˜¾ç¤ºå°‘æ•°æŒ‰é”®ï¼Œå¯ä»¥é€šè¿‡ constrain='domain' æˆ–è®¾ç½®æœ€å°èŒƒå›´æ¥ä¼˜åŒ–è§†è§‰æ•ˆæœ
            # è¿™é‡Œæˆ‘ä»¬è®© Plotly è‡ªåŠ¨å¤„ç†ï¼Œä½†åœ¨åç»­å›¾è¡¨æ›´æ–°ä¸­å¯èƒ½éœ€è¦å‰ç«¯å›è°ƒæ¥é™åˆ¶

        # é…ç½®å¸ƒå±€
        fig.update_layout(
            title='ç€‘å¸ƒå›¾ - é’¢ç´æŒ‰é”®äº‹ä»¶æ—¶åºå¯è§†åŒ–',
            xaxis_title='æ—¶é—´ (ms)',
            yaxis_title='æŒ‰é”®ID' + (' (å¤šæ–‡ä»¶åç§»)' if is_multi_file else ''),
            xaxis=dict(
                fixedrange=False,  # å…è®¸Xè½´é€šè¿‡æ»šè½®ç¼©æ”¾
                autorange=True  # å…è®¸è‡ªåŠ¨è°ƒæ•´èŒƒå›´
            ),
            yaxis=y_axis_config,
            height=total_height,
            showlegend=True,
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='left',
                x=0.0,
                bgcolor='rgba(255, 255, 255, 0.9)',
                bordercolor='gray',
                borderwidth=1
            ),
            template='plotly_white',
            hovermode='closest',
            dragmode='zoom',  # è®¾ç½®æ‹–æ‹½æ¨¡å¼ä¸ºç¼©æ”¾ï¼Œæ”¯æŒæ»šè½®ç¼©æ”¾
            margin=dict(l=80, r=60, t=100, b=80)
        )
