#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SPMIDéŸ³ç¬¦åŒ¹é…å™¨

è´Ÿè´£SPMIDæ•°æ®çš„æŒ‰é”®åŒ¹é…ï¼ŒåŒ…æ‹¬ï¼š
- éŸ³ç¬¦åŒ¹é…ç®—æ³•
- åŒ¹é…å¯¹ç”Ÿæˆ
- åŒ¹é…ç»“æœç®¡ç†
"""

from .spmid_reader import Note
from typing import List, Tuple, Dict, Any
from utils.logger import Logger

logger = Logger.get_logger()

class NoteMatcher:
    """SPMIDéŸ³ç¬¦åŒ¹é…å™¨ç±»"""
    
    def __init__(self, global_time_offset: float = 0.0):
        """
        åˆå§‹åŒ–éŸ³ç¬¦åŒ¹é…å™¨
        
        Args:
            global_time_offset: å…¨å±€æ—¶é—´åç§»é‡ï¼ˆå·²åºŸå¼ƒï¼Œå›ºå®šä¸º0ï¼‰
        """
        self.global_time_offset = 0.0  # å›ºå®šä¸º0ï¼Œä¸å†ä½¿ç”¨å…¨å±€åç§»
        self.matched_pairs: List[Tuple[int, int, Note, Note]] = []
        # è®°å½•åŒ¹é…å¤±è´¥åŸå› ï¼škey=(data_type, index)ï¼Œvalue=str
        self.failure_reasons: Dict[Tuple[str, int], str] = {}
        # è®°å½•è¶…è¿‡é˜ˆå€¼ä½†æœ‰æœ€ä½³é…å¯¹çš„åŒ¹é…å¯¹ï¼šList[Tuple[int, int, Note, Note]]ï¼Œæ ¼å¼ä¸matched_pairsç›¸åŒ
        # è¿™äº›åŒ¹é…å¯¹è™½ç„¶è¶…è¿‡é˜ˆå€¼ï¼Œä½†ä»ç„¶æœ‰æœ€ä½³é…å¯¹ï¼Œå¯ä»¥ç”¨äºæ˜¾ç¤ºå¯¹æ¯”æ›²çº¿å›¾
        self.exceeds_threshold_matched_pairs: List[Tuple[int, int, Note, Note]] = []
    
    def find_all_matched_pairs(self, record_data: List[Note], replay_data: List[Note]) -> List[Tuple[int, int, Note, Note]]:
        """
        ä»¥å½•åˆ¶æ•°æ®ä¸ºåŸºå‡†ï¼Œåœ¨æ’­æ”¾æ•°æ®ä¸­å¯»æ‰¾åŒ¹é…çš„éŸ³ç¬¦å¯¹
        
        Args:
            record_data: å½•åˆ¶æ•°æ®
            replay_data: æ’­æ”¾æ•°æ®
            
        Returns:
            List[Tuple[int, int, Note, Note]]: åŒ¹é…å¯¹åˆ—è¡¨
        """
        matched_pairs = []
        used_replay_indices = set()
        # æ¸…ç©ºä¸Šä¸€è½®å¤±è´¥åŸå› å’Œè¶…è¿‡é˜ˆå€¼çš„åŒ¹é…å¯¹
        self.failure_reasons.clear()
        self.exceeds_threshold_matched_pairs.clear()
        
        logger.info(f"ğŸ¯ å¼€å§‹éŸ³ç¬¦åŒ¹é…: å½•åˆ¶æ•°æ®{len(record_data)}ä¸ªéŸ³ç¬¦, å›æ”¾æ•°æ®{len(replay_data)}ä¸ªéŸ³ç¬¦")
        
        # å½•åˆ¶æ•°æ®åœ¨æ’­æ”¾æ•°æ®ä¸­åŒ¹é…
        for i, record_note in enumerate(record_data):
            note_info = self._extract_note_info(record_note, i)

            # ç”Ÿæˆå€™é€‰åˆ—è¡¨ï¼ˆæŒ‰æ€»è¯¯å·®å‡åºï¼‰ï¼Œä»…ä¿ç•™åœ¨åŠ¨æ€é˜ˆå€¼å†…çš„å€™é€‰
            candidates, threshold, reason_if_empty = self._generate_sorted_candidates_within_threshold(
                replay_data,
                target_keyon=note_info["keyon"],
                target_keyoff=note_info["keyoff"],
                target_key_id=note_info["key_id"]
            )

            # å¦‚æœæ²¡æœ‰åœ¨é˜ˆå€¼å†…çš„å€™é€‰ï¼Œå°è¯•è·å–æ‰€æœ‰å€™é€‰ï¼ˆåŒ…æ‹¬è¶…è¿‡é˜ˆå€¼çš„ï¼‰ä»¥é€‰æ‹©æœ€ä½³åŒ¹é…
            all_candidates = None
            exceeds_threshold = False
            if not candidates:
                # è·å–æ‰€æœ‰å€™é€‰ï¼ˆåŒ…æ‹¬è¶…è¿‡é˜ˆå€¼çš„ï¼‰ï¼Œç”¨äºé€‰æ‹©æœ€ä½³åŒ¹é…
                all_candidates, threshold, reason_if_empty = self._generate_all_candidates_sorted(
                    replay_data,
                    target_keyon=note_info["keyon"],
                    target_keyoff=note_info["keyoff"],
                    target_key_id=note_info["key_id"]
                )
                
                if not all_candidates:
                    # å®Œå…¨æ²¡æœ‰å€™é€‰ï¼Œåˆ¤å®šå¤±è´¥ - æ˜ç¡®è¯´æ˜"å½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— "
                    reason = f"å½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— ï¼ˆæ²¡æœ‰æ‰¾åˆ°é”®ID {note_info['key_id']} çš„æ’­æ”¾éŸ³ç¬¦ï¼‰"
                    logger.info(f"âŒ åŒ¹é…å¤±è´¥: é”®ID={note_info['key_id']}, å½•åˆ¶ç´¢å¼•={i}, "
                               f"å½•åˆ¶æ—¶é—´=({note_info['keyon']/10:.2f}ms, {note_info['keyoff']/10:.2f}ms), "
                               f"åŸå› : {reason}")
                    self.failure_reasons[("record", i)] = reason
                    continue
                
                # å³ä½¿è¶…è¿‡é˜ˆå€¼ï¼Œä¹Ÿé€‰æ‹©è¯¯å·®æœ€å°çš„å€™é€‰ä½œä¸ºåŒ¹é…å¯¹ï¼ˆæ ‡è®°ä¸ºå¼‚å¸¸ï¼‰
                best_error_ms = all_candidates[0]['total_error'] / 10.0
                threshold_ms = threshold / 10.0
                reason = f"æ—¶é—´è¯¯å·®è¿‡å¤§ï¼ˆè¯¯å·®:{best_error_ms:.1f}ms, é˜ˆå€¼:{threshold_ms:.1f}msï¼‰ï¼Œè¶…è¿‡é˜ˆå€¼ä½†å­˜åœ¨æœ€ä½³åŒ¹é…å¯¹"
                logger.info(f"âš ï¸ æ‰€æœ‰å€™é€‰éƒ½è¶…è¿‡é˜ˆå€¼ï¼Œé€‰æ‹©æœ€ä½³åŒ¹é…ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰: é”®ID={note_info['key_id']}, å½•åˆ¶ç´¢å¼•={i}, "
                           f"æœ€ä½³è¯¯å·®={best_error_ms:.2f}ms, é˜ˆå€¼={threshold_ms:.2f}ms")
                candidates = all_candidates  # ä½¿ç”¨æ‰€æœ‰å€™é€‰ï¼ˆåŒ…æ‹¬è¶…è¿‡é˜ˆå€¼çš„ï¼‰
                exceeds_threshold = True  # æ ‡è®°ä¸ºè¶…è¿‡é˜ˆå€¼
                # è®°å½•è¶…è¿‡é˜ˆå€¼çš„åŸå› ï¼ˆç”¨äºåç»­æ˜¾ç¤ºï¼‰
                self.failure_reasons[("record", i)] = reason

            # ä»å€™é€‰ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæœªè¢«å ç”¨çš„é‡æ”¾ç´¢å¼•
            chosen = None
            for cand in candidates:
                cand_index = cand['index']
                if cand_index not in used_replay_indices:
                    chosen = cand
                    break

            if chosen is not None:
                replay_index = chosen['index']
                replay_note = replay_data[replay_index]
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                if exceeds_threshold or chosen['total_error'] > threshold:
                    # è¶…è¿‡é˜ˆå€¼ï¼ŒåŠ å…¥exceeds_threshold_matched_pairs
                    self.exceeds_threshold_matched_pairs.append((i, replay_index, record_note, replay_note))
                    used_replay_indices.add(replay_index)
                    # è®°å½•è¶…è¿‡é˜ˆå€¼çš„åŸå› ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®°å½•ï¼‰
                    if ("record", i) not in self.failure_reasons:
                        best_error_ms = chosen['total_error'] / 10.0
                        threshold_ms = threshold / 10.0
                        self.failure_reasons[("record", i)] = f"æ—¶é—´è¯¯å·®è¿‡å¤§ï¼ˆè¯¯å·®:{best_error_ms:.1f}ms, é˜ˆå€¼:{threshold_ms:.1f}msï¼‰ï¼Œè¶…è¿‡é˜ˆå€¼ä½†å­˜åœ¨æœ€ä½³åŒ¹é…å¯¹"
                    logger.info(f"âš ï¸ è¶…è¿‡é˜ˆå€¼åŒ¹é…å¯¹å·²è®°å½•: é”®ID={note_info['key_id']}, å½•åˆ¶ç´¢å¼•={i}, å›æ”¾ç´¢å¼•={replay_index}, "
                               f"è¯¯å·®={chosen['total_error']/10:.2f}ms, é˜ˆå€¼={threshold/10:.2f}ms")
                else:
                    # åœ¨é˜ˆå€¼å†…ï¼ŒåŠ å…¥æ­£å¸¸çš„matched_pairs
                    matched_pairs.append((i, replay_index, record_note, replay_note))
                    used_replay_indices.add(replay_index)
                
                # è®°å½•åŒ¹é…æˆåŠŸçš„è¯¦ç»†ä¿¡æ¯
                record_keyon, record_keyoff = self._calculate_note_times(record_note)
                replay_keyon, replay_keyoff = self._calculate_note_times(replay_note)
                keyon_offset = replay_keyon - record_keyon
                keyoff_offset = replay_keyoff - record_keyoff
                
                status = "âš ï¸ åŒ¹é…æˆåŠŸï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰" if (exceeds_threshold or chosen['total_error'] > threshold) else "âœ… åŒ¹é…æˆåŠŸ"
                logger.info(f"{status}: é”®ID={note_info['key_id']}, "
                           f"å½•åˆ¶ç´¢å¼•={i}, å›æ”¾ç´¢å¼•={replay_index}, "
                           f"å½•åˆ¶æ—¶é—´=({record_keyon/10:.2f}ms, {record_keyoff/10:.2f}ms), "
                           f"å›æ”¾æ—¶é—´=({replay_keyon/10:.2f}ms, {replay_keyoff/10:.2f}ms), "
                           f"åç§»=({keyon_offset/10:.2f}ms, {keyoff_offset/10:.2f}ms), "
                           f"æ€»è¯¯å·®={chosen['total_error']/10:.2f}ms, "
                           f"é˜ˆå€¼={threshold/10:.2f}ms")
            else:
                # æ‰€æœ‰å€™é€‰éƒ½è¢«å ç”¨ - æ˜ç¡®è¯´æ˜åŸå› 
                if len(candidates) > 0:
                    # æœ‰å€™é€‰ä½†éƒ½è¢«å ç”¨
                    reason = f"æ‰€æœ‰å€™é€‰å·²è¢«å ç”¨ï¼ˆå€™é€‰æ•°:{len(candidates)}, é˜ˆå€¼:{threshold/10:.1f}msï¼‰ï¼Œå½•åˆ¶æœ‰ï¼Œæ’­æ”¾æœ‰ä½†å·²è¢«å…¶ä»–å½•åˆ¶éŸ³ç¬¦åŒ¹é…"
                else:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸ºå‰é¢å·²ç»å¤„ç†äº†æ²¡æœ‰å€™é€‰çš„æƒ…å†µï¼‰
                    reason = f"å½•åˆ¶æœ‰ï¼Œæ’­æ”¾æ— ï¼ˆæ²¡æœ‰å¯ç”¨å€™é€‰ï¼‰"
                logger.info(f"âŒ åŒ¹é…å¤±è´¥: é”®ID={note_info['key_id']}, å½•åˆ¶ç´¢å¼•={i}, "
                           f"å½•åˆ¶æ—¶é—´=({note_info['keyon']/10:.2f}ms, {note_info['keyoff']/10:.2f}ms), "
                           f"åŸå› : {reason}")
                
                # è®°å½•è¢«å ç”¨çš„å€™é€‰è¯¦ç»†ä¿¡æ¯
                for j, cand in enumerate(candidates[:3]):  # åªè®°å½•å‰3ä¸ªå€™é€‰
                    cand_note = replay_data[cand['index']]
                    cand_keyon, cand_keyoff = self._calculate_note_times(cand_note)
                    logger.info(f"   å€™é€‰{j+1}: å›æ”¾ç´¢å¼•={cand['index']}, "
                               f"å›æ”¾æ—¶é—´=({cand_keyon/10:.2f}ms, {cand_keyoff/10:.2f}ms), "
                               f"æ€»è¯¯å·®={cand['total_error']/10:.2f}ms")
                
                self.failure_reasons[("record", i)] = reason
        
        self.matched_pairs = matched_pairs
        
        # è®°å½•åŒ¹é…ç»“æœç»Ÿè®¡
        success_count = len(matched_pairs)
        failure_count = len(record_data) - success_count
        logger.info(f"ğŸ¯ éŸ³ç¬¦åŒ¹é…å®Œæˆ: æˆåŠŸåŒ¹é…{success_count}å¯¹, å¤±è´¥{failure_count}ä¸ª, "
                   f"æˆåŠŸç‡{success_count/len(record_data)*100:.1f}%")
        
        return matched_pairs

    def _generate_sorted_candidates_within_threshold(self, notes_list: List[Note], target_keyon: float, target_keyoff: float, target_key_id: int) -> Tuple[List[Dict[str, float]], float, str]:
        """
        ç”Ÿæˆåœ¨åŠ¨æ€é˜ˆå€¼å†…çš„å€™é€‰åˆ—è¡¨ï¼ˆæŒ‰æ€»è¯¯å·®å‡åºï¼‰ã€‚

        å‚æ•°å•ä½ï¼š
            - target_keyon/target_keyoffï¼š0.1msï¼ˆç»å¯¹æ—¶é—´ = after_touch.index + offsetï¼‰
            - è¯¯å·®/é˜ˆå€¼ï¼š0.1msï¼ˆå†…éƒ¨ç»Ÿä¸€å•ä½ï¼‰

        Returns:
            (candidates, max_allowed_error, reason_if_empty)
        """
        # 1) è¿‡æ»¤åŒé”®ID
        matching = []
        for idx, note in enumerate(notes_list):
            if getattr(note, 'id', None) == target_key_id:
                matching.append((idx, note))

        if not matching:
            return [], 0.0, f"æ²¡æœ‰æ‰¾åˆ°é”®ID {target_key_id} çš„éŸ³ç¬¦"

        # 2) æ„å»ºå€™é€‰å¹¶è®¡ç®—è¯¯å·®
        # æ³¨æ„ï¼šæ­¤æ—¶æ‰€æœ‰éŸ³ç¬¦éƒ½å·²é€šè¿‡æ•°æ®è¿‡æ»¤ï¼Œä¿è¯æœ‰hammerså’Œafter_touchæ•°æ®
        candidates: List[Dict[str, float]] = []
        for idx, note in matching:
            # è®¡ç®—æŒ‰é”®å¼€å§‹å’Œç»“æŸæ—¶é—´
            current_keyon = note.after_touch.index[0] + note.offset
            current_keyoff = note.after_touch.index[-1] + note.offset

            # åªä½¿ç”¨keyon_offsetè®¡ç®—è¯¯å·®
            keyon_offset = current_keyon - target_keyon

            # è¯„åˆ†ï¼šåªä½¿ç”¨ |keyon_offset| ï¼ˆå•ä½ï¼š0.1msï¼‰
            total_error = abs(keyon_offset)

            candidates.append({
                'index': idx,
                'total_error': total_error,
                'keyon_error': abs(keyon_offset)
            })

        # ç”±äºæ•°æ®å·²è¿‡æ»¤ï¼Œç†è®ºä¸Šä¸ä¼šå‡ºç°ç©ºå€™é€‰åˆ—è¡¨ï¼ˆé™¤éæ²¡æœ‰ç›¸åŒé”®IDï¼‰
        # ä½†ä¿ç•™æ­¤æ£€æŸ¥ä»¥é˜²ä¸‡ä¸€
        if not candidates:
            return [], 0.0, f"æ²¡æœ‰æ‰¾åˆ°é”®ID {target_key_id} çš„å€™é€‰éŸ³ç¬¦"

        # 3) åŠ¨æ€é˜ˆå€¼ï¼ˆå•ä½ï¼š0.1msï¼›base_threshold=500â†’50msï¼›èŒƒå›´çº¦30â€“50msï¼‰
        base_threshold = 500.0
        duration = (target_keyoff - target_keyon)
        # æŒç»­æ—¶é—´å¿…é¡»å¤§äº0ï¼Œå¦åˆ™è§†ä¸ºå¼‚å¸¸éŸ³ç¬¦ï¼ˆç´¢å¼•æˆ–æ•°æ®å¼‚å¸¸ï¼‰
        # TODO
        if duration <= 0:
            return [], 0.0, "æ— æ•ˆæŒç»­æ—¶é—´(â‰¤0)ï¼Œç–‘ä¼¼å¼‚å¸¸éŸ³ç¬¦"
        duration_factor = min(1.0, max(0.6, duration / 500.0))
        max_allowed_error = base_threshold * duration_factor

        # 4) è¿‡æ»¤å‡ºåœ¨é˜ˆå€¼å†…çš„å€™é€‰å¹¶æ’åº
        within = [c for c in candidates if c['total_error'] <= max_allowed_error]
        within.sort(key=lambda x: x['total_error'])

        if not within:
            # å³ä½¿æœ‰å€™é€‰ï¼Œä½†å…¨éƒ¨è¶…é˜ˆå€¼
            # é€‰å‡ºæœ€å°è¯¯å·®ç”¨äºæç¤º
            best_total = min(c['total_error'] for c in candidates)
            # æ—¥å¿—/åŸå› å­—ç¬¦ä¸²ä»¥msæ˜¾ç¤ºï¼ˆå†…éƒ¨0.1mséœ€/10ï¼‰
            return [], max_allowed_error, (
                f"æ—¶é—´è¯¯å·®è¿‡å¤§(è¯¯å·®:{best_total/10:.1f}ms, é˜ˆå€¼:{max_allowed_error/10:.1f}ms)"
            )

        return within, max_allowed_error, ""
    
    def _generate_all_candidates_sorted(self, notes_list: List[Note], target_keyon: float, target_keyoff: float, target_key_id: int) -> Tuple[List[Dict[str, float]], float, str]:
        """
        ç”Ÿæˆæ‰€æœ‰å€™é€‰åˆ—è¡¨ï¼ˆåŒ…æ‹¬è¶…è¿‡é˜ˆå€¼çš„ï¼‰ï¼ŒæŒ‰æ€»è¯¯å·®å‡åºæ’åº
        
        ç”¨äºåœ¨æ²¡æœ‰ä»»ä½•å€™é€‰åœ¨é˜ˆå€¼å†…æ—¶ï¼Œé€‰æ‹©æœ€ä½³åŒ¹é…ï¼ˆå³ä½¿è¶…è¿‡é˜ˆå€¼ï¼‰
        
        å‚æ•°å•ä½ï¼š
            - target_keyon/target_keyoffï¼š0.1msï¼ˆç»å¯¹æ—¶é—´ = after_touch.index + offsetï¼‰
            - è¯¯å·®/é˜ˆå€¼ï¼š0.1msï¼ˆå†…éƒ¨ç»Ÿä¸€å•ä½ï¼‰

        Returns:
            (candidates, max_allowed_error, reason_if_empty)
        """
        # 1) è¿‡æ»¤åŒé”®ID
        matching = []
        for idx, note in enumerate(notes_list):
            if getattr(note, 'id', None) == target_key_id:
                matching.append((idx, note))

        if not matching:
            return [], 0.0, f"æ²¡æœ‰æ‰¾åˆ°é”®ID {target_key_id} çš„éŸ³ç¬¦"

        # 2) æ„å»ºå€™é€‰å¹¶è®¡ç®—è¯¯å·®
        candidates: List[Dict[str, float]] = []
        for idx, note in matching:
            # è®¡ç®—æŒ‰é”®å¼€å§‹å’Œç»“æŸæ—¶é—´
            current_keyon = note.after_touch.index[0] + note.offset
            current_keyoff = note.after_touch.index[-1] + note.offset

            # åªä½¿ç”¨keyon_offsetè®¡ç®—è¯¯å·®
            keyon_offset = current_keyon - target_keyon

            # è¯„åˆ†ï¼šåªä½¿ç”¨ |keyon_offset| ï¼ˆå•ä½ï¼š0.1msï¼‰
            total_error = abs(keyon_offset)

            candidates.append({
                'index': idx,
                'total_error': total_error,
                'keyon_error': abs(keyon_offset)
            })

        if not candidates:
            return [], 0.0, f"æ²¡æœ‰æ‰¾åˆ°é”®ID {target_key_id} çš„å€™é€‰éŸ³ç¬¦"

        # 3) åŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        base_threshold = 500.0
        duration = (target_keyoff - target_keyon)
        if duration <= 0:
            return [], 0.0, "æ— æ•ˆæŒç»­æ—¶é—´(â‰¤0)ï¼Œç–‘ä¼¼å¼‚å¸¸éŸ³ç¬¦"
        duration_factor = min(1.0, max(0.6, duration / 500.0))
        max_allowed_error = base_threshold * duration_factor

        # 4) æ’åºæ‰€æœ‰å€™é€‰ï¼ˆåŒ…æ‹¬è¶…è¿‡é˜ˆå€¼çš„ï¼‰
        candidates.sort(key=lambda x: x['total_error'])

        return candidates, max_allowed_error, ""
    
    def _extract_note_info(self, note: Note, index: int) -> Dict:
        """
        æå–éŸ³ç¬¦åŸºæœ¬ä¿¡æ¯
        
        Args:
            note: éŸ³ç¬¦å¯¹è±¡
            index: éŸ³ç¬¦ç´¢å¼•
            
        Returns:
            Dict: éŸ³ç¬¦ä¿¡æ¯å­—å…¸
        """
        # è®¡ç®—ç»å¯¹æ—¶é—´æˆ³
        absolute_keyon = note.after_touch.index[0] + note.offset
        absolute_keyoff = note.after_touch.index[-1] + note.offset
        
        return {
            'keyon': absolute_keyon,
            'keyoff': absolute_keyoff,
            'key_id': note.id,
            'index': index,
            'relative_keyon': note.after_touch.index[0] + note.offset,
            'relative_keyoff': note.after_touch.index[-1] + note.offset
        }
    
    def extract_normal_matched_pairs(self, matched_pairs: List[Tuple[int, int, Note, Note]], 
                                   multi_hammers: List, drop_hammers: List) -> Tuple[List[Note], List[Note]]:
        """
        ä»åŒ¹é…å¯¹ä¸­æå–æ­£å¸¸åŒ¹é…çš„éŸ³ç¬¦å¯¹
        
        Args:
            matched_pairs: åŒ¹é…å¯¹åˆ—è¡¨
            multi_hammers: å¤šé”¤åˆ—è¡¨
            drop_hammers: ä¸¢é”¤åˆ—è¡¨
            
        Returns:
            Tuple[List[Note], List[Note]]: (matched_record_data, matched_replay_data)
        """
        matched_record_data = []
        matched_replay_data = []
        
        for record_index, replay_index, record_note, replay_note in matched_pairs:
            matched_record_data.append(record_note)
            matched_replay_data.append(replay_note)
        
        return matched_record_data, matched_replay_data
    
    def get_matched_pairs(self) -> List[Tuple[int, int, Note, Note]]:
        """
        è·å–åŒ¹é…å¯¹åˆ—è¡¨
        
        Returns:
            List[Tuple[int, int, Note, Note]]: åŒ¹é…å¯¹åˆ—è¡¨
        """
        return self.matched_pairs.copy()
    
    # TODO
    def get_offset_alignment_data(self) -> List[Dict[str, Any]]:
        """
        è·å–åç§»å¯¹é½æ•°æ® - è®¡ç®—æ¯ä¸ªåŒ¹é…å¯¹çš„æ—¶é—´åç§»
        
        Returns:
            List[Dict[str, Any]]: åç§»å¯¹é½æ•°æ®åˆ—è¡¨
        """
        offset_data = []
        
        for record_idx, replay_idx, record_note, replay_note in self.matched_pairs:
            # è®¡ç®—å½•åˆ¶å’Œæ’­æ”¾éŸ³ç¬¦çš„æ—¶é—´
            record_keyon, record_keyoff = self._calculate_note_times(record_note)
            replay_keyon, replay_keyoff = self._calculate_note_times(replay_note)
            
            # è®¡ç®—åç§»é‡ï¼šåªä½¿ç”¨keyon_offset
            keyon_offset = replay_keyon - record_keyon
            record_duration = record_keyoff - record_keyon
            replay_duration = replay_keyoff - replay_keyon
            duration_diff = replay_duration - record_duration
            duration_offset = duration_diff
            # åªä½¿ç”¨keyon_offsetè®¡ç®—average_offset
            avg_offset = abs(keyon_offset)
    
            
            offset_data.append({
                'record_index': record_idx,
                'replay_index': replay_idx,
                'key_id': record_note.id,
                'record_keyon': record_keyon,
                'replay_keyon': replay_keyon,
                'keyon_offset': keyon_offset,
                'record_keyoff': record_keyoff,
                'replay_keyoff': replay_keyoff,
                'duration_offset': duration_offset,
                'average_offset': avg_offset,  
                'record_duration': record_duration,
                'replay_duration': replay_duration,
                'duration_diff': duration_diff
            })
        
        return offset_data
    
    def get_invalid_notes_offset_analysis(self, record_data: List[Note], replay_data: List[Note]) -> List[Dict[str, Any]]:
        """
        è·å–æ— æ•ˆéŸ³ç¬¦çš„åç§»å¯¹é½åˆ†æ
        
        Args:
            record_data: å½•åˆ¶æ•°æ®
            replay_data: æ’­æ”¾æ•°æ®
            
        Returns:
            List[Dict[str, Any]]: æ— æ•ˆéŸ³ç¬¦åç§»åˆ†ææ•°æ®
        """
        invalid_offset_data = []
        
        # è·å–å·²åŒ¹é…çš„éŸ³ç¬¦ç´¢å¼•
        matched_record_indices = set(pair[0] for pair in self.matched_pairs)
        matched_replay_indices = set(pair[1] for pair in self.matched_pairs)
        
        # åˆ†æå½•åˆ¶æ•°æ®ä¸­çš„æ— æ•ˆéŸ³ç¬¦ï¼ˆæœªåŒ¹é…çš„éŸ³ç¬¦ï¼‰
        invalid_offset_data.extend(
            self._analyze_invalid_notes(record_data, matched_record_indices, 'record', replay_data)
        )
        
        # åˆ†ææ’­æ”¾æ•°æ®ä¸­çš„æ— æ•ˆéŸ³ç¬¦ï¼ˆæœªåŒ¹é…çš„éŸ³ç¬¦ï¼‰
        invalid_offset_data.extend(
            self._analyze_invalid_notes(replay_data, matched_replay_indices, 'replay', record_data)
        )
        
        return invalid_offset_data
    
    def _analyze_invalid_notes(self, notes_data: List[Note], matched_indices: set, data_type: str, 
                              other_notes_data: List[Note] = None) -> List[Dict[str, Any]]:
        """
        åˆ†ææ— æ•ˆéŸ³ç¬¦çš„é€šç”¨æ–¹æ³•
        
        Args:
            notes_data: éŸ³ç¬¦æ•°æ®åˆ—è¡¨
            matched_indices: å·²åŒ¹é…çš„éŸ³ç¬¦ç´¢å¼•é›†åˆ
            data_type: æ•°æ®ç±»å‹ ('record' æˆ– 'replay')
            other_notes_data: å¦ä¸€ä¸ªæ•°æ®ç±»å‹çš„éŸ³ç¬¦åˆ—è¡¨ï¼Œç”¨äºåˆ†æåŒ¹é…å¤±è´¥åŸå› 
            
        Returns:
            List[Dict[str, Any]]: æ— æ•ˆéŸ³ç¬¦åˆ†ææ•°æ®
        """
        invalid_notes = []
        
        for i, note in enumerate(notes_data):
            if i not in matched_indices:  # æœªåŒ¹é…çš„éŸ³ç¬¦
                try:
                    keyon_time, keyoff_time = self._calculate_note_times(note)
                    
                    # ä¼˜å…ˆä½¿ç”¨åŒ¹é…é˜¶æ®µè®°å½•çš„çœŸå®å¤±è´¥åŸå› ï¼ˆä»…recordä¾§æœ‰ï¼‰
                    analysis_reason = None
                    if data_type == 'record' and (data_type, i) in self.failure_reasons:
                        analysis_reason = self.failure_reasons[(data_type, i)]
                    else:
                        # å›æ”¾ä¾§æˆ–æ— è®°å½•æ—¶ï¼Œå†åšæ¨æ–­åˆ†æ
                        analysis_reason = self._get_actual_unmatch_reason(note, data_type, i, other_notes_data)
                    
                    invalid_notes.append({
                        'data_type': data_type,
                        'note_index': i,
                        'key_id': note.id,
                        'keyon_time': keyon_time,
                        'keyoff_time': keyoff_time,
                        'status': 'unmatched',
                        'analysis_reason': analysis_reason
                    })
                except (IndexError, AttributeError) as e:
                    # å¤„ç†æ•°æ®å¼‚å¸¸çš„æƒ…å†µ
                    invalid_notes.append({
                        'data_type': data_type,
                        'note_index': i,
                        'key_id': note.id,
                        'keyon_time': 0.0,
                        'keyoff_time': 0.0,
                        'status': 'data_error',
                        'analysis_reason': f'æ•°æ®å¼‚å¸¸: {str(e)}'
                    })
        
        return invalid_notes
    
    def _get_actual_unmatch_reason(self, note: Note, data_type: str, note_index: int, 
                                  other_notes_data: List[Note] = None) -> str:
        """
        åˆ†ææœªåŒ¹é…éŸ³ç¬¦çš„å®é™…å¤±è´¥åŸå› 
        
        Args:
            note: éŸ³ç¬¦å¯¹è±¡
            data_type: æ•°æ®ç±»å‹ ('record' æˆ– 'replay')
            note_index: éŸ³ç¬¦ç´¢å¼•
            other_notes_data: å¦ä¸€ä¸ªæ•°æ®ç±»å‹çš„éŸ³ç¬¦åˆ—è¡¨
            
        Returns:
            str: åŒ¹é…å¤±è´¥åŸå› 
        """
        if other_notes_data is None:
            return "æ— æ³•åˆ†æåŒ¹é…å¤±è´¥åŸå› (ç¼ºå°‘å¯¹æ¯”æ•°æ®)"
        
        try:
            # æå–å½“å‰éŸ³ç¬¦ä¿¡æ¯
            note_info = self._extract_note_info(note, note_index)
            
            # åˆ†æåŒ¹é…å¤±è´¥çš„å…·ä½“åŸå› 
            return self._analyze_match_failure_reason(note_info, other_notes_data, data_type)
            
        except Exception as e:
            return f"åˆ†æåŒ¹é…å¤±è´¥åŸå› æ—¶å‡ºé”™: {str(e)}"
    
    def _analyze_match_failure_reason(self, note_info: Dict, other_notes_data: List[Note], data_type: str) -> str:
        """
        åˆ†æåŒ¹é…å¤±è´¥çš„å…·ä½“åŸå› ï¼ˆå›æ”¾ä¾§æ¨æ–­ç”¨ï¼‰
        
        æ³¨æ„ï¼šå½•åˆ¶ä¾§å·²åœ¨åŒ¹é…é˜¶æ®µè®°å½•çœŸå®åŸå› ï¼Œæ­¤æ–¹æ³•ä¸»è¦ç”¨äºå›æ”¾ä¾§æ¨æ–­
        
        Args:
            note_info: éŸ³ç¬¦ä¿¡æ¯å­—å…¸
            other_notes_data: å¦ä¸€ä¸ªæ•°æ®ç±»å‹çš„éŸ³ç¬¦åˆ—è¡¨
            data_type: æ•°æ®ç±»å‹
            
        Returns:
            str: åŒ¹é…å¤±è´¥åŸå› 
        """
        target_key_id = note_info["key_id"]
        target_keyon = note_info["keyon"]
        target_keyoff = note_info["keyoff"]
        
        # è°ƒç”¨ç›¸åŒçš„å€™é€‰ç”Ÿæˆé€»è¾‘ï¼ˆç¡®ä¿ä¸åŒ¹é…é˜¶æ®µä¸€è‡´ï¼‰
        candidates, threshold, reason_if_empty = self._generate_sorted_candidates_within_threshold(
            other_notes_data,
            target_keyon=target_keyon,
            target_keyoff=target_keyoff,
            target_key_id=target_key_id
        )
        
        if not candidates:
            return reason_if_empty
        
        # æœ‰åœ¨é˜ˆå€¼å†…çš„å€™é€‰ï¼Œä½†æœªè¢«åŒ¹é… -> å¯èƒ½å…¨è¢«å ç”¨ï¼ˆå›æ”¾ä¾§æ— æ³•å¾—çŸ¥å ç”¨æƒ…å†µï¼‰
        return f"å¯èƒ½æ‰€æœ‰å€™é€‰å·²è¢«å ç”¨(å€™é€‰æ•°:{len(candidates)}, é˜ˆå€¼:{threshold:.1f}ms)"
    
    def _calculate_note_times(self, note: Note) -> Tuple[float, float]:
        """
        è®¡ç®—éŸ³ç¬¦çš„æŒ‰é”®å¼€å§‹å’Œç»“æŸæ—¶é—´
        
        Args:
            note: éŸ³ç¬¦å¯¹è±¡
            
        Returns:
            Tuple[float, float]: (keyon_time, keyoff_time)
        """

        keyon_time = note.after_touch.index[0] + note.offset
        keyoff_time = note.after_touch.index[-1] + note.offset
        
        return keyon_time, keyoff_time
    
    # TODO  
    def get_global_average_delay(self) -> float:
        """
        è®¡ç®—æ•´é¦–æ›²å­çš„å¹³å‡æ—¶å»¶ï¼ˆåŸºäºå·²é…å¯¹æ•°æ®ï¼‰
        
        ä½¿ç”¨å¸¦ç¬¦å·çš„ keyon_offset è®¡ç®—ï¼šå…¨å±€å¹³å‡æ—¶å»¶ = mean(keyon_offset)
        æ­£å€¼è¡¨ç¤º replay å»¶è¿Ÿï¼Œè´Ÿå€¼è¡¨ç¤º replay æå‰
        
        æ³¨æ„ï¼šæ­¤æŒ‡æ ‡ä¸å¹³å‡è¯¯å·®ï¼ˆMEï¼Œget_mean_error()ï¼‰åœ¨è®¡ç®—å’Œæ¦‚å¿µä¸Šå®Œå…¨ç›¸åŒï¼Œ
        éƒ½æ˜¯å¯¹æ‰€æœ‰ keyon_offset æ±‚ç®—æœ¯å¹³å‡ï¼Œåæ˜ æ•´ä½“çš„æå‰/æ»åæ–¹å‘æ€§ã€‚
        å¦‚æœéœ€è¦ä¸è€ƒè™‘æ–¹å‘çš„å¹³å‡å»¶æ—¶å¹…åº¦ï¼Œåº”ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€‚
        
        Returns:
            float: å¹³å‡æ—¶å»¶ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
        """
        if not self.matched_pairs:
            return 0.0
        
        # è·å–åç§»æ•°æ®
        offset_data = self.get_offset_alignment_data()
        
        # ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetï¼ˆä¸å–ç»å¯¹å€¼ï¼‰
        keyon_offsets = [item.get('keyon_offset', 0) for item in offset_data if item.get('keyon_offset') is not None]
        
        if not keyon_offsets:
            return 0.0
        
        # è®¡ç®—å¹³å‡å€¼ï¼ˆ0.1mså•ä½ï¼Œå¸¦ç¬¦å·ï¼‰
        average_delay = sum(keyon_offsets) / len(keyon_offsets)
        
        logger.info(f"ğŸ“Š æ•´é¦–æ›²å­å¹³å‡æ—¶å»¶(keyon): {average_delay/10:.2f}ms (åŸºäº{len(keyon_offsets)}ä¸ªåŒ¹é…å¯¹ï¼Œå¸¦ç¬¦å·)")
        
        return average_delay
    
    def get_variance(self) -> float:
        """
        è®¡ç®—å·²åŒ¹é…æŒ‰é”®å¯¹çš„æ€»ä½“æ–¹å·®ï¼ˆPopulation Varianceï¼‰
        
        è¯´æ˜ï¼š
        - "åŒ¹é…å¯¹"æŒ‡çš„æ˜¯matched_pairsä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œæ˜¯ä¸€ä¸ª(record_note, replay_note)çš„é…å¯¹
        - å¯¹æ¯ä¸ªåŒ¹é…å¯¹è®¡ç®—keyon_offset = replay_keyon - record_keyon
        - ä½¿ç”¨å¸¦ç¬¦å·çš„keyon_offsetè®¡ç®—æ–¹å·®ï¼ŒæŒ‰ç…§æ ‡å‡†æ€»ä½“æ–¹å·®å…¬å¼
        
        æ ‡å‡†æ•°å­¦å…¬å¼ï¼š
        ÏƒÂ² = (1/n) * Î£(x_i - Î¼)Â²
        å…¶ä¸­ x_i æ˜¯å¸¦ç¬¦å·çš„keyon_offsetï¼ŒÎ¼ = (1/n) * Î£ x_iï¼ˆæ€»ä½“å‡å€¼ï¼‰
        
        Returns:
            float: æ€»ä½“æ–¹å·®ï¼ˆå•ä½ï¼š(0.1ms)Â²ï¼Œè½¬æ¢ä¸ºmsÂ²éœ€è¦é™¤ä»¥100ï¼‰
        """
        if not self.matched_pairs:
            return 0.0
        
        # è·å–åç§»å¯¹é½æ•°æ®
        offset_data = self.get_offset_alignment_data()
        
        # æå–æ‰€æœ‰å¸¦ç¬¦å·çš„keyon_offset
        offsets = []
        for item in offset_data:
            keyon_offset = item.get('keyon_offset', 0)
            offsets.append(keyon_offset)  # ä½¿ç”¨å¸¦ç¬¦å·å€¼
        
        if len(offsets) <= 1:
            return 0.0
        
        # è®¡ç®—æ€»ä½“æ–¹å·®ï¼ˆä½¿ç”¨æ ‡å‡†å…¬å¼ï¼Œåˆ†æ¯ nï¼‰
        # å…¬å¼ï¼šÏƒÂ² = (1/n) * Î£(x_i - Î¼)Â²
        # å…¶ä¸­ Î¼ = (1/n) * Î£ x_iï¼ˆæ€»ä½“å‡å€¼ï¼‰
        mean = sum(offsets) / len(offsets)  # æ€»ä½“å‡å€¼ï¼ˆå¸¦ç¬¦å·ï¼‰
        variance = sum((x - mean) ** 2 for x in offsets) / len(offsets)  # æ€»ä½“æ–¹å·®ä½¿ç”¨ n
        return variance
    
    def get_standard_deviation(self) -> float:
        """
        è®¡ç®—å·²é…å¯¹æŒ‰é”®çš„æ€»ä½“æ ‡å‡†å·®ï¼ˆPopulation Standard Deviationï¼‰
        å¯¹æ‰€æœ‰å·²åŒ¹é…æŒ‰é”®å¯¹çš„å¸¦ç¬¦å·keyon_offsetè®¡ç®—æ€»ä½“æ ‡å‡†å·®
        æ€»ä½“æ ‡å‡†å·® = sqrt(æ€»ä½“æ–¹å·®)
        
        æŒ‰ç…§æ ‡å‡†æ•°å­¦å…¬å¼ï¼šÏƒ = âˆš(ÏƒÂ²) = âˆš((1/n) * Î£(x_i - Î¼)Â²)
        å…¶ä¸­ x_i æ˜¯å¸¦ç¬¦å·çš„keyon_offsetï¼ŒÎ¼ = (1/n) * Î£ x_iï¼ˆæ€»ä½“å‡å€¼ï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç›´æ¥è°ƒç”¨ get_variance() ç„¶åå¼€å¹³æ–¹æ ¹ï¼Œç¡®ä¿ä¸æ–¹å·®è®¡ç®—çš„ä¸€è‡´æ€§
        ç”±äº get_variance() ä½¿ç”¨å¸¦ç¬¦å·å€¼è®¡ç®—ï¼Œæ­¤æ–¹æ³•ä¹Ÿä½¿ç”¨å¸¦ç¬¦å·å€¼
        
        Returns:
            float: æ€»ä½“æ ‡å‡†å·®ï¼ˆå•ä½ï¼š0.1msï¼Œè½¬æ¢ä¸ºmséœ€è¦é™¤ä»¥10ï¼‰
        """
        variance = self.get_variance()
        if variance < 0:
            # ç†è®ºä¸Šä¸åº”è¯¥å‡ºç°è´Ÿæ•°ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
            logger.warning(f"âš ï¸ æ€»ä½“æ–¹å·®ä¸ºè´Ÿæ•°: {variance}ï¼Œè¿”å›0")
            return 0.0
        std = variance ** 0.5
        return std
    
    def get_mean_absolute_error(self) -> float:
        """
        è®¡ç®—å·²é…å¯¹æŒ‰é”®çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰
        å¯¹æ‰€æœ‰å·²åŒ¹é…æŒ‰é”®å¯¹çš„å»¶æ—¶ç»å¯¹å€¼æ±‚å¹³å‡
        
        Returns:
            float: å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆå•ä½ï¼š0.1msï¼Œè½¬æ¢ä¸ºmséœ€è¦é™¤ä»¥10ï¼‰
        """
        if not self.matched_pairs:
            return 0.0
        
        # è·å–åç§»å¯¹é½æ•°æ®
        offset_data = self.get_offset_alignment_data()
        
        # æå–æ‰€æœ‰å»¶æ—¶çš„ç»å¯¹å€¼
        abs_errors = []
        for item in offset_data:
            keyon_offset = item.get('keyon_offset', 0)
            abs_error = abs(keyon_offset)
            abs_errors.append(abs_error)
        
        # è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®
        if abs_errors:
            mae = sum(abs_errors) / len(abs_errors)
            return mae
        else:
            return 0.0
    
    def get_mean_squared_error(self) -> float:
        """
        è®¡ç®—å·²é…å¯¹æŒ‰é”®çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
        å¯¹æ‰€æœ‰å·²åŒ¹é…æŒ‰é”®å¯¹çš„å»¶æ—¶çš„å¹³æ–¹æ±‚å¹³å‡
        
        Returns:
            float: å‡æ–¹è¯¯å·®ï¼ˆå•ä½ï¼š(0.1ms)Â²ï¼Œè½¬æ¢ä¸ºmsÂ²éœ€è¦é™¤ä»¥100ï¼‰
        """
        if not self.matched_pairs:
            return 0.0
        
        # è·å–åç§»å¯¹é½æ•°æ®
        offset_data = self.get_offset_alignment_data()
        
        # æå–æ‰€æœ‰å»¶æ—¶çš„å¹³æ–¹å€¼
        squared_errors = []
        for item in offset_data:
            keyon_offset = item.get('keyon_offset', 0)
            squared_error = keyon_offset ** 2  # æ³¨æ„è¿™é‡Œä½¿ç”¨åŸå§‹å€¼ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰ï¼Œå¹³æ–¹åä¸ºæ­£
            squared_errors.append(squared_error)
        
        # è®¡ç®—å‡æ–¹è¯¯å·®
        if squared_errors:
            mse = sum(squared_errors) / len(squared_errors)
            return mse
        else:
            return 0.0

    def get_mean_error(self) -> float:
        """
        è®¡ç®—å·²åŒ¹é…æŒ‰é”®å¯¹çš„å¹³å‡è¯¯å·®ï¼ˆMEï¼Œå¸¦ç¬¦å·çš„å¹³å‡åå·®ï¼‰
        å¯¹æ‰€æœ‰åŒ¹é…å¯¹çš„keyon_offsetï¼ˆreplay_keyon - record_keyonï¼‰æ±‚ç®—æœ¯å¹³å‡ã€‚
        
        Returns:
            float: å¹³å‡è¯¯å·®MEï¼ˆå•ä½ï¼š0.1msï¼ŒUIæ˜¾ç¤ºä¸ºmséœ€é™¤ä»¥10ï¼‰
        """
        if not self.matched_pairs:
            return 0.0
        
        offset_data = self.get_offset_alignment_data()
        offsets = [item.get('keyon_offset', 0) for item in offset_data]
        if not offsets:
            return 0.0
        return sum(offsets) / len(offsets)
    

    
    def get_offset_statistics(self) -> Dict[str, Any]:
        """
        è·å–åç§»ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: åç§»ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.matched_pairs:
            return {
                'total_pairs': 0,
                'keyon_offset_stats': {'average': 0.0, 'max': 0.0, 'min': 0.0, 'std': 0.0},
                'duration_offset_stats': {'average': 0.0, 'max': 0.0, 'min': 0.0, 'std': 0.0},
                'overall_offset_stats': {'average': 0.0, 'max': 0.0, 'min': 0.0, 'std': 0.0}
            }
        
        # è·å–åç§»æ•°æ®
        offset_data = self.get_offset_alignment_data()
        
        # æå–åç§»å€¼ï¼ˆåªä½¿ç”¨keyon_offsetï¼‰
        keyon_offsets = [item['keyon_offset'] for item in offset_data]
        duration_offsets = [item.get('duration_offset', 0.0) for item in offset_data]
        # æ•´ä½“ç»Ÿè®¡åªä½¿ç”¨keyon_offsetçš„ç»å¯¹å€¼
        overall_offsets = [abs(item.get('keyon_offset', 0)) for item in offset_data if item.get('keyon_offset') is not None]
        
        return {
            'total_pairs': len(self.matched_pairs),
            'keyon_offset_stats': self._calculate_offset_stats(keyon_offsets),
            'duration_offset_stats': self._calculate_offset_stats(duration_offsets),
            'overall_offset_stats': self._calculate_offset_stats(overall_offsets)  # åªä½¿ç”¨keyon_offset
        }
    
    def _calculate_offset_stats(self, offsets: List[float]) -> Dict[str, float]:
        """
        è®¡ç®—åç§»ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            offsets: åç§»å€¼åˆ—è¡¨
            
        Returns:
            Dict[str, float]: ç»Ÿè®¡ä¿¡æ¯
        """
        if not offsets:
            return {'average': 0.0, 'max': 0.0, 'min': 0.0, 'std': 0.0}
        
        average = sum(offsets) / len(offsets)
        max_val = max(offsets)
        min_val = min(offsets)
        
        # è®¡ç®—æ ‡å‡†å·®
        if len(offsets) <= 1:
            std = 0.0
        else:
            variance = sum((x - average) ** 2 for x in offsets) / (len(offsets) - 1)
            std = variance ** 0.5
        
        return {
            'average': average,
            'max': max_val,
            'min': min_val,
            'std': std
        }
